---
title: "Ensemble Recommender System for Book Recommendations: A Comparative Analysis of Collaborative Filtering Approaches"
author: "Hope Hennessy"
date: "2025-10-01"
output:
  pdf_document:
    latex_engine: xelatex
---

\newpage

# Abstract

This study presents a comprehensive comparative analysis of four collaborative filtering approaches for book recommendation systems using a modified Book-Crossing dataset. We implement item-based collaborative filtering, user-based collaborative filtering, matrix factorization, and neural network-based methods to build an ensemble recommender system. Our analysis includes cross-validation performance evaluation, cold start problem handling, and investigation of dataset size effects on predictive accuracy. The results demonstrate the relative strengths and limitations of each approach, providing insights for practical recommendation system deployment.

## Assignment Overview

Build an ensemble recommender system for book recommendations using a modified "Book-Crossing" dataset containing ratings (0-10 scale) from 10,000 users on 150 books.

### Core Requirements

1. **Build Four Types of Recommender Systems:**
   - Item-based collaborative filtering (code from scratch)
   - User-based collaborative filtering (code from scratch)
   - Matrix factorization-based collaborative filtering
   - Neural network-based collaborative filtering

2. **System Capabilities:**
   - Recommend books to existing users
   - Handle new users (assuming they provide ratings for ≤5 books initially)

3. **Evaluation and Analysis:**
   - Compare accuracy across all four methods using cross-validation
   - Investigate the relationship between dataset size and accuracy
   - Determine if there's a point where adding more titles doesn't improve accuracy

4. **Data Analysis:**
   - Conduct exploratory data analysis (EDA)
   - Use findings to inform train/test data splitting

# 1. Introduction

The rapid expansion of digital content has created an overwhelming abundance of choice, making it difficult for users to find relevant information. Recommender systems address this challenge by predicting user preferences and suggesting items tailored to individual tastes. Among these, collaborative filtering (CF) has proven highly effective by leveraging patterns in user–item interactions to infer preferences from similar users or items.

Collaborative filtering methods can be broadly categorized into memory-based and model-based approaches. Memory-based methods, such as user-based (UBCF) and item-based (IBCF) filtering, calculate similarities directly from the user–item rating matrix to generate predictions through weighted averages. Model-based methods, including matrix factorization (MF) and neural network-based CF (NN-CF), learn latent factors that capture hidden preference patterns in lower-dimensional spaces. Each approach offers distinct strengths and trade-offs depending on dataset sparsity, dimensionality, and computational requirements.

Book recommendation systems present unique challenges, including extreme sparsity—since users typically rate only a small fraction of available books—and the cold start problem, where new users or books lack sufficient rating data.

This study compares four collaborative filtering approaches under identical conditions in the book recommendation domain to evaluate their relative performance and sensitivity to catalog size. Using a modified Book-Crossing dataset with 10,000 users and 150 books, the analysis incorporates confidence-weighted similarity calculations for memory-based methods and employs the recosystem and H2O packages for model-based implementations. Evaluation is performed on explicit ratings (1–10 scale) using five-fold cross-validation with RMSE as the primary metric. By systematically varying the number of training books (30–150) while maintaining a consistent test set, the study examines how dataset scale influences prediction accuracy and when additional items yield diminishing returns.

The findings offer empirical insights into handling data sparsity, implicit feedback, and cold start issues, while providing practical guidance for selecting and combining collaborative filtering methods in high-sparsity recommendation environments.


```{r setup, include=FALSE}
# ============================================================================
# GLOBAL SETTINGS AND CONFIGURATION
# ============================================================================

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center",
  fig.width = 8,
  fig.height = 6,
  fig.show = "hold",
  out.width = "80%",
  dpi = 300
)

# Set random seed for reproducibility
set.seed(123)

# ============================================================================
# LIBRARY IMPORTS
# ============================================================================

library(tidyverse)
library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
library(cowplot)

library(knitr)
library(kableExtra)

library(caret)
library(recosystem)
library(h2o)

options(
  lintr.linter_file = "none",
  lintr.exclude_linters = c(
    "object_usage_linter",
    "object_name_linter",
    "cyclocomp_linter",
    "line_length_linter"
  )
)
```

# Data Preparation and Exploratory Analysis

## Data Loading and Initial Inspection

```{r data-loading}
# ============================================================================
# DATA LOADING
# ============================================================================

load("book_ratings.Rdata")

head(book_info)
str(book_info)
dim(book_info)
head(book_ratings)
str(book_ratings)
dim(book_ratings)
head(user_info) # don't need Age to build recommender, but can include this info if want to go further
str(user_info)
dim(user_info)

# Check for missing values
missing_summary <- data.frame(
  Dataset = c("Book Info", "Book Ratings", "User Info"),
  Missing_Values = c(
    sum(is.na(book_info)),
    sum(is.na(book_ratings)),
    sum(is.na(user_info))   # missing age values
  )
)

kable(missing_summary,
      caption = "Missing Values Summary") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Data Cleaning and Duplicate Handling

```{r data-cleaning}
# ============================================================================
# DUPLICATE DETECTION AND HANDLING
# ============================================================================

# Initial data merging
initial_data <- book_ratings %>%
  left_join(book_info, by = "ISBN")

# Clean book titles and authors for duplicate detection
book_info_clean <- book_info %>%
  mutate(
    clean_title = Book.Title %>%
      tolower() %>%
      gsub('\\(.*?\\)', '', .) %>%
      gsub('\\s*:\\s*.*$', '', .) %>%
      gsub('^(the|a|an)\\s+', '', .) %>%
      gsub('[^a-z0-9\\s-]', '', .) %>%
      gsub('\\s+', ' ', .) %>%
      trimws(),
    clean_author = Book.Author %>%
      tolower() %>%
      gsub('\\s+jr\\.*$', '', .) %>%
      gsub('\\s+sr\\.*$', '', .) %>%
      gsub('\\s+(iii|ii)$', '', .) %>%
      gsub('\\s+', ' ', .) %>%
      trimws()
  )

# Identify duplicates
duplicates <- book_info_clean %>%
  group_by(clean_title, clean_author) %>%
  filter(n() > 1) %>%
  arrange(clean_title, clean_author, ISBN) %>%
  ungroup()

# ============================================================================
# MERGE RATINGS FUNCTION
# ============================================================================

merge_ratings <- function(ratings_data, book_info_data, duplicates_data) {
  
  if (nrow(duplicates_data) > 0) {
    # Create canonical ISBN mapping
    duplicate_mapping <- duplicates_data %>%
      group_by(clean_title, clean_author) %>%
      arrange(ISBN) %>%
      mutate(canonical_ISBN = first(ISBN)) %>%
      ungroup() %>%
      select(ISBN, canonical_ISBN)
    
    # Create complete mapping
    all_books_mapping <- book_info_data %>%
      select(ISBN) %>%
      left_join(duplicate_mapping, by = "ISBN") %>%
      mutate(canonical_ISBN = ifelse(is.na(canonical_ISBN), ISBN, canonical_ISBN))
    
    # Apply mapping to ratings
    ratings_mapped <- ratings_data %>%
      left_join(all_books_mapping, by = "ISBN") %>%
      mutate(
        original_ISBN = ISBN,
        ISBN = canonical_ISBN
      ) %>%
      select(-canonical_ISBN, -original_ISBN)
    
    # Keep highest rating per user-book combination
    ratings_merged <- ratings_mapped %>%
      group_by(User.ID, ISBN) %>%
      slice_max(Book.Rating, n = 1, with_ties = FALSE) %>%
      ungroup()
    
    # Update book info
    book_info_updated <- book_info_data %>%
      left_join(all_books_mapping, by = "ISBN") %>%
      filter(ISBN == canonical_ISBN) %>%
      select(ISBN, Book.Title, Book.Author)
    
  } else {
    ratings_merged <- ratings_data
    book_info_updated <- book_info_data
    all_books_mapping <- NULL
  }
  
  return(list(
    ratings_merged = ratings_merged,
    book_info_updated = book_info_updated,
    mapping = all_books_mapping
  ))
}

# Execute merge
merged_result <- merge_ratings(book_ratings, book_info, duplicates)

# Report results
cat("Original ratings:", nrow(book_ratings), "\n")
cat("Merged ratings:", nrow(merged_result$ratings_merged), "\n")
cat("Ratings removed:", nrow(book_ratings) - nrow(merged_result$ratings_merged), "\n")
cat("Original unique books:", length(unique(book_info$ISBN)), "\n")
cat("Final unique books:", length(unique(merged_result$book_info_updated$ISBN)), "\n")

# Create final clean dataset
data <- merged_result$ratings_merged %>%
  left_join(merged_result$book_info_updated, by = "ISBN") %>%
  select(User.ID, ISBN, Book.Rating, Book.Title, Book.Author)

book_info <- merged_result$book_info_updated

# Verify no duplicates remain
remaining_duplicates <- data %>%
  group_by(User.ID, ISBN) %>%
  filter(n() > 1)

if (nrow(remaining_duplicates) == 0) {
  cat("\n✓ No duplicate user-book pairs remain\n")
} else {
  cat("\n⚠ Warning:", nrow(remaining_duplicates), "duplicates remain\n")
}
```



## Exploratory Data Analysis

```{r eda-summary}
# ============================================================================
# DATASET SUMMARY STATISTICS
# ============================================================================

# Basic statistics
data_summary <- data.frame(
  Metric = c(
    "Total Users",
    "Total Books",
    "Total Ratings",
    "Average Rating",
    "Rating Range",
    "Matrix Sparsity"
  ),
  Value = c(
    length(unique(data$User.ID)),
    length(unique(data$ISBN)),
    nrow(data),
    round(mean(data$Book.Rating, na.rm = TRUE), 2),
    paste(min(data$Book.Rating), "-", max(data$Book.Rating)),
    paste0(
      round(
        (1 - nrow(data) / 
          (length(unique(data$User.ID)) * length(unique(data$ISBN)))) * 100,
        2
      ),
      "%"
    )
  )
)

kable(data_summary,
      caption = "Dataset Overview") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# ============================================================================
# CRITICAL SPARSITY ANALYSIS
# ============================================================================

# Calculate sparsity metrics
n_users <- length(unique(data$User.ID))
n_books <- length(unique(data$ISBN))
n_ratings <- nrow(data)
n_explicit <- sum(data$Book.Rating > 0)

sparsity_analysis <- data.frame(
  Metric = c(
    "Total Possible Ratings",
    "Actual Ratings",
    "Explicit Ratings Only",
    "Overall Sparsity",
    "Explicit-Only Sparsity"
  ),
  Value = c(
    format(n_users * n_books, big.mark = ","),
    format(n_ratings, big.mark = ","),
    format(n_explicit, big.mark = ","),
    paste0(round((1 - n_ratings/(n_users * n_books)) * 100, 2), "%"),
    paste0(round((1 - n_explicit/(n_users * n_books)) * 100, 2), "%")
  )
)

kable(sparsity_analysis,
      caption = "Critical Sparsity Analysis for Collaborative Filtering") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# ============================================================================
# FILTERING THRESHOLD IMPACT ANALYSIS
# ============================================================================

# Show impact of filtering thresholds used in matrix creation
user_activity <- data %>%
  group_by(User.ID) %>%
  summarise(n_ratings = n(), .groups = "drop")

book_activity <- data %>%
  group_by(ISBN) %>%
  summarise(n_ratings = n(), .groups = "drop")

filtering_impact <- data.frame(
  Metric = c(
    "Users with ≥3 ratings",
    "Users dropped (<3 ratings)",
    "Books with ≥5 ratings", 
    "Books dropped (<5 ratings)"
  ),
  Count = c(
    sum(user_activity$n_ratings >= 3),
    sum(user_activity$n_ratings < 3),
    sum(book_activity$n_ratings >= 5),
    sum(book_activity$n_ratings < 5)
  ),
  Percentage = c(
    paste0(round(sum(user_activity$n_ratings >= 3) / nrow(user_activity) * 100, 1), "%"),
    paste0(round(sum(user_activity$n_ratings < 3) / nrow(user_activity) * 100, 1), "%"),
    paste0(round(sum(book_activity$n_ratings >= 5) / nrow(book_activity) * 100, 1), "%"),
    paste0(round(sum(book_activity$n_ratings < 5) / nrow(book_activity) * 100, 1), "%")
  )
)

kable(filtering_impact,
      caption = "Impact of Minimum Rating Thresholds on Data Retention",
      col.names = c("Metric", "Count", "Percentage")) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) 

# Rating distribution with explicit/implicit distinction
rating_dist <- data %>%
  count(Book.Rating) %>%
  mutate(
    Type = ifelse(Book.Rating == 0, "Implicit (Read)", "Explicit (Rated)"),
    Percentage = round(n / sum(n) * 100, 1)
  ) %>%
  select(Rating = Book.Rating, Type, Count = n, Percentage)

kable(rating_dist,
      caption = "Rating Distribution: Explicit vs Implicit Feedback") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) 
```

```{r eda-visualizations, fig.cap="Key Dataset Characteristics for Recommender Systems. Panel A: Distribution of explicit ratings (1-10 scale) showing user rating patterns, with red dashed line indicating the mean rating. Panel B: User activity distribution showing the sparsity challenge in collaborative filtering, where most users interact with few books (long tail) while few users are highly active. Panel C: Relationship between book popularity and average rating quality, with red line showing the trend to understand rating biases and inform cold start problem solutions. Panel D: Proportion of explicit vs implicit feedback in the dataset, where explicit ratings (1-10) are used for model training while implicit ratings (0, read but not rated) are used with confidence weighting."}
# ============================================================================
# FOCUSED VISUALIZATIONS: MOST INFORMATIVE FOR RECOMMENDER SYSTEMS
# ============================================================================

# 1. EXPLICIT RATING DISTRIBUTION (Most Important)
explicit_data <- data %>% filter(Book.Rating > 0)

p1 <- ggplot(explicit_data, aes(x = Book.Rating)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "white", alpha = 0.8) +
  geom_vline(xintercept = mean(explicit_data$Book.Rating), 
             linetype = "dashed", color = "red", size = 1) +
  annotate("text", x = mean(explicit_data$Book.Rating), y = max(table(explicit_data$Book.Rating)) * 0.9, 
           label = paste("Mean =", round(mean(explicit_data$Book.Rating), 1)), 
           color = "red", hjust = -0.1, size = 3.5) +
  labs(
    title = "Distribution of Explicit Ratings",
    x = "Rating",
    y = "Frequency"
  ) +
  theme_cowplot() +
  theme(plot.title = element_text(size = 14, face = "bold"))

# 2. SPARSITY ANALYSIS (Critical for CF)
sparsity_data <- data %>%
  group_by(User.ID) %>%
  summarise(
    total_books_rated = n(),
    explicit_ratings = sum(Book.Rating > 0),
    implicit_ratings = sum(Book.Rating == 0),
    .groups = "drop"
  ) %>%
  mutate(
    explicit_ratio = explicit_ratings / total_books_rated,
    user_type = case_when(
      explicit_ratio == 1 ~ "Only Explicit",
      explicit_ratio == 0 ~ "Only Implicit", 
      explicit_ratio > 0.5 ~ "Mostly Explicit",
      TRUE ~ "Mostly Implicit"
    )
  )

p2 <- ggplot(sparsity_data, aes(x = total_books_rated)) +
  geom_histogram(binwidth = 1, fill = "lightcoral", color = "white", alpha = 0.8) +
  scale_x_continuous(limits = c(0, quantile(sparsity_data$total_books_rated, 0.95))) +
  labs(
    title = "User Activity Distribution",
    x = "Total Books per User",
    y = "Number of Users"
  ) +
  theme_cowplot() +
  theme(plot.title = element_text(size = 14, face = "bold"))

# 3. BOOK POPULARITY (Important for Cold Start)
book_popularity <- data %>%
  group_by(ISBN) %>%
  summarise(
    total_interactions = n(),
    explicit_ratings = sum(Book.Rating > 0),
    avg_rating = mean(Book.Rating[Book.Rating > 0], na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(!is.na(avg_rating))  # Only books with explicit ratings

p3 <- ggplot(book_popularity, aes(x = explicit_ratings, y = avg_rating)) +
  geom_point(alpha = 0.6, color = "darkgreen", size = 2) +
  labs(
    title = "Book Popularity vs Average Rating",
    x = "Number of Explicit Ratings",
    y = "Average Rating"
  ) +
  theme_cowplot() +
  theme(plot.title = element_text(size = 14, face = "bold"))

# 4. EXPLICIT VS IMPLICIT RATIO (Key Insight)
interaction_summary <- data %>%
  summarise(
    total_interactions = n(),
    explicit_count = sum(Book.Rating > 0),
    implicit_count = sum(Book.Rating == 0),
    explicit_pct = round(mean(Book.Rating > 0) * 100, 1),
    implicit_pct = round(mean(Book.Rating == 0) * 100, 1)
  )

# Create a simple bar chart for the ratio
ratio_data <- data.frame(
  Type = c("Explicit Ratings", "Implicit Ratings"),
  Count = c(interaction_summary$explicit_count, interaction_summary$implicit_count),
  Percentage = c(interaction_summary$explicit_pct, interaction_summary$implicit_pct)
)

p4 <- ggplot(ratio_data, aes(x = Type, y = Count, fill = Type)) +
  geom_col(alpha = 0.8) +
  geom_text(aes(label = paste0(Percentage, "%")), vjust = -0.5, size = 4, fontface = "bold") +
  scale_fill_manual(values = c("Explicit Ratings" = "steelblue", "Implicit Ratings" = "lightcoral")) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  labs(
    title = "Explicit vs Implicit Feedback",
    x = "Feedback Type",
    y = "Count"
  ) +
  theme_cowplot() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    legend.position = "none"
  )

# Create the combined plot with legend
combined_plot <- plot_grid(
  p1, p2,
  p3, p4,
  ncol = 2, nrow = 2,
  labels = c("A", "B", "C", "D"),
  label_size = 12
)

# Display the plot
combined_plot
```


# Methodology

## User-Item Matrix Construction

```{r matrix-construction}
# ============================================================================
# USER-ITEM MATRIX CREATION FUNCTION
# ============================================================================

create_user_item_matrix <- function(ratings_data,
                                     min_ratings_per_book = 3,
                                     min_ratings_per_user = 2,
                                     implicit_rating = 4,
                                     implicit_confidence = 0.5) {
  
  # Handle implicit ratings (rating = 0)
  ratings_clean <- ratings_data %>%
    mutate(
      is_implicit = (Book.Rating == 0),
      original_rating = Book.Rating,
      Book.Rating = ifelse(Book.Rating == 0, implicit_rating, Book.Rating),
      confidence = ifelse(is_implicit, implicit_confidence, 1.0)
    )
  
  # Create matrices
  user_item_matrix <- ratings_clean %>%
    select(User.ID, ISBN, Book.Rating) %>%
    pivot_wider(
      names_from = ISBN,
      values_from = Book.Rating,
      values_fill = NA
    )
  
  confidence_matrix <- ratings_clean %>%
    select(User.ID, ISBN, confidence) %>%
    pivot_wider(
      names_from = ISBN,
      values_from = confidence,
      values_fill = NA
    )
  
  original_ratings_matrix <- ratings_clean %>%
    select(User.ID, ISBN, original_rating) %>%
    pivot_wider(
      names_from = ISBN,
      values_from = original_rating,
      values_fill = NA
    )
  
  # Convert to matrix format
  user_ids <- user_item_matrix$User.ID
  user_item_matrix <- as.matrix(user_item_matrix[, -1])
  confidence_matrix <- as.matrix(confidence_matrix[, -1])
  original_ratings_matrix <- as.matrix(original_ratings_matrix[, -1])
  
  rownames(user_item_matrix) <- user_ids
  rownames(confidence_matrix) <- user_ids
  rownames(original_ratings_matrix) <- user_ids
  
  # Filter by minimum ratings
  books_to_keep <- colSums(!is.na(user_item_matrix)) >= min_ratings_per_book
  user_item_matrix <- user_item_matrix[, books_to_keep]
  confidence_matrix <- confidence_matrix[, books_to_keep]
  original_ratings_matrix <- original_ratings_matrix[, books_to_keep]
  
  users_to_keep <- rowSums(!is.na(user_item_matrix)) >= min_ratings_per_user
  user_item_matrix <- user_item_matrix[users_to_keep, ]
  confidence_matrix <- confidence_matrix[users_to_keep, ]
  original_ratings_matrix <- original_ratings_matrix[users_to_keep, ]
  
  return(list(
    ratings = user_item_matrix,
    confidence = confidence_matrix,
    original_ratings = original_ratings_matrix
  ))
}

# ============================================================================
# EVALUATION FUNCTION
# ============================================================================

evaluate_predictions <- function(pred, actual, original_ratings) {
  # Separate explicit and implicit ratings
  is_implicit <- (original_ratings == 0)
  explicit_mask <- !is_implicit & !is.na(actual)
  
  # Calculate RMSE on explicit ratings only
  rmse_explicit <- sqrt(mean((pred[explicit_mask] - actual[explicit_mask])^2, na.rm = TRUE))
  rmse_overall <- sqrt(mean((pred - actual)^2, na.rm = TRUE))
  
  return(list(
    rmse_explicit = rmse_explicit,
    rmse_overall = rmse_overall,
    n_explicit = sum(explicit_mask),
    n_implicit = sum(is_implicit)
  ))
}
```


## Common Functions for All Methods

```{r common-functions}
# ============================================================================
# NORMALIZATION FUNCTIONS
# ============================================================================

# User-mean normalization (for UBCF and NN)
normalize_matrix_user <- function(user_item_matrix) {
  user_means <- rowMeans(user_item_matrix, na.rm = TRUE)
  user_item_matrix_normalized <- sweep(user_item_matrix, 1, user_means, FUN = "-")
  
  return(list(
    normalized = user_item_matrix_normalized,
    means = user_means
  ))
}

# Item-mean normalization (for IBCF)
normalize_matrix_item <- function(user_item_matrix) {
  item_means <- colMeans(user_item_matrix, na.rm = TRUE)
  user_item_matrix_normalized <- sweep(user_item_matrix, 2, item_means, FUN = "-")
  
  return(list(
    normalized = user_item_matrix_normalized,
    means = item_means
  ))
}
```



# Collaborative Filtering Implementations

## User-Based Collaborative Filtering (UBCF)

### Similarity Computation

```{r ubcf-similarity}
# ============================================================================
# UBCF: USER SIMILARITY COMPUTATION
# ============================================================================

compute_user_similarity_matrix <- function(user_item_matrix_normalized,
                                            confidence_matrix) {
  
  # Prepare matrices
  mat <- user_item_matrix_normalized
  conf <- confidence_matrix
  mat[is.na(mat)] <- 0
  conf[is.na(conf)] <- 0
  
  # Weighted ratings
  weighted_mat <- mat * conf
  
  # Compute cosine similarity
  numerator <- weighted_mat %*% t(weighted_mat)
  magnitudes <- sqrt(rowSums(weighted_mat^2))
  denominator <- outer(magnitudes, magnitudes)
  
  user_similarity_matrix <- numerator / denominator
  user_similarity_matrix[is.nan(user_similarity_matrix)] <- 0
  diag(user_similarity_matrix) <- 0
  
  rownames(user_similarity_matrix) <- rownames(user_item_matrix_normalized)
  colnames(user_similarity_matrix) <- rownames(user_item_matrix_normalized)
  
  return(user_similarity_matrix)
}
```

### Recommendation Functions

```{r ubcf-recommendations}
# ============================================================================
# UBCF: RECOMMENDATION FOR EXISTING USERS
# ============================================================================

recommend_for_user_ubcf <- function(target_user,
                                     user_item_matrix,
                                     user_item_matrix_normalized,
                                     confidence_matrix,
                                     user_sim_matrix,
                                     user_means,
                                     book_info,
                                     n_recommendations = 10,
                                     k = NULL) {
  
  target_user <- as.character(target_user)
  
  # Find unrated books
  unrated_books <- colnames(user_item_matrix)[is.na(user_item_matrix[target_user, ])]
  if (length(unrated_books) == 0) return(data.frame())
  
  # Get similarities
  sims <- user_sim_matrix[target_user, ]
  sims[is.na(sims) | is.nan(sims)] <- 0
  
  # Apply k-NN filtering
  if (!is.null(k) && k < length(sims)) {
    non_zero_count <- sum(sims != 0)
    if (non_zero_count > 0) {
      k_actual <- min(k, non_zero_count)
      top_k_users <- names(sort(abs(sims), decreasing = TRUE)[1:k_actual])
      sims_filtered <- rep(0, length(sims))
      names(sims_filtered) <- names(sims)
      sims_filtered[top_k_users] <- sims[top_k_users]
      sims <- sims_filtered
    }
  }
  
  if (sum(abs(sims) > 0) == 0) return(data.frame())
  
  # Prepare weighted matrices
  mat <- user_item_matrix_normalized
  conf <- confidence_matrix
  mat[is.na(mat)] <- 0
  conf[is.na(conf)] <- 0
  weighted_mat <- mat * conf
  
  # Calculate predictions
  weighted_ratings <- t(weighted_mat[, unrated_books, drop = FALSE]) %*% sims
  
  rated_mask <- !is.na(confidence_matrix[, unrated_books, drop = FALSE])
  conf_weighted_sims <- sweep(
    conf[, unrated_books, drop = FALSE] * rated_mask,
    1,
    abs(sims),
    "*"
  )
  sum_weighted_sims <- colSums(conf_weighted_sims)
  sum_weighted_sims[sum_weighted_sims == 0] <- 1
  
  preds <- weighted_ratings / sum_weighted_sims
  preds[is.nan(preds)] <- NA
  preds <- preds + user_means[target_user]
  preds <- as.vector(preds)
  names(preds) <- unrated_books
  preds <- pmin(pmax(preds, 1), 10)
  
  # Get top recommendations
  preds_valid <- preds[!is.na(preds)]
  if (length(preds_valid) == 0) return(data.frame())
  
  top_books <- sort(preds_valid, decreasing = TRUE)[1:min(n_recommendations, length(preds_valid))]
  
  recommendations <- data.frame(
    ISBN = names(top_books),
    Predicted_Rating = as.numeric(top_books)
  ) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}

# ============================================================================
# UBCF: RECOMMENDATION FOR NEW USERS (COLD START)
# ============================================================================

recommend_for_new_user_ubcf <- function(new_user_ratings,
                                         user_item_matrix,
                                         user_item_matrix_normalized,
                                         confidence_matrix,
                                         user_means,
                                         book_info,
                                         n_recommendations = 10,
                                         k = NULL) {
  
  if (length(new_user_ratings) == 0) return(data.frame())
  
  # Create new user vector
  new_user_vector <- rep(NA, ncol(user_item_matrix))
  names(new_user_vector) <- colnames(user_item_matrix)
  
  matched_books <- intersect(names(new_user_ratings), names(new_user_vector))
  if (length(matched_books) == 0) return(data.frame())
  
  new_user_vector[matched_books] <- new_user_ratings[matched_books]
  
  # Normalize
  new_user_mean <- mean(new_user_vector, na.rm = TRUE)
  new_user_normalized <- new_user_vector - new_user_mean
  new_user_normalized[is.na(new_user_normalized)] <- 0
  new_user_conf <- ifelse(is.na(new_user_vector), 0, 1)
  new_user_weighted <- new_user_normalized * new_user_conf
  
  # Prepare matrices
  mat_normalized <- user_item_matrix_normalized
  mat_normalized[is.na(mat_normalized)] <- 0
  conf <- confidence_matrix
  conf[is.na(conf)] <- 0
  mat_weighted <- mat_normalized * conf
  
  # Compute similarities
  existing_magnitudes <- sqrt(rowSums(mat_weighted^2))
  new_user_magnitude <- sqrt(sum(new_user_weighted^2))
  
  if (new_user_magnitude == 0) return(data.frame())
  
  new_user_sims <- as.vector(
    (mat_weighted %*% new_user_weighted) /
      (existing_magnitudes * new_user_magnitude)
  )
  new_user_sims[!is.finite(new_user_sims)] <- 0
  names(new_user_sims) <- rownames(user_item_matrix_normalized)
  
  # Apply k-NN
  if (!is.null(k) && k > 0 && k < length(new_user_sims)) {
    threshold <- sort(abs(new_user_sims), decreasing = TRUE)[min(k, sum(new_user_sims != 0))]
    new_user_sims[abs(new_user_sims) < threshold] <- 0
  }
  
  if (all(new_user_sims == 0)) return(data.frame())
  
  # Make predictions
  unrated_books <- names(new_user_vector)[is.na(new_user_vector)]
  if (length(unrated_books) == 0) return(data.frame())
  
  weighted_ratings <- as.vector(t(mat_weighted[, unrated_books, drop = FALSE]) %*% new_user_sims)
  
  rated_mask <- !is.na(user_item_matrix[, unrated_books, drop = FALSE])
  conf_for_unrated <- conf[, unrated_books, drop = FALSE] * rated_mask
  sum_weighted_sims <- as.vector(t(conf_for_unrated) %*% abs(new_user_sims))
  sum_weighted_sims[sum_weighted_sims == 0] <- 1
  
  preds <- (weighted_ratings / sum_weighted_sims) + new_user_mean
  preds <- pmin(pmax(preds, 1), 10)
  names(preds) <- unrated_books
  
  preds_valid <- preds[!is.na(preds)]
  if (length(preds_valid) == 0) return(data.frame())
  
  n_actual <- min(n_recommendations, length(preds_valid))
  top_indices <- order(preds_valid, decreasing = TRUE)[1:n_actual]
  
  recommendations <- data.frame(
    ISBN = names(preds_valid)[top_indices],
    Predicted_Rating = round(preds_valid[top_indices], 2)
  ) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}
```



### Cross-Validation

```{r ubcf-cv}
# ============================================================================
# UBCF: CROSS-VALIDATION
# ============================================================================

cross_validate_ubcf <- function(user_item_matrix,
                                 confidence_matrix,
                                 original_ratings_matrix,
                                 n_folds = 5,
                                 k = 30) {
  
  set.seed(123)
  
  # Get observed ratings
  observed <- which(!is.na(user_item_matrix), arr.ind = TRUE)
  n_ratings <- nrow(observed)
  fold_indices <- sample(rep(1:n_folds, length.out = n_ratings))
  
  cv_results <- data.frame(
    fold = integer(n_folds),
    rmse = numeric(n_folds)
  )
  
  for (fold in 1:n_folds) {
    cat("Fold", fold, "of", n_folds, "... ")
    
    # Split data
    test_indices <- which(fold_indices == fold)
    test_obs <- observed[test_indices, , drop = FALSE]
    
    # Create training matrices
    train_matrix <- user_item_matrix
    train_confidence <- confidence_matrix
    train_matrix[test_obs] <- NA
    train_confidence[test_obs] <- NA
    
    # Normalize and compute similarity
    norm_result <- normalize_matrix_user(train_matrix)
    train_normalized <- norm_result$normalized
    user_means <- norm_result$means
    user_sim_matrix <- compute_user_similarity_matrix(train_normalized, train_confidence)
    
    # Prepare for prediction
    train_norm_filled <- train_normalized
    train_norm_filled[is.na(train_norm_filled)] <- 0
    train_conf_filled <- train_confidence
    train_conf_filled[is.na(train_conf_filled)] <- 0
    
    # Make predictions
    predictions <- numeric(nrow(test_obs))
    
    for (i in 1:nrow(test_obs)) {
      user_idx <- test_obs[i, 1]
      item_idx <- test_obs[i, 2]
      
      sims <- user_sim_matrix[user_idx, ]
      sims[is.na(sims) | is.nan(sims)] <- 0
      
      # Apply k-NN
      if (k < length(sims) && sum(abs(sims) > 0) > 0) {
        k_actual <- min(k, sum(abs(sims) > 0))
        top_k_idx <- order(abs(sims), decreasing = TRUE)[1:k_actual]
        sims_filtered <- rep(0, length(sims))
        sims_filtered[top_k_idx] <- sims[top_k_idx]
        sims <- sims_filtered
      }
      
      rated_mask <- !is.na(train_matrix[, item_idx])
      
      if (sum(abs(sims[rated_mask])) == 0) {
        predictions[i] <- user_means[user_idx]
        next
      }
      
      # Confidence-weighted prediction
      weighted_ratings <- train_norm_filled[rated_mask, item_idx] *
        train_conf_filled[rated_mask, item_idx]
      weighted_sims <- abs(sims[rated_mask]) *
        train_conf_filled[rated_mask, item_idx]
      
      weighted_sum <- sum(sims[rated_mask] * weighted_ratings)
      sum_weighted_sims <- sum(weighted_sims)
      
      if (sum_weighted_sims > 0) {
        predictions[i] <- (weighted_sum / sum_weighted_sims) + user_means[user_idx]
      } else {
        predictions[i] <- user_means[user_idx]
      }
    }
    
    predictions <- pmin(pmax(predictions, 1), 10)
    actual <- user_item_matrix[test_obs]
    original_ratings <- original_ratings_matrix[test_obs]
    metrics <- evaluate_predictions(predictions, actual, original_ratings)
    
    cv_results[fold, ] <- list(fold, metrics$rmse_explicit)
    cat("RMSE:", round(metrics$rmse_explicit, 4), "\n")
  }
  
  return(cv_results)
}
```

### Implementation

```{r ubcf-implementation}
# ============================================================================
# UBCF: MODEL TRAINING AND EVALUATION
# ============================================================================

# Create matrices
matrix_result <- create_user_item_matrix(
  data,
  min_ratings_per_book = 5,
  min_ratings_per_user = 3,
  implicit_rating = 4,
  implicit_confidence = 0.5
)

user_item_matrix_ubcf <- matrix_result$ratings
confidence_matrix_ubcf <- matrix_result$confidence
original_ratings_matrix_ubcf <- matrix_result$original_ratings

cat("=== UBCF Matrix Information ===\n")
cat("Users:", nrow(user_item_matrix_ubcf), "\n")
cat("Items:", ncol(user_item_matrix_ubcf), "\n")
cat("Sparsity:", round(mean(is.na(user_item_matrix_ubcf)) * 100, 2), "%\n\n")

# Normalize and compute similarity
normalized_result_ubcf <- normalize_matrix_user(user_item_matrix_ubcf)
user_item_matrix_normalized_ubcf <- normalized_result_ubcf$normalized
user_means <- normalized_result_ubcf$means

user_similarity_matrix <- compute_user_similarity_matrix(
  user_item_matrix_normalized_ubcf,
  confidence_matrix_ubcf
)

# Generate recommendations for existing user
sample_user <- rownames(user_item_matrix_ubcf)[3]

recs_ubcf <- recommend_for_user_ubcf(
  target_user = sample_user,
  user_item_matrix = user_item_matrix_ubcf,
  user_item_matrix_normalized = user_item_matrix_normalized_ubcf,
  confidence_matrix = confidence_matrix_ubcf,
  user_sim_matrix = user_similarity_matrix,
  user_means = user_means,
  book_info = book_info,
  n_recommendations = 10,
  k = 50
)

recs_ubcf %>%
  mutate(Rank = row_number()) %>%
  select(Rank, Book.Title, Book.Author, Predicted_Rating) %>%
  kable(
    caption = "UBCF: Top 10 Recommendations for Existing User",
    digits = 2,
    align = c("c", "l", "l", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# Generate recommendations for new user (cold start)
sample_books <- colnames(user_item_matrix_ubcf)[1:5]
new_user_ratings <- setNames(c(7, 8, 6, 5, 7), sample_books)

new_user_recs <- recommend_for_new_user_ubcf(
  new_user_ratings = new_user_ratings,
  user_item_matrix = user_item_matrix_ubcf,
  user_item_matrix_normalized = user_item_matrix_normalized_ubcf,
  confidence_matrix = confidence_matrix_ubcf,
  user_means = user_means,
  book_info = book_info,
  n_recommendations = 10,
  k = 50
)

new_user_recs %>%
  mutate(Rank = row_number()) %>%
  select(Rank, Book.Title, Book.Author, Predicted_Rating) %>%
  kable(
    caption = "UBCF: Top 10 Recommendations for New User (Cold Start)",
    digits = 2,
    align = c("c", "l", "l", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# Run cross-validation
cv_results_ubcf <- cross_validate_ubcf(
  user_item_matrix_ubcf,
  confidence_matrix_ubcf,
  original_ratings_matrix_ubcf,
  n_folds = 5,
  k = 50
)

cv_results_ubcf %>%
  kable(
    caption = "UBCF: Cross-Validation Results",
    digits = 4,
    col.names = c("Fold", "RMSE"),
    align = c("c", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

cat("\n=== UBCF Performance Summary ===\n")
cat("Mean RMSE:", round(mean(cv_results_ubcf$rmse), 4), "\n")
cat("SD RMSE:", round(sd(cv_results_ubcf$rmse), 4), "\n")
```



## Item-Based Collaborative Filtering (IBCF)

### Similarity Computation

```{r ibcf-similarity}
# ============================================================================
# IBCF: ITEM SIMILARITY COMPUTATION
# ============================================================================

compute_item_similarity_matrix <- function(user_item_matrix_normalized,
                                            confidence_matrix) {
  
  # Prepare matrices
  mat <- user_item_matrix_normalized
  mat[is.na(mat)] <- 0
  conf <- confidence_matrix
  conf[is.na(conf)] <- 0
  
  # Transpose for item-item similarity
  mat_t <- t(mat)
  conf_t <- t(conf)
  
  # Weighted ratings
  weighted_mat_t <- mat_t * conf_t
  
  # Compute cosine similarity
  numerator <- weighted_mat_t %*% t(weighted_mat_t)
  magnitudes <- sqrt(rowSums(weighted_mat_t^2))
  denominator <- outer(magnitudes, magnitudes)
  
  item_similarity_matrix <- numerator / denominator
  item_similarity_matrix[is.nan(item_similarity_matrix)] <- 0
  diag(item_similarity_matrix) <- 0
  
  rownames(item_similarity_matrix) <- colnames(user_item_matrix_normalized)
  colnames(item_similarity_matrix) <- colnames(user_item_matrix_normalized)
  
  return(item_similarity_matrix)
}
```

### Recommendation Functions

```{r ibcf-recommendations}
# ============================================================================
# IBCF: RECOMMENDATION FOR EXISTING USERS
# ============================================================================

recommend_for_user_ibcf <- function(target_user,
                                     user_item_matrix,
                                     user_item_matrix_normalized,
                                     confidence_matrix,
                                     item_sim_matrix,
                                     item_means,
                                     book_info,
                                     n_recommendations = 10,
                                     k = NULL) {
  
  target_user <- as.character(target_user)
  
  # Find unrated books
  unrated_books <- colnames(user_item_matrix)[is.na(user_item_matrix[target_user, ])]
  if (length(unrated_books) == 0) return(data.frame())
  
  # Get item similarities
  item_sims <- item_sim_matrix[unrated_books, , drop = FALSE]
  item_sims[is.na(item_sims) | is.nan(item_sims)] <- 0
  
  # Apply k-NN filtering
  if (!is.null(k) && k < ncol(item_sims)) {
    for (i in 1:nrow(item_sims)) {
      sims <- item_sims[i, ]
      non_zero_count <- sum(sims != 0)
      if (non_zero_count > 0) {
        k_actual <- min(k, non_zero_count)
        top_k_items <- names(sort(abs(sims), decreasing = TRUE)[1:k_actual])
        sims_filtered <- rep(0, length(sims))
        names(sims_filtered) <- names(sims)
        sims_filtered[top_k_items] <- sims[top_k_items]
        item_sims[i, ] <- sims_filtered
      }
    }
  }
  
  if (sum(abs(item_sims) > 0) == 0) return(data.frame())
  
  # Prepare weighted matrices
  mat <- user_item_matrix_normalized
  conf <- confidence_matrix
  mat[is.na(mat)] <- 0
  conf[is.na(conf)] <- 0
  weighted_mat <- mat * conf
  
  # Calculate predictions
  weighted_ratings <- item_sims %*% weighted_mat[target_user, ]
  
  rated_mask <- !is.na(user_item_matrix[target_user, ])
  rated_item_mask <- matrix(
    rated_mask,
    nrow = nrow(item_sims),
    ncol = length(rated_mask),
    byrow = TRUE
  )
  
  conf_weighted_sims <- rated_item_mask * abs(item_sims) * conf[target_user, ]
  sum_weighted_sims <- rowSums(conf_weighted_sims)
  sum_weighted_sims[sum_weighted_sims == 0] <- 1
  
  preds <- weighted_ratings / sum_weighted_sims
  preds[is.nan(preds)] <- NA
  preds <- preds + item_means[unrated_books]
  preds <- as.vector(preds)
  names(preds) <- unrated_books
  preds <- pmin(pmax(preds, 1), 10)
  
  # Get top recommendations
  preds_valid <- preds[!is.na(preds)]
  if (length(preds_valid) == 0) return(data.frame())
  
  top_books <- sort(preds_valid, decreasing = TRUE)[1:min(n_recommendations, length(preds_valid))]
  
  recommendations <- data.frame(
    ISBN = names(top_books),
    Predicted_Rating = as.numeric(top_books)
  ) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}

# ============================================================================
# IBCF: RECOMMENDATION FOR NEW USERS (COLD START)
# ============================================================================

recommend_for_new_user_ibcf <- function(new_user_ratings,
                                         user_item_matrix,
                                         user_item_matrix_normalized,
                                         confidence_matrix,
                                         item_means,
                                         item_sim_matrix,
                                         book_info,
                                         n_recommendations = 10,
                                         k = NULL) {
  
  if (length(new_user_ratings) == 0) return(data.frame())
  
  # Create new user vector
  new_user_vector <- rep(NA, ncol(user_item_matrix))
  names(new_user_vector) <- colnames(user_item_matrix)
  
  matched_books <- intersect(names(new_user_ratings), names(new_user_vector))
  if (length(matched_books) == 0) return(data.frame())
  
  new_user_vector[matched_books] <- new_user_ratings[matched_books]
  
  # Normalize
  new_user_normalized <- new_user_vector - item_means
  new_user_normalized[is.na(new_user_normalized)] <- 0
  new_user_conf <- ifelse(is.na(new_user_vector), 0, 1)
  new_user_weighted <- new_user_normalized * new_user_conf
  
  # Find unrated books
  unrated_books <- names(new_user_vector)[is.na(new_user_vector)]
  if (length(unrated_books) == 0) return(data.frame())
  
  # Get item similarities
  item_sims <- item_sim_matrix[unrated_books, , drop = FALSE]
  item_sims[is.na(item_sims) | is.nan(item_sims)] <- 0
  
  # Apply k-NN filtering
  if (!is.null(k) && k < ncol(item_sims)) {
    for (i in 1:nrow(item_sims)) {
      sims <- item_sims[i, ]
      non_zero_count <- sum(sims != 0)
      if (non_zero_count > 0) {
        k_actual <- min(k, non_zero_count)
        top_k_items <- names(sort(abs(sims), decreasing = TRUE)[1:k_actual])
        sims_filtered <- rep(0, length(sims))
        names(sims_filtered) <- names(sims)
        sims_filtered[top_k_items] <- sims[top_k_items]
        item_sims[i, ] <- sims_filtered
      }
    }
  }
  
  if (sum(abs(item_sims) > 0) == 0) return(data.frame())
  
  # Make predictions
  weighted_ratings <- item_sims %*% new_user_weighted
  
  rated_mask <- !is.na(new_user_vector)
  rated_item_mask <- matrix(
    rated_mask,
    nrow = nrow(item_sims),
    ncol = length(rated_mask),
    byrow = TRUE
  )
  
  conf_weighted_sims <- rated_item_mask * abs(item_sims) * new_user_conf
  sum_weighted_sims <- rowSums(conf_weighted_sims)
  sum_weighted_sims[sum_weighted_sims == 0] <- 1
  
  preds <- weighted_ratings / sum_weighted_sims
  preds[is.nan(preds)] <- NA
  preds <- preds + item_means[unrated_books]
  preds <- as.vector(preds)
  names(preds) <- unrated_books
  preds <- pmin(pmax(preds, 1), 10)
  
  preds_valid <- preds[!is.na(preds)]
  if (length(preds_valid) == 0) return(data.frame())
  
  n_actual <- min(n_recommendations, length(preds_valid))
  top_indices <- order(preds_valid, decreasing = TRUE)[1:n_actual]
  
  recommendations <- data.frame(
    ISBN = names(preds_valid)[top_indices],
    Predicted_Rating = round(preds_valid[top_indices], 2)
  ) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}
```

### Cross-Validation

```{r ibcf-cv}
# ============================================================================
# IBCF: CROSS-VALIDATION
# ============================================================================

cross_validate_ibcf <- function(user_item_matrix,
                                 confidence_matrix,
                                 original_ratings_matrix,
                                 n_folds = 5,
                                 k = 30) {
  
  set.seed(123)
  
  # Get observed ratings
  observed <- which(!is.na(user_item_matrix), arr.ind = TRUE)
  n_ratings <- nrow(observed)
  fold_indices <- sample(rep(1:n_folds, length.out = n_ratings))
  
  cv_results <- data.frame(
    fold = integer(n_folds),
    rmse = numeric(n_folds)
  )
  
  for (fold in 1:n_folds) {
    cat("Fold", fold, "of", n_folds, "... ")
    
    # Split data
    test_indices <- which(fold_indices == fold)
    test_obs <- observed[test_indices, , drop = FALSE]
    
    # Create training matrices
    train_matrix <- user_item_matrix
    train_confidence <- confidence_matrix
    train_matrix[test_obs] <- NA
    train_confidence[test_obs] <- NA
    
    # Normalize and compute similarity
    norm_result <- normalize_matrix_item(train_matrix)
    train_normalized <- norm_result$normalized
    item_means <- norm_result$means
    item_sim_matrix <- compute_item_similarity_matrix(train_normalized, train_confidence)
    
    # Prepare for prediction
    train_norm_filled <- train_normalized
    train_norm_filled[is.na(train_norm_filled)] <- 0
    train_conf_filled <- train_confidence
    train_conf_filled[is.na(train_conf_filled)] <- 0
    
    # Make predictions
    predictions <- numeric(nrow(test_obs))
    
    for (i in 1:nrow(test_obs)) {
      user_idx <- test_obs[i, 1]
      item_idx <- test_obs[i, 2]
      
      rated_items <- which(!is.na(train_matrix[user_idx, ]))
      
      if (length(rated_items) == 0) {
        predictions[i] <- item_means[item_idx]
        next
      }
      
      sims <- item_sim_matrix[item_idx, rated_items]
      sims[is.na(sims) | is.nan(sims)] <- 0
      
      # Apply k-NN
      if (sum(abs(sims) > 0) > 0 && k < length(sims)) {
        k_actual <- min(k, sum(abs(sims) > 0))
        top_k_idx <- order(abs(sims), decreasing = TRUE)[1:k_actual]
        sims_filtered <- rep(0, length(sims))
        sims_filtered[top_k_idx] <- sims[top_k_idx]
        sims <- sims_filtered
      }
      
      # Confidence-weighted prediction
      if (sum(abs(sims)) > 0) {
        weighted_ratings <- train_norm_filled[user_idx, rated_items] *
          train_conf_filled[user_idx, rated_items]
        weighted_sims <- abs(sims) * train_conf_filled[user_idx, rated_items]
        
        weighted_sum <- sum(sims * weighted_ratings)
        sum_weighted_sims <- sum(weighted_sims)
        
        if (sum_weighted_sims > 0) {
          predictions[i] <- (weighted_sum / sum_weighted_sims) + item_means[item_idx]
        } else {
          predictions[i] <- item_means[item_idx]
        }
      } else {
        predictions[i] <- item_means[item_idx]
      }
    }
    
    predictions <- pmin(pmax(predictions, 1), 10)
    actual <- user_item_matrix[test_obs]
    original_ratings <- original_ratings_matrix[test_obs]
    metrics <- evaluate_predictions(predictions, actual, original_ratings)
    
    cv_results[fold, ] <- list(fold, metrics$rmse_explicit)
    cat("RMSE:", round(metrics$rmse_explicit, 4), "\n")
  }
  
  return(cv_results)
}
```

### Implementation

```{r ibcf-implementation}
# ============================================================================
# IBCF: MODEL TRAINING AND EVALUATION
# ============================================================================

# Create matrices
matrix_result_ibcf <- create_user_item_matrix(
  data,
  min_ratings_per_book = 5,
  min_ratings_per_user = 3,
  implicit_rating = 4,
  implicit_confidence = 0.5
)

user_item_matrix_ibcf <- matrix_result_ibcf$ratings
confidence_matrix_ibcf <- matrix_result_ibcf$confidence
original_ratings_matrix_ibcf <- matrix_result_ibcf$original_ratings

cat("=== IBCF Matrix Information ===\n")
cat("Users:", nrow(user_item_matrix_ibcf), "\n")
cat("Items:", ncol(user_item_matrix_ibcf), "\n")
cat("Sparsity:", round(mean(is.na(user_item_matrix_ibcf)) * 100, 2), "%\n\n")

# Normalize and compute similarity
normalized_result_ibcf <- normalize_matrix_item(user_item_matrix_ibcf)
user_item_matrix_normalized_ibcf <- normalized_result_ibcf$normalized
item_means <- normalized_result_ibcf$means

item_similarity_matrix <- compute_item_similarity_matrix(
  user_item_matrix_normalized_ibcf,
  confidence_matrix_ibcf
)

# Generate recommendations for existing user
sample_user <- rownames(user_item_matrix_ibcf)[3]

recs_ibcf <- recommend_for_user_ibcf(
  target_user = sample_user,
  user_item_matrix = user_item_matrix_ibcf,
  user_item_matrix_normalized = user_item_matrix_normalized_ibcf,
  confidence_matrix = confidence_matrix_ibcf,
  item_sim_matrix = item_similarity_matrix,
  item_means = item_means,
  book_info = book_info,
  n_recommendations = 10,
  k = 50
)

recs_ibcf %>%
  mutate(Rank = row_number()) %>%
  select(Rank, Book.Title, Book.Author, Predicted_Rating) %>%
  kable(
    caption = "IBCF: Top 10 Recommendations for Existing User",
    digits = 2,
    align = c("c", "l", "l", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# Generate recommendations for new user (cold start)
sample_books <- colnames(user_item_matrix_ibcf)[1:5]
new_user_ratings <- setNames(c(7, 8, 6, 5, 7), sample_books)

new_user_recs_ibcf <- recommend_for_new_user_ibcf(
  new_user_ratings = new_user_ratings,
  user_item_matrix = user_item_matrix_ibcf,
  user_item_matrix_normalized = user_item_matrix_normalized_ibcf,
  confidence_matrix = confidence_matrix_ibcf,
  item_means = item_means,
  item_sim_matrix = item_similarity_matrix,
  book_info = book_info,
  n_recommendations = 10,
  k = 50
)

new_user_recs_ibcf %>%
  mutate(Rank = row_number()) %>%
  select(Rank, Book.Title, Book.Author, Predicted_Rating) %>%
  kable(
    caption = "IBCF: Top 10 Recommendations for New User (Cold Start)",
    digits = 2,
    align = c("c", "l", "l", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# Run cross-validation
cv_results_ibcf <- cross_validate_ibcf(
  user_item_matrix_ibcf,
  confidence_matrix_ibcf,
  original_ratings_matrix_ibcf,
  n_folds = 5,
  k = 50
)

cv_results_ibcf %>%
  kable(
    caption = "IBCF: Cross-Validation Results",
    digits = 4,
    col.names = c("Fold", "RMSE"),
    align = c("c", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

cat("\n=== IBCF Performance Summary ===\n")
cat("Mean RMSE:", round(mean(cv_results_ibcf$rmse), 4), "\n")
cat("SD RMSE:", round(sd(cv_results_ibcf$rmse), 4), "\n")
```


## Matrix Factorization

### Data Preparation

```{r mf-data-prep}
# ============================================================================
# MF: DATA PREPARATION (EXPLICIT RATINGS ONLY)
# ============================================================================

create_explicit_matrix <- function(ratings_data,
                                    min_ratings_per_book = 3,
                                    min_ratings_per_user = 2) {
  
  # Filter explicit ratings only
  explicit_ratings <- ratings_data %>%
    filter(Book.Rating > 0) %>%
    select(User.ID, ISBN, Book.Rating)
  
  # Create user-item matrix
  user_item_matrix <- explicit_ratings %>%
    pivot_wider(
      names_from = ISBN,
      values_from = Book.Rating,
      values_fill = NA
    )
  
  # Convert to matrix
  user_ids <- user_item_matrix$User.ID
  user_item_matrix <- as.matrix(user_item_matrix[, -1])
  rownames(user_item_matrix) <- user_ids
  
  # Filter by minimum ratings
  books_to_keep <- colSums(!is.na(user_item_matrix)) >= min_ratings_per_book
  user_item_matrix <- user_item_matrix[, books_to_keep]
  
  users_to_keep <- rowSums(!is.na(user_item_matrix)) >= min_ratings_per_user
  user_item_matrix <- user_item_matrix[users_to_keep, ]
  
  return(user_item_matrix)
}

prepare_recosystem_data <- function(user_item_matrix, original_ratings_matrix) {
  observed <- which(!is.na(user_item_matrix), arr.ind = TRUE)
  
  train_data <- data.frame(
    user_index = observed[, 1] - 1,
    item_index = observed[, 2] - 1,
    rating = user_item_matrix[observed],
    original_rating = original_ratings_matrix[observed]
  )
  
  # Filter explicit ratings only
  train_data_explicit <- train_data[train_data$original_rating != 0, ]
  
  user_ids <- data.frame(
    user_index = 0:(nrow(user_item_matrix) - 1),
    user_id = rownames(user_item_matrix)
  )
  
  item_ids <- data.frame(
    item_index = 0:(ncol(user_item_matrix) - 1),
    item_id = colnames(user_item_matrix)
  )
  
  list(
    train_data = train_data_explicit,
    user_ids = user_ids,
    item_ids = item_ids,
    n_users = nrow(user_item_matrix),
    n_items = ncol(user_item_matrix)
  )
}
```

### Model Training

```{r mf-training}
# ============================================================================
# MF: HYPERPARAMETER TUNING
# ============================================================================

tune_reco_model <- function(train_data,
                             n_factors = c(10, 20, 30),
                             learning_rate = c(0.1, 0.05, 0.01),
                             costp_l2 = c(0.01, 0.1),
                             costq_l2 = c(0.01, 0.1),
                             n_iter = 50,
                             verbose = TRUE) {
  
  train_set <- data_memory(
    user_index = train_data$user_index,
    item_index = train_data$item_index,
    rating = train_data$rating,
    index1 = FALSE
  )
  
  rs <- Reco()
  
  opts <- rs$tune(train_set, opts = list(
    dim = n_factors,
    lrate = learning_rate,
    costp_l2 = costp_l2,
    costq_l2 = costq_l2,
    niter = n_iter,
    nthread = 4,
    verbose = verbose
  ))
  
  return(opts)
}

# ============================================================================
# MF: MODEL TRAINING
# ============================================================================

train_reco_model <- function(train_data,
                              n_factors = 20,
                              learning_rate = 0.1,
                              costp_l2 = 0.01,
                              costq_l2 = 0.01,
                              n_iter = 100,
                              n_threads = 4,
                              verbose = TRUE) {
  
  train_set <- data_memory(
    user_index = train_data$user_index,
    item_index = train_data$item_index,
    rating = train_data$rating,
    index1 = FALSE
  )
  
  model <- Reco()
  model$train(train_set, opts = list(
    dim = n_factors,
    lrate = learning_rate,
    costp_l2 = costp_l2,
    costq_l2 = costq_l2,
    niter = n_iter,
    nthread = n_threads,
    verbose = verbose
  ))
  
  return(model)
}
```

### Recommendation Functions

```{r mf-recommendations}
# ============================================================================
# MF: RECOMMENDATION FOR EXISTING USERS
# ============================================================================

recommend_for_user_mf <- function(model,
                                   user_item_matrix,
                                   user_id,
                                   n_recommendations = 10,
                                   user_ids_map,
                                   item_ids_map,
                                   book_info) {
  
  if (!user_id %in% user_ids_map$user_id) stop("User not found")
  
  user_idx <- user_ids_map$user_index[user_ids_map$user_id == user_id]
  user_row <- user_item_matrix[as.character(user_id), ]
  unrated_items <- which(is.na(user_row))
  
  if (length(unrated_items) == 0) return(data.frame())
  
  item_indices <- unrated_items - 1
  pred_set <- data_memory(
    user_index = rep(user_idx, length(item_indices)),
    item_index = item_indices,
    index1 = FALSE
  )
  
  pred_ratings <- model$predict(pred_set, out_memory())
  pred_ratings <- pmax(pmin(pred_ratings, 10), 1)
  
  top_indices <- order(pred_ratings, decreasing = TRUE)[1:min(n_recommendations, length(pred_ratings))]
  recommended_items <- colnames(user_item_matrix)[unrated_items[top_indices]]
  
  data.frame(
    ISBN = recommended_items,
    Predicted_Rating = round(pred_ratings[top_indices], 2)
  ) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
}

# ============================================================================
# MF: RECOMMENDATION FOR NEW USERS (COLD START)
# ============================================================================

recommend_for_new_user_mf <- function(model,
                                       new_user_ratings,
                                       user_item_matrix,
                                       item_ids_map,
                                       book_info,
                                       n_recommendations = 10) {
  
  # Hybrid approach for cold start
  ratings_df <- if (is.numeric(new_user_ratings) && !is.null(names(new_user_ratings))) {
    data.frame(item_id = names(new_user_ratings), rating = as.numeric(new_user_ratings))
  } else {
    new_user_ratings
  }
  
  ratings_df <- ratings_df[ratings_df$item_id %in% item_ids_map$item_id, ]
  if (nrow(ratings_df) == 0) stop("No rated items exist in training data")
  
  # Calculate item means for fallback
  item_means <- colMeans(user_item_matrix, na.rm = TRUE)
  
  # Get unrated items
  all_items <- item_ids_map$item_id
  rated_items <- ratings_df$item_id
  unrated_items <- setdiff(all_items, rated_items)
  
  if (length(unrated_items) == 0) return(data.frame())
  
  # Use item means adjusted by user bias
  user_mean_rating <- mean(ratings_df$rating)
  global_mean_rating <- mean(user_item_matrix, na.rm = TRUE)
  user_bias <- user_mean_rating - global_mean_rating
  
  pred_ratings <- item_means[unrated_items] + user_bias
  pred_ratings <- pmax(pmin(pred_ratings, 10), 1)
  
  top_indices <- order(pred_ratings, decreasing = TRUE)[1:min(n_recommendations, length(pred_ratings))]
  recommended_items <- names(pred_ratings)[top_indices]
  
  data.frame(
    ISBN = recommended_items,
    Predicted_Rating = round(pred_ratings[top_indices], 2)
  ) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
}
```

### Cross-Validation

```{r mf-cv}
# ============================================================================
# MF: CROSS-VALIDATION
# ============================================================================

cross_validate_mf <- function(user_item_matrix,
                               original_ratings_matrix,
                               optimal_params,
                               n_folds = 5,
                               n_iter = 50,
                               verbose = TRUE) {
  
  set.seed(123)
  
  # Get observed ratings
  observed <- which(!is.na(user_item_matrix), arr.ind = TRUE)
  n_ratings <- nrow(observed)
  
  if (n_ratings < n_folds * 2) {
    stop("Not enough ratings to perform cross-validation.")
  }
  
  fold_indices <- sample(rep(1:n_folds, length.out = n_ratings))
  
  cv_results <- data.frame(
    Fold = integer(n_folds),
    RMSE = numeric(n_folds)
  )
  
  if (verbose) {
    cat("Starting", n_folds, "-fold cross-validation...\n")
  }
  
  for (fold in 1:n_folds) {
    if (verbose) cat("Fold", fold, "of", n_folds, "... ")
    
    # Split data
    test_idx <- which(fold_indices == fold)
    train_idx <- which(fold_indices != fold)
    
    train_obs <- observed[train_idx, , drop = FALSE]
    test_obs <- observed[test_idx, , drop = FALSE]
    
    # Prepare training data (explicit only)
    train_data <- data.frame(
      user_index = train_obs[, 1] - 1,
      item_index = train_obs[, 2] - 1,
      rating = user_item_matrix[train_obs],
      original_rating = original_ratings_matrix[train_obs]
    )
    
    train_data_explicit <- train_data[train_data$original_rating != 0, ]
    
    # Prepare test data (explicit only)
    test_data <- data.frame(
      user_index = test_obs[, 1] - 1,
      item_index = test_obs[, 2] - 1,
      rating = user_item_matrix[test_obs],
      original_rating = original_ratings_matrix[test_obs]
    )
    
    test_data_explicit <- test_data[test_data$original_rating != 0, ]
    
    if (nrow(test_data_explicit) == 0) {
      cv_results[fold, ] <- c(fold, NA)
      next
    }
    
    # Train model
    train_set <- data_memory(
      user_index = train_data_explicit$user_index,
      item_index = train_data_explicit$item_index,
      rating = train_data_explicit$rating,
      index1 = FALSE
    )
    
    test_set <- data_memory(
      user_index = test_data_explicit$user_index,
      item_index = test_data_explicit$item_index,
      rating = test_data_explicit$rating,
      index1 = FALSE
    )
    
    r <- Reco()
    r$train(train_set, opts = list(
      dim = optimal_params$min$dim,
      lrate = optimal_params$min$lrate,
      costp_l2 = optimal_params$min$costp_l2,
      costq_l2 = optimal_params$min$costq_l2,
      niter = n_iter,
      nthread = 4,
      verbose = FALSE
    ))
    
    preds <- r$predict(test_set, out_memory())
    preds <- pmax(pmin(preds, 10), 1)
    
    # Calculate RMSE
    actual <- test_data_explicit$rating
    original <- test_data_explicit$original_rating
    metrics <- evaluate_predictions(preds, actual, original)
    
    cv_results[fold, ] <- c(fold, metrics$rmse_explicit)
    cat("RMSE:", round(metrics$rmse_explicit, 4), "\n")
  }
  
  return(cv_results)
}
```

### Implementation

```{r mf-implementation}
# ============================================================================
# MF: MODEL TRAINING AND EVALUATION
# ============================================================================

# Create matrices
matrix_result_mf <- create_user_item_matrix(
  data,
  min_ratings_per_book = 5,
  min_ratings_per_user = 3,
  implicit_rating = 4,
  implicit_confidence = 0.5
)

user_item_matrix_mf <- matrix_result_mf$ratings
original_ratings_matrix_mf <- matrix_result_mf$original_ratings

# Prepare data for recosystem
prepared <- prepare_recosystem_data(user_item_matrix_mf, original_ratings_matrix_mf)

cat("=== MF Matrix Information ===\n")
cat("Total ratings in matrix:", sum(!is.na(user_item_matrix_mf)), "\n")
cat("Explicit ratings used:", nrow(prepared$train_data), "\n")
cat("Implicit ratings excluded:", 
    sum(!is.na(user_item_matrix_mf)) - nrow(prepared$train_data), "\n\n")

# Hyperparameter tuning
optimal_params_mf <- tune_reco_model(
  train_data = prepared$train_data,
  n_factors = c(10, 20, 30),
  learning_rate = c(0.1, 0.05, 0.01),
  costp_l2 = c(0.01, 0.1),
  costq_l2 = c(0.01, 0.1),
  n_iter = 50,
  verbose = FALSE
)

cat("=== Optimal Hyperparameters ===\n")
cat("Factors:", optimal_params_mf$min$dim, "\n")
cat("Learning rate:", optimal_params_mf$min$lrate, "\n")
cat("User regularization:", optimal_params_mf$min$costp_l2, "\n")
cat("Item regularization:", optimal_params_mf$min$costq_l2, "\n\n")

# Train final model
model_mf <- train_reco_model(
  train_data = prepared$train_data,
  n_factors = optimal_params_mf$min$dim,
  learning_rate = optimal_params_mf$min$lrate,
  costp_l2 = optimal_params_mf$min$costp_l2,
  costq_l2 = optimal_params_mf$min$costq_l2,
  n_iter = 100,
  n_threads = 4,
  verbose = FALSE
)

cat("✓ Model training complete!\n\n")

# Generate recommendations for existing user
sample_user <- rownames(user_item_matrix_mf)[3]

recs_mf <- recommend_for_user_mf(
  model = model_mf,
  user_item_matrix = user_item_matrix_mf,
  user_id = sample_user,
  n_recommendations = 10,
  user_ids_map = prepared$user_ids,
  item_ids_map = prepared$item_ids,
  book_info = book_info
)

recs_mf %>%
  mutate(Rank = row_number()) %>%
  select(Rank, Book.Title, Book.Author, Predicted_Rating) %>%
  kable(
    caption = "MF: Top 10 Recommendations for Existing User",
    digits = 2,
    align = c("c", "l", "l", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# Generate recommendations for new user (cold start)
sample_books <- colnames(user_item_matrix_mf)[1:5]
new_user_ratings <- setNames(c(7, 8, 6, 5, 7), sample_books)

recs_new_mf <- recommend_for_new_user_mf(
  model = model_mf,
  new_user_ratings = new_user_ratings,
  user_item_matrix = user_item_matrix_mf,
  item_ids_map = prepared$item_ids,
  book_info = book_info,
  n_recommendations = 10
)

recs_new_mf %>%
  mutate(Rank = row_number()) %>%
  select(Rank, Book.Title, Book.Author, Predicted_Rating) %>%
  kable(
    caption = "MF: Top 10 Recommendations for New User (Cold Start)",
    digits = 2,
    align = c("c", "l", "l", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# Run cross-validation
cv_results_mf <- cross_validate_mf(
  user_item_matrix = user_item_matrix_mf,
  original_ratings_matrix = original_ratings_matrix_mf,
  optimal_params = optimal_params_mf,
  n_folds = 5,
  n_iter = 50,
  verbose = TRUE
)

cv_results_mf %>%
  kable(
    caption = "MF: Cross-Validation Results",
    digits = 4,
    col.names = c("Fold", "RMSE"),
    align = c("c", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

cat("\n=== MF Performance Summary ===\n")
cat("Mean RMSE:", round(mean(cv_results_mf$RMSE, na.rm = TRUE), 4), "\n")
cat("SD RMSE:", round(sd(cv_results_mf$RMSE, na.rm = TRUE), 4), "\n")
```



## Neural Network-Based Collaborative Filtering

### Data Preparation

```{r nn-data-prep}
# ============================================================================
# NN: DATA PREPARATION
# ============================================================================

prepare_h2o_data <- function(user_item_matrix, original_ratings_matrix = NULL) {
  
  # Apply user-mean normalization
  user_means <- rowMeans(user_item_matrix, na.rm = TRUE)
  normalized_matrix <- sweep(user_item_matrix, 1, user_means, FUN = "-")
  
  # Get observed ratings
  observed <- which(!is.na(normalized_matrix), arr.ind = TRUE)
  
  # Create training data
  train_data <- data.frame(
    user_id = rownames(user_item_matrix)[observed[, 1]],
    book_id = colnames(user_item_matrix)[observed[, 2]],
    rating = as.numeric(normalized_matrix[observed])
  )
  
  # Add original rating information
  if (!is.null(original_ratings_matrix)) {
    train_data$original_rating <- as.numeric(original_ratings_matrix[observed])
    train_data$is_implicit <- (train_data$original_rating == 0)
  } else {
    train_data$original_rating <- train_data$rating + user_means[observed[, 1]]
    train_data$is_implicit <- FALSE
  }
  
  return(list(
    train_data = train_data,
    user_ids = data.frame(user_id = rownames(user_item_matrix)),
    book_ids = data.frame(book_id = colnames(user_item_matrix)),
    n_users = nrow(user_item_matrix),
    n_items = ncol(user_item_matrix),
    user_means = user_means,
    original_matrix = user_item_matrix,
    original_ratings_matrix = original_ratings_matrix
  ))
}
```

### Hyperparameter Tuning

```{r nn-tuning}
# ============================================================================
# NN: HYPERPARAMETER TUNING WITH GRID SEARCH
# ============================================================================

# Initialize H2O cluster
h2o.init(nthreads = -1, max_mem_size = "4G")

# Create matrices
matrix_result_nn <- create_user_item_matrix(
  data,
  min_ratings_per_book = 5,
  min_ratings_per_user = 3,
  implicit_rating = 4,
  implicit_confidence = 0.5
)

user_item_matrix_nn <- matrix_result_nn$ratings
original_ratings_matrix_nn <- matrix_result_nn$original_ratings

# Prepare data
h2o_data_tuning <- prepare_h2o_data(user_item_matrix_nn, original_ratings_matrix_nn)

# Filter explicit ratings only
train_data_explicit <- h2o_data_tuning$train_data[!h2o_data_tuning$train_data$is_implicit, ]

cat("=== NN Training Data Information ===\n")
cat("Total ratings in matrix:", sum(!is.na(user_item_matrix_nn)), "\n")
cat("Explicit ratings used:", nrow(train_data_explicit), "\n")
cat("Implicit ratings excluded:", sum(h2o_data_tuning$train_data$is_implicit), "\n\n")

# Convert to H2O frame
train_h2o_tuning <- as.h2o(train_data_explicit[, c("user_id", "book_id", "rating")])
train_h2o_tuning$user_id <- as.factor(train_h2o_tuning$user_id)
train_h2o_tuning$book_id <- as.factor(train_h2o_tuning$book_id)

# Define hyperparameter grid
hyper_params <- list(
  activation = c("Tanh", "Rectifier"),
  hidden = list(
    c(16),
    c(32, 16),
    c(64, 32),
    c(64, 32, 16),
    c(128, 64, 32)
  ),
  l1 = c(0.00001, 0.0001),
  l2 = c(0.00001, 0.0001)
)

cat("=== Grid Search Configuration ===\n")
cat("Total models to train:", 
    length(hyper_params$activation) * length(hyper_params$hidden) * 
    length(hyper_params$l1) * length(hyper_params$l2), "\n\n")

set.seed(123)

# Run grid search
model_grid <- h2o.grid(
  algorithm = "deeplearning",
  grid_id = "nn_grid_book_recommendations",
  hyper_params = hyper_params,
  x = c("user_id", "book_id"),
  y = "rating",
  training_frame = train_h2o_tuning,
  nfolds = 5,
  stopping_metric = "RMSE",
  stopping_rounds = 5,
  stopping_tolerance = 0.001,
  seed = 123,
  adaptive_rate = TRUE,
  epochs = 50,
  max_w2 = 10,
  initial_weight_distribution = "UniformAdaptive"
)

# Get results
grid_results <- h2o.getGrid(
  "nn_grid_book_recommendations",
  sort_by = "rmse",
  decreasing = FALSE
)

all_models_df <- as.data.frame(grid_results@summary_table)

# Display top models
cat("\n=== Top 10 Models ===\n")
kable(head(all_models_df, 10),
      caption = "Neural Network Grid Search Results") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# Select best model
best_model_id <- grid_results@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
cv_rmse <- h2o.rmse(best_model, xval = TRUE)

best_nn_results <- data.frame(
  Activation = best_model@allparameters$activation,
  Hidden_Layers = paste(best_model@allparameters$hidden, collapse = ", "),
  N_Layers = length(best_model@allparameters$hidden),
  L1_Regularization = best_model@allparameters$l1,
  L2_Regularization = best_model@allparameters$l2,
  CV_RMSE = round(cv_rmse, 4)
)

cat("\n=== Best Model Selected ===\n")
kable(best_nn_results,
      caption = "Optimal Neural Network Configuration") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

### Model Training

```{r nn-training}
# ============================================================================
# NN: MODEL TRAINING FUNCTION
# ============================================================================

train_h2o_model <- function(train_data,
                             hidden = c(64, 32),
                             epochs = 30,
                             activation = "Tanh",
                             l1 = 0.0001,
                             l2 = 0.0001,
                             seed = 123,
                             verbose = FALSE) {
  
  if (!verbose) h2o.no_progress()
  
  model <- h2o.deeplearning(
    x = c("user_id", "book_id"),
    y = "rating",
    training_frame = train_data,
    hidden = hidden,
    epochs = epochs,
    activation = activation,
    l1 = l1,
    l2 = l2,
    adaptive_rate = TRUE,
    seed = seed,
    stopping_rounds = 5,
    stopping_metric = "RMSE",
    stopping_tolerance = 0.001,
    max_w2 = 10,
    initial_weight_distribution = "UniformAdaptive",
    initial_weight_scale = 0.5
  )
  
  return(model)
}
```

### Recommendation Functions

```{r nn-recommendations}
# ============================================================================
# NN: RECOMMENDATION FOR EXISTING USERS
# ============================================================================

recommend_for_user_nn <- function(model,
                                   user_item_matrix,
                                   user_id,
                                   n_recommendations = 10,
                                   user_ids,
                                   book_ids,
                                   user_means,
                                   book_info) {
  
  if (!user_id %in% user_ids$user_id) {
    stop("User ID not found in training data")
  }
  
  # Find unrated books
  user_row <- user_item_matrix[as.character(user_id), , drop = TRUE]
  unrated_books <- which(is.na(user_row))
  
  if (length(unrated_books) == 0) {
    message("User has rated all available books")
    return(data.frame())
  }
  
  # Prepare prediction input
  pred_data <- data.frame(
    user_id = rep(user_id, length(unrated_books)),
    book_id = colnames(user_item_matrix)[unrated_books]
  )
  
  # Convert to H2O frame
  pred_h2o <- as.h2o(pred_data)
  pred_h2o$user_id <- as.factor(pred_h2o$user_id)
  pred_h2o$book_id <- as.factor(pred_h2o$book_id)
  
  # Predict ratings
  predictions <- h2o.predict(model, pred_h2o)
  predictions_df <- as.data.frame(predictions)
  
  # Denormalize predictions
  user_mean <- user_means[as.character(user_id)]
  pred_ratings <- predictions_df$predict + user_mean
  pred_ratings <- pmax(pmin(pred_ratings, 10), 1)
  
  # Select top recommendations
  n_to_recommend <- min(n_recommendations, length(pred_ratings))
  top_indices <- order(pred_ratings, decreasing = TRUE)[1:n_to_recommend]
  
  recommendations <- data.frame(
    ISBN = pred_data$book_id[top_indices],
    Predicted_Rating = round(pred_ratings[top_indices], 2)
  )
  
  recommendations <- recommendations %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}

# ============================================================================
# NN: RECOMMENDATION FOR NEW USERS (COLD START)
# ============================================================================

recommend_for_new_user_nn <- function(model,
                                       new_user_ratings,
                                       user_item_matrix,
                                       book_ids,
                                       book_info,
                                       n_recommendations = 10,
                                       user_means = NULL) {
  
  if (length(new_user_ratings) == 0) {
    stop("No ratings provided for new user")
  }
  
  rated_books <- names(new_user_ratings)
  valid_books <- rated_books[rated_books %in% book_ids$book_id]
  
  if (length(valid_books) == 0) {
    stop("None of the rated books exist in training data")
  }
  
  # Find similar users based on rated books
  new_user_mean <- mean(new_user_ratings)
  new_user_normalized <- new_user_ratings - new_user_mean
  
  user_similarities <- numeric(nrow(user_item_matrix))
  names(user_similarities) <- rownames(user_item_matrix)
  
  for (i in 1:nrow(user_item_matrix)) {
    user_ratings <- user_item_matrix[i, valid_books]
    valid_overlap <- !is.na(user_ratings)
    
    if (sum(valid_overlap) == 0) {
      user_similarities[i] <- 0
      next
    }
    
    overlap_books <- valid_books[valid_overlap]
    existing_mean <- mean(user_ratings[valid_overlap], na.rm = TRUE)
    existing_normalized <- user_ratings[valid_overlap] - existing_mean
    new_normalized <- new_user_normalized[overlap_books]
    
    # Cosine similarity
    numerator <- sum(existing_normalized * new_normalized)
    denominator <- sqrt(sum(existing_normalized^2)) * sqrt(sum(new_normalized^2))
    
    if (denominator > 0) {
      user_similarities[i] <- numerator / denominator
    } else {
      user_similarities[i] <- 0
    }
  }
  
  # Use top-K similar users
  k <- min(30, sum(user_similarities > 0))
  
  if (k == 0) {
    # Fallback to item popularity
    item_means <- colMeans(user_item_matrix, na.rm = TRUE)
    all_books <- book_ids$book_id
    unrated_books <- setdiff(all_books, rated_books)
    pred_ratings <- item_means[unrated_books] + (new_user_mean - mean(user_means))
  } else {
    top_k_users <- names(sort(user_similarities, decreasing = TRUE)[1:k])
    similarities <- user_similarities[top_k_users]
    
    all_books <- book_ids$book_id
    unrated_books <- setdiff(all_books, rated_books)
    
    # Weighted average of similar users' ratings
    weighted_preds <- numeric(length(unrated_books))
    names(weighted_preds) <- unrated_books
    
    for (book in unrated_books) {
      similar_user_ratings <- user_item_matrix[top_k_users, book]
      valid_ratings <- !is.na(similar_user_ratings)
      
      if (sum(valid_ratings) > 0) {
        weighted_sum <- sum(similarities[valid_ratings] * similar_user_ratings[valid_ratings])
        sum_weights <- sum(abs(similarities[valid_ratings]))
        weighted_preds[book] <- weighted_sum / sum_weights
      } else {
        weighted_preds[book] <- mean(user_item_matrix[, book], na.rm = TRUE)
      }
    }
    
    pred_ratings <- weighted_preds + new_user_mean - mean(user_means)
  }
  
  pred_ratings <- pmax(pmin(pred_ratings, 10), 1)
  
  top_indices <- order(pred_ratings, decreasing = TRUE)[1:min(n_recommendations, length(pred_ratings))]
  
  recommendations <- data.frame(
    ISBN = names(pred_ratings)[top_indices],
    Predicted_Rating = round(pred_ratings[top_indices], 2)
  ) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}
```

### Cross-Validation

```{r nn-cv}
# ============================================================================
# NN: CROSS-VALIDATION
# ============================================================================

cross_validate_nn <- function(user_item_matrix,
                               original_ratings_matrix = NULL,
                               n_folds = 5,
                               seed = 123,
                               hidden = c(64, 32),
                               epochs = 20,
                               activation = "Tanh",
                               l1 = 0.0001,
                               l2 = 0.0001) {
  
  set.seed(seed)
  h2o.no_progress()
  
  # Get observed ratings
  observed <- which(!is.na(user_item_matrix), arr.ind = TRUE)
  n_ratings <- nrow(observed)
  
  if (n_ratings < n_folds * 2) {
    stop("Not enough ratings to perform cross-validation")
  }
  
  fold_indices <- sample(rep(1:n_folds, length.out = n_ratings))
  
  cv_results <- data.frame(
    fold = integer(n_folds),
    rmse = numeric(n_folds)
  )
  
  cat("Starting", n_folds, "-fold cross-validation for Neural Network...\n")
  cat("Training on EXPLICIT ratings only\n\n")
  
  for (fold in 1:n_folds) {
    cat("Fold", fold, "of", n_folds, "... ")
    
    # Split data
    test_indices <- which(fold_indices == fold)
    train_indices <- which(fold_indices != fold)
    
    test_obs <- observed[test_indices, , drop = FALSE]
    train_obs <- observed[train_indices, , drop = FALSE]
    
    # Create training matrix
    train_matrix <- user_item_matrix
    train_matrix[test_obs] <- NA
    
    # Normalize
    train_user_means <- rowMeans(train_matrix, na.rm = TRUE)
    train_normalized <- sweep(train_matrix, 1, train_user_means, FUN = "-")
    
    # Create training data
    train_data <- data.frame(
      user_id = rownames(user_item_matrix)[train_obs[, 1]],
      book_id = colnames(user_item_matrix)[train_obs[, 2]],
      rating = as.numeric(train_normalized[train_obs])
    )
    
    # Filter explicit ratings
    if (!is.null(original_ratings_matrix)) {
      train_data$original_rating <- original_ratings_matrix[train_obs]
      train_data_explicit <- train_data[train_data$original_rating != 0, ]
    } else {
      train_data_explicit <- train_data
    }
    
    if (nrow(train_data_explicit) == 0) {
      cv_results[fold, ] <- c(fold, NA)
      cat("No explicit ratings - skipping\n")
      next
    }
    
    # Convert to H2O
    train_h2o <- as.h2o(train_data_explicit[, c("user_id", "book_id", "rating")])
    train_h2o$user_id <- as.factor(train_h2o$user_id)
    train_h2o$book_id <- as.factor(train_h2o$book_id)
    
    # Train model
    model <- h2o.deeplearning(
      x = c("user_id", "book_id"),
      y = "rating",
      training_frame = train_h2o,
      hidden = hidden,
      epochs = epochs,
      activation = activation,
      l1 = l1,
      l2 = l2,
      adaptive_rate = TRUE,
      seed = seed,
      stopping_rounds = 3,
      stopping_tolerance = 0.001,
      max_w2 = 10,
      initial_weight_distribution = "UniformAdaptive",
      initial_weight_scale = 0.5,
      verbose = FALSE
    )
    
    # Prepare test data (explicit only)
    test_user_means <- train_user_means[rownames(user_item_matrix)[test_obs[, 1]]]
    test_data <- data.frame(
      user_id = rownames(user_item_matrix)[test_obs[, 1]],
      book_id = colnames(user_item_matrix)[test_obs[, 2]],
      rating = as.numeric(user_item_matrix[test_obs]) - test_user_means
    )
    
    if (!is.null(original_ratings_matrix)) {
      test_data$original_rating <- original_ratings_matrix[test_obs]
      test_data_explicit <- test_data[test_data$original_rating != 0, ]
      test_user_means_explicit <- test_user_means[test_data$original_rating != 0]
      actual_explicit <- user_item_matrix[test_obs][test_data$original_rating != 0]
    } else {
      test_data_explicit <- test_data
      test_user_means_explicit <- test_user_means
      actual_explicit <- user_item_matrix[test_obs]
    }
    
    # Convert to H2O
    test_h2o <- as.h2o(test_data_explicit[, c("user_id", "book_id", "rating")])
    test_h2o$user_id <- as.factor(test_h2o$user_id)
    test_h2o$book_id <- as.factor(test_h2o$book_id)
    
    # Predict
    predictions <- as.data.frame(h2o.predict(model, test_h2o))
    predictions_denorm <- predictions$predict + test_user_means_explicit
    predictions_denorm <- pmax(pmin(predictions_denorm, 10), 1)
    
    # Calculate RMSE
    rmse_value <- sqrt(mean((predictions_denorm - actual_explicit)^2, na.rm = TRUE))
    
    cv_results[fold, ] <- list(fold, rmse_value)
    cat("RMSE:", round(rmse_value, 4), "\n")
    
    # Cleanup
    h2o.rm(model)
    h2o.rm(train_h2o)
    h2o.rm(test_h2o)
    gc()
  }
  
  return(cv_results)
}
```

### Implementation

```{r nn-implementation}
# ============================================================================
# NN: MODEL TRAINING AND EVALUATION
# ============================================================================

# Prepare data for final model
prepared <- prepare_h2o_data(user_item_matrix_nn, original_ratings_matrix_nn)

cat("=== NN Data Prepared ===\n")
cat("Users:", prepared$n_users, "\n")
cat("Items:", prepared$n_items, "\n")
cat("Total ratings:", nrow(prepared$train_data), "\n")
cat("Explicit ratings:", sum(!prepared$train_data$is_implicit), "\n")
cat("Implicit ratings:", sum(prepared$train_data$is_implicit), "\n\n")

# Extract optimal parameters
if (exists("best_model") && !is.null(best_model)) {
  optimal_params_nn <- list(
    hidden = best_model@allparameters$hidden,
    epochs = ifelse(is.null(best_model@allparameters$epochs), 30, best_model@allparameters$epochs),
    activation = best_model@allparameters$activation,
    l1 = best_model@allparameters$l1,
    l2 = best_model@allparameters$l2
  )
} else {
  optimal_params_nn <- list(
    hidden = c(64, 32),
    epochs = 30,
    activation = "Tanh",
    l1 = 0.0001,
    l2 = 0.0001
  )
}

cat("=== Using Optimal Parameters ===\n")
print(optimal_params_nn)
cat("\n")

# Run cross-validation
nn_cv_results <- cross_validate_nn(
  user_item_matrix_nn,
  original_ratings_matrix_nn,
  n_folds = 5,
  hidden = optimal_params_nn$hidden,
  epochs = optimal_params_nn$epochs,
  activation = optimal_params_nn$activation,
  l1 = optimal_params_nn$l1,
  l2 = optimal_params_nn$l2
)

nn_cv_results %>%
  kable(
    caption = "NN: Cross-Validation Results (Explicit Ratings Only)",
    digits = 4,
    col.names = c("Fold", "RMSE"),
    align = c("c", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

cat("\n=== NN Performance Summary ===\n")
cat("Mean RMSE:", round(mean(nn_cv_results$rmse, na.rm = TRUE), 4), "\n")
cat("SD RMSE:", round(sd(nn_cv_results$rmse, na.rm = TRUE), 4), "\n\n")

# Train final model
cat("Training final model...\n")
train_data_explicit <- prepared$train_data[!prepared$train_data$is_implicit, ]

train_h2o <- as.h2o(train_data_explicit[, c("user_id", "book_id", "rating")])
train_h2o$user_id <- as.factor(train_h2o$user_id)
train_h2o$book_id <- as.factor(train_h2o$book_id)

model_nn <- train_h2o_model(
  train_data = train_h2o,
  hidden = optimal_params_nn$hidden,
  epochs = optimal_params_nn$epochs,
  activation = optimal_params_nn$activation,
  l1 = optimal_params_nn$l1,
  l2 = optimal_params_nn$l2,
  verbose = FALSE
)

cat("✓ Model training complete!\n\n")

# Generate recommendations for existing user
sample_user <- rownames(user_item_matrix_nn)[3]

recs_nn <- recommend_for_user_nn(
  model = model_nn,
  user_item_matrix = user_item_matrix_nn,
  user_id = sample_user,
  n_recommendations = 10,
  user_ids = prepared$user_ids,
  book_ids = prepared$book_ids,
  user_means = prepared$user_means,
  book_info = book_info
)

recs_nn %>%
  mutate(Rank = row_number()) %>%
  select(Rank, Book.Title, Book.Author, Predicted_Rating) %>%
  kable(
    caption = "NN: Top 10 Recommendations for Existing User",
    digits = 2,
    align = c("c", "l", "l", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# Generate recommendations for new user (cold start)
sample_books <- colnames(user_item_matrix_nn)[1:5]
new_user_ratings <- setNames(c(7, 8, 6, 5, 7), sample_books)

new_user_recs_nn <- recommend_for_new_user_nn(
  model = model_nn,
  new_user_ratings = new_user_ratings,
  user_item_matrix = user_item_matrix_nn,
  book_ids = prepared$book_ids,
  book_info = book_info,
  n_recommendations = 10,
  user_means = prepared$user_means
)

new_user_recs_nn %>%
  mutate(Rank = row_number()) %>%
  select(Rank, Book.Title, Book.Author, Predicted_Rating) %>%
  kable(
    caption = "NN: Top 10 Recommendations for New User (Cold Start)",
    digits = 2,
    align = c("c", "l", "l", "c")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# Shutdown H2O
h2o.shutdown(prompt = FALSE)
```


# Results and Analysis

## Cross-Validation Performance Comparison

```{r comprehensive-comparison}
# ============================================================================
# COMPREHENSIVE CROSS-VALIDATION COMPARISON
# ============================================================================

run_comprehensive_cv_comparison <- function(user_item_matrix,
                                            original_ratings_matrix,
                                            n_folds = 5,
                                            optimal_params_mf,
                                            optimal_params_nn,
                                            k = 50) {
  
  # Initialize H2O
  h2o.init(nthreads = -1, max_mem_size = "4G")
  h2o.no_progress()
  
  all_results <- data.frame(
    Method = character(4),
    CV_RMSE_Mean = numeric(4),
    CV_RMSE_SD = numeric(4)
  )
  
  # User-Based CF
  cat("=== Running User-Based CF ===\n")
  confidence_matrix_cf <- ifelse(original_ratings_matrix == 0, 0.5, 1.0)
  confidence_matrix_cf[is.na(confidence_matrix_cf)] <- 0.5
  
  ubcf_cv <- cross_validate_ubcf(
    user_item_matrix,
    confidence_matrix_cf,
    original_ratings_matrix,
    n_folds = n_folds,
    k = k
  )
  
  all_results[1, ] <- list(
    "User-Based CF",
    round(mean(ubcf_cv$rmse), 3),
    round(sd(ubcf_cv$rmse), 3)
  )
  
  # Item-Based CF
  cat("\n=== Running Item-Based CF ===\n")
  ibcf_cv <- cross_validate_ibcf(
    user_item_matrix,
    confidence_matrix_cf,
    original_ratings_matrix,
    n_folds = n_folds,
    k = k
  )
  
  all_results[2, ] <- list(
    "Item-Based CF",
    round(mean(ibcf_cv$rmse), 3),
    round(sd(ibcf_cv$rmse), 3)
  )
  
  # Matrix Factorization
  cat("\n=== Running Matrix Factorization ===\n")
  user_item_matrix_mf <- create_explicit_matrix(
    data,
    min_ratings_per_book = 5,
    min_ratings_per_user = 3
  )
  
  original_ratings_matrix_mf <- user_item_matrix_mf
  
  mf_cv_results <- cross_validate_mf(
    user_item_matrix_mf,
    original_ratings_matrix_mf,
    optimal_params_mf,
    n_folds = n_folds,
    n_iter = 50,
    verbose = FALSE
  )
  
  all_results[3, ] <- list(
    "Matrix Factorization",
    round(mean(mf_cv_results$RMSE, na.rm = TRUE), 3),
    round(sd(mf_cv_results$RMSE, na.rm = TRUE), 3)
  )
  
  # Neural Network
  cat("\n=== Running Neural Network ===\n")
  nn_cv_results <- cross_validate_nn(
    user_item_matrix,
    original_ratings_matrix,
    n_folds = n_folds,
    hidden = optimal_params_nn$hidden,
    epochs = optimal_params_nn$epochs,
    activation = optimal_params_nn$activation,
    l1 = optimal_params_nn$l1,
    l2 = optimal_params_nn$l2
  )
  
  all_results[4, ] <- list(
    "Neural Network",
    round(mean(nn_cv_results$rmse, na.rm = TRUE), 3),
    round(sd(nn_cv_results$rmse, na.rm = TRUE), 3)
  )
  
  h2o.shutdown(prompt = FALSE)
  
  return(all_results)
}

# ============================================================================
# PREPARE DATA AND RUN COMPARISON
# ============================================================================

# Create unified matrices
if (!exists("user_item_matrix") || !exists("original_ratings_matrix")) {
  matrix_result_unified <- create_user_item_matrix(
    data,
    min_ratings_per_book = 5,
    min_ratings_per_user = 3,
    implicit_rating = 4,
    implicit_confidence = 0.5
  )
  
  user_item_matrix <- matrix_result_unified$ratings
  original_ratings_matrix <- matrix_result_unified$original_ratings
}

cat("=== Unified Matrix Information ===\n")
cat("Dimensions:", nrow(user_item_matrix), "users x", ncol(user_item_matrix), "items\n")
cat("Sparsity:", round(mean(is.na(user_item_matrix)) * 100, 2), "%\n\n")

# Ensure optimal parameters exist
if (!exists("optimal_params_mf")) {
  optimal_params_mf <- list(
    min = list(
      dim = 20,
      lrate = 0.1,
      costp_l2 = 0.01,
      costq_l2 = 0.01
    )
  )
}

if (!exists("optimal_params_nn")) {
  optimal_params_nn <- list(
    hidden = c(64, 32),
    epochs = 20,
    activation = "Tanh",
    l1 = 0.00001,
    l2 = 0.00001
  )
}

# Run comprehensive comparison
cv_comparison_results <- run_comprehensive_cv_comparison(
  user_item_matrix = user_item_matrix,
  original_ratings_matrix = original_ratings_matrix,
  n_folds = 5,
  optimal_params_mf = optimal_params_mf,
  optimal_params_nn = optimal_params_nn,
  k = 50
)

# Display results
kable(
  cv_comparison_results,
  caption = "Cross-Validation Performance Comparison - All Methods",
  col.names = c("Method", "Mean RMSE", "SD RMSE")
) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE
  ) %>%
  column_spec(2:3, bold = TRUE)
```

```{r results-visualization, fig.cap="Performance Comparison Across Methods"}
# ============================================================================
# VISUALIZATION: PERFORMANCE COMPARISON
# ============================================================================

# Create bar plot for RMSE comparison
ggplot(cv_comparison_results, aes(x = reorder(Method, CV_RMSE_Mean), y = CV_RMSE_Mean)) +
  geom_bar(stat = "identity", fill = "steelblue", alpha = 0.7) +
  geom_errorbar(
    aes(ymin = CV_RMSE_Mean - CV_RMSE_SD, ymax = CV_RMSE_Mean + CV_RMSE_SD),
    width = 0.2
  ) +
  geom_text(aes(label = round(CV_RMSE_Mean, 3)), vjust = -0.5) +
  labs(
    title = "Cross-Validation Performance Comparison",
    subtitle = "Lower RMSE indicates better performance",
    x = "Method",
    y = "Mean RMSE"
  ) +
  theme_cowplot() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold")
  )
```


# Dataset Size Impact Analysis

```{r dataset-size-analysis}
analyze_dataset_size_impact_corrected <- function(data, 
                                                   book_sizes = c(30, 60, 90, 120, 150),
                                                   n_folds = 5,
                                                   best_method = "Item-Based CF") {
  
  results <- data.frame()
  
  # Create ONE master train/test split that you'll use for all experiments
  set.seed(123)
  all_books <- unique(data$ISBN)
  all_users <- unique(data$User.ID)
  
  # Create user-item matrix from full data
  full_matrix <- create_user_item_matrix(data)
  
  # Hold out 20% of ratings for testing (consistent across all experiments)
  observed <- which(!is.na(full_matrix$ratings), arr.ind = TRUE)
  n_test <- floor(0.2 * nrow(observed))
  test_indices <- sample(1:nrow(observed), n_test)
  test_set <- observed[test_indices, ]
  train_set <- observed[-test_indices, ]
  
  for (n_books in book_sizes) {
    cat("\n=== Training with", n_books, "books ===\n")
    
    # Sample n_books for training
    sampled_books <- sample(all_books, min(n_books, length(all_books)))
    
    # Filter TRAINING data to only include sampled books
    train_data_filtered <- data %>%
      filter(ISBN %in% sampled_books)
    
    # Create training matrix from filtered books
    train_matrix <- create_user_item_matrix(train_data_filtered)
    
    # Train model on this subset
    if (best_method == "Item-Based CF") {
      # Normalize and compute similarities
      normalized <- normalize_matrix_item(train_matrix$ratings)
      item_sim <- compute_item_similarity_matrix(normalized$normalized, 
                                                   train_matrix$confidence)
    }
    
    # NOW TEST on the SAME held-out test set (which includes all books)
    # Make predictions for test set
    predictions <- numeric(nrow(test_set))
    
    for (i in 1:nrow(test_set)) {
      user_idx <- test_set[i, 1]
      item_idx <- test_set[i, 2]
      
      # Check if this book was in our training set
      test_book <- colnames(full_matrix$ratings)[item_idx]
      
      if (test_book %in% sampled_books) {
        # Can make prediction using trained model
        # [Your prediction code here]
      } else {
        # Book not in training set - use fallback (e.g., user mean)
        predictions[i] <- rowMeans(train_matrix$ratings[user_idx, ], na.rm = TRUE)
      }
    }
    
    # Calculate RMSE on the consistent test set
    actual <- full_matrix$ratings[test_set]
    rmse <- sqrt(mean((predictions - actual)^2, na.rm = TRUE))
    
    results <- rbind(results, data.frame(
      n_books = n_books,
      rmse = rmse,
      coverage = mean(test_book %in% sampled_books) # How many test items were in training
    ))
  }
  
  return(results)
}
```


# Discussion

## Key Findings

### Performance Ranking

Based on the cross-validation results, we can rank the four collaborative filtering approaches:

1. **Best Performer**: The method with the lowest RMSE demonstrates superior prediction accuracy
2. **Trade-offs**: Each method exhibits distinct strengths and computational requirements
3. **Consistency**: Standard deviation indicates prediction stability across folds

### Method-Specific Observations

**User-Based Collaborative Filtering:**
- Leverages user similarity patterns effectively
- Performance sensitive to neighborhood size (k parameter)
- Confidence weighting improves handling of implicit feedback

**Item-Based Collaborative Filtering:**
- More stable than UBCF as item relationships change less frequently
- Effective for sparse datasets with many users
- Scalable for large item catalogs

**Matrix Factorization:**
- Captures latent factors in user-item interactions
- Requires explicit ratings for optimal performance
- Hyperparameter tuning critical for success

**Neural Network:**
- Learns complex non-linear patterns
- Requires substantial training data
- Computationally intensive but potentially most accurate

## Cold Start Problem

All four methods implement strategies for new users:
- **Memory-based methods** (UBCF, IBCF): Use similarity to existing users/items
- **Model-based methods** (MF, NN): Employ hybrid approaches with item popularity

## Limitations and Future Work

### Current Limitations

1. **Dataset Size**: Limited to 150 books may not capture full diversity
2. **Implicit Feedback**: Handling of zero ratings could be refined
3. **Temporal Dynamics**: User preferences may evolve over time
4. **Scalability**: Some methods computationally expensive for large datasets

### Future Directions

1. **Hybrid Ensemble**: Combine multiple methods to leverage their strengths
2. **Deep Learning**: Explore advanced architectures (autoencoders, attention mechanisms)
3. **Context-Aware**: Incorporate metadata (genres, authors, publication dates)
4. **Online Learning**: Implement incremental updates as new ratings arrive

\newpage

# Conclusion

This study successfully implemented and compared four collaborative filtering approaches for book recommendations. Key contributions include:

1. **Comprehensive Evaluation**: Rigorous cross-validation across identical conditions
2. **Cold Start Solutions**: Practical strategies for new user recommendations
3. **Implementation Details**: Reusable code with consistent structure and documentation
4. **Performance Insights**: Clear understanding of method trade-offs

The results demonstrate that [method with best RMSE] achieves superior prediction accuracy, while each approach offers unique advantages depending on deployment constraints. The confidence-weighted similarity approach effectively handles implicit feedback, improving prediction quality across memory-based methods.

For practitioners, we recommend:
- **High accuracy requirements**: Use the best-performing method from cross-validation
- **Real-time systems**: Consider IBCF for its stability and scalability  
- **Cold start scenarios**: Implement hybrid approaches combining collaborative and content-based filtering
- **Resource constraints**: Balance accuracy against computational requirements

This research provides a solid foundation for developing production-ready book recommendation systems, with extensible code and methodologies applicable to other domains.





# EDA stufff

User Activity Analysis

```{r user-activity-preliminary}
# Group and count the number of records per User.ID
hist_data <- data %>%
  group_by(User.ID) %>%
  count(name = "count")
hist_data

# Check the maximum count
max(hist_data$count)

```


Demographic Data Integration

```{r age-analysis}
# Merge with user_info to include age 
full_data <- data %>%
  left_join(user_info, by = "User.ID")

boxplot(full_data$Age) # Age has some very large outliers
full_data <- full_data %>% filter(full_data$Age < 110) 
# data %>% filter(Age < 5)
```


Exploratory Data Analysis

Dataset Characteristics

```{r data-summary}
# Basic summary statistics
data_summary <- data.frame(
  Metric = c("Total Users", "Total Books", "Total Ratings", 
             "Average Rating", "Rating Range"),
  Value = c(
    length(unique(data$User.ID)),
    length(unique(data$ISBN)),
    nrow(data),
    round(mean(data$Book.Rating, na.rm = TRUE), 2),
    paste(min(data$Book.Rating, na.rm = TRUE), "-", max(data$Book.Rating, na.rm = TRUE))
  )
)

kable(data_summary, caption = "Dataset Summary") %>%
  kable_styling(latex_options = "HOLD_position")

# Rating distribution
rating_dist <- table(data$Book.Rating)
rating_dist_df <- data.frame(
  Rating = names(rating_dist),
  Count = as.numeric(rating_dist),
  Percentage = round(as.numeric(rating_dist) / sum(rating_dist) * 100, 1)
)

kable(rating_dist_df, caption = "Rating Distribution") %>%
  kable_styling(latex_options = "HOLD_position")

# Rating histogram
hist(data$Book.Rating, main = "Distribution of Book Ratings",
     col = "steelblue", xlab = "Rating", ylab = "Frequency")

# ================================================================
# USER AND BOOK ACTIVITY ANALYSIS
# ================================================================

# User activity analysis
user_activity <- data %>%
  group_by(User.ID) %>%
  summarise(
    num_ratings = n(),
    avg_rating = mean(Book.Rating, na.rm = TRUE),
    .groups = "drop"
  )

# Book popularity analysis
book_popularity <- data %>%
  group_by(ISBN) %>%
  summarise(
    num_ratings = n(),
    avg_rating = mean(Book.Rating, na.rm = TRUE),
    .groups = "drop"
  )

# User activity summary table
user_activity_summary <- data.frame(
  Metric = c(
    "Total Users",
    "Min Ratings per User", 
    "Max Ratings per User",
    "Mean Ratings per User",
    "Users with 1-2 Ratings",
    "Users with 3-5 Ratings", 
    "Users with 6-10 Ratings",
    "Users with 11-20 Ratings",
    "Users with >20 Ratings"
  ),
  Value = c(
    nrow(user_activity),
    min(user_activity$num_ratings),
    max(user_activity$num_ratings),
    round(mean(user_activity$num_ratings), 2),
    sum(user_activity$num_ratings <= 2),
    sum(user_activity$num_ratings >= 3 & user_activity$num_ratings <= 5),
    sum(user_activity$num_ratings >= 6 & user_activity$num_ratings <= 10),
    sum(user_activity$num_ratings >= 11 & user_activity$num_ratings <= 20),
    sum(user_activity$num_ratings > 20)
  )
)

kable(user_activity_summary, caption = "User Activity Distribution") %>%
  kable_styling(latex_options = "HOLD_position")

# Book popularity summary table
book_popularity_summary <- data.frame(
  Metric = c(
    "Total Books",
    "Min Ratings per Book",
    "Max Ratings per Book", 
    "Mean Ratings per Book",
    "Books with 1-4 Ratings",
    "Books with 5-9 Ratings",
    "Books with 10-19 Ratings", 
    "Books with 20-49 Ratings",
    "Books with ≥50 Ratings"
  ),
  Value = c(
    nrow(book_popularity),
    min(book_popularity$num_ratings),
    max(book_popularity$num_ratings),
    round(mean(book_popularity$num_ratings), 2),
    sum(book_popularity$num_ratings <= 4),
    sum(book_popularity$num_ratings >= 5 & book_popularity$num_ratings <= 9),
    sum(book_popularity$num_ratings >= 10 & book_popularity$num_ratings <= 19),
    sum(book_popularity$num_ratings >= 20 & book_popularity$num_ratings <= 49),
    sum(book_popularity$num_ratings >= 50)
  )
)

kable(book_popularity_summary, caption = "Book Popularity Distribution") %>%
  kable_styling(latex_options = "HOLD_position")

# Dataset sparsity analysis
total_possible_ratings <- length(unique(data$User.ID)) * length(unique(data$ISBN))
actual_ratings <- nrow(data)
sparsity_percentage <- round((1 - actual_ratings / total_possible_ratings) * 100, 2)

sparsity_summary <- data.frame(
  Metric = c(
    "Total Possible User-Book Pairs",
    "Actual Ratings",
    "Sparsity Percentage",
    "Data Density"
  ),
  Value = c(
    format(total_possible_ratings, big.mark = ","),
    format(actual_ratings, big.mark = ","),
    paste0(sparsity_percentage, "%"),
    paste0(round(100 - sparsity_percentage, 2), "%")
  )
)

kable(sparsity_summary, caption = "Dataset Sparsity Analysis") %>%
  kable_styling(latex_options = "HOLD_position")
```