---
title: "DS4I_Assignment 2_Version 2"
author: "Hope Hennessy"
date: "2025-10-01"
output: pdf_document
---

Build an ensemble recommender system for book recommendations using a modified "Book-Crossing" dataset containing ratings (0-10 scale) from 10,000 users on 150 books.
Core Requirements

1. Build Four Types of Recommender Systems:

* Item-based collaborative filtering (code from scratch)
* User-based collaborative filtering (code from scratch)
* Matrix factorization-based collaborative filtering
* Neural network-based collaborative filtering

2. System Capabilities:
 
Recommend books to existing users
Handle new users (assuming they provide ratings for ≤5 books initially)

3. Evaluation and Analysis:

* Compare accuracy across all four methods using cross-validation
* Investigate the relationship between dataset size and accuracy (e.g., how does accuracy change with 5 vs 50 vs 100 titles?)
* Determine if there's a point where adding more titles doesn't improve accuracy

4. Data Analysis:

* Conduct exploratory data analysis (EDA)
* Use findings to inform train/test data splitting


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 5, fig.height = 5, 
                      fig.align = "center", warning = FALSE, message = FALSE, 
                      fig.show = 'hold', out.width = '70%')


library(tidyverse)
library(patchwork)
library(caret)
library(kableExtra)
library(recosystem)
```

```{r}
load("book_ratings.Rdata")

# Data structure
head(book_info)
str(book_info)
dim(book_info)
head(book_ratings)
str(book_ratings)
dim(book_ratings)
head(user_info) # don't need Age to build recommender, but can include this info if want to go further
str(user_info)
dim(user_info)


# print("Missing values in ratings:")
sum(is.na(book_ratings$Book.Rating))
print("Unique users:")
length(unique(book_ratings$User.ID))
print("Unique books:")
length(unique(book_ratings$ISBN))


# Check for missing values 
colSums(is.na(book_info))
colSums(is.na(book_ratings))
colSums(is.na(user_info))  # 12098 missing Age values

```

```{r}
# Merging book_ratings with book_info 
data <- book_ratings %>%
  left_join(book_info, by = "ISBN")

summary(data) # can clearly see age has some impossible outliers
head(data)
dim(data)

sapply(data, function(x) if(is.numeric(x)) range(x, na.rm = TRUE)) # check var ranges

# Check rating distribution
table(data$Book.Rating)

length(unique(data$User.ID))
length(data$User.ID)

```

```{r}
# Group and count the number of records per User.ID
hist_data <- data %>%
  group_by(User.ID) %>%
  count(name = "count")
hist_data

# Check the maximum count
max(hist_data$count)

# Plot using ggplot
ggplot(hist_data, aes(x = User.ID, y = count)) +
  geom_col() +
  labs(x = "User ID", y = "Count", title = "Counts per User ID") +
  theme_minimal()

```


### Adding Age to dataset

```{r}
# Merge with user_info to include age 
full_data <- data %>%
  left_join(user_info, by = "User.ID")

boxplot(full_data$Age) # Age has some very large outliers
full_data <- full_data %>% filter(full_data$Age < 110) 
# data %>% filter(Age < 5)
```


```{r}
hist(data$Book.Rating, main = "Distribution of Jester Ratings",
     col = "yellow", xlab = "Ratings")
```



# SAMPLE DATA FOR TESTING

```{r}
set.seed(123)
sample_users <- sample(unique(book_ratings$User.ID), 4000)
sample_data <- data %>% filter(User.ID %in% sample_users)
```


```{r}
# -------------------------------------------------------------------
# 1. Count ratings per book
# -------------------------------------------------------------------
counts_per_book <- data %>%
  group_by(ISBN) %>%
  summarise(num_ratings = n(), .groups = "drop")
counts_per_book


# Plot distribution
ggplot(counts_per_book, aes(x = num_ratings)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  scale_x_continuous(limits = c(0, quantile(counts_per_book$num_ratings, 0.95))) +
  labs(title = "Distribution of Ratings per Book",
       x = "Number of Ratings",
       y = "Count of Books")

# Percentiles
quantile(counts_per_book$num_ratings, probs = c(0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 1))

# -------------------------------------------------------------------
# 2. Count ratings per user
# -------------------------------------------------------------------
counts_per_user <- data %>%
  group_by(User.ID) %>%
  summarise(num_ratings = n(), .groups = "drop")
counts_per_user

users_per_count <- counts_per_user %>%
  count(num_ratings, name = "num_users")

users_per_count


# Plot distribution
ggplot(counts_per_user, aes(x = num_ratings)) +
  geom_histogram(binwidth = 1, fill = "lightgreen", color = "black") +
  scale_x_continuous(limits = c(0, quantile(counts_per_user$num_ratings, 0.95))) +
  labs(title = "Distribution of Ratings per User",
       x = "Number of Ratings",
       y = "Count of Users")


length(unique(data$User.ID))
```




# User-Based Collaborative Filtering (UBCF)

User-Mean Normalization is ESSENTIALY - Some users rate everything 8-10, others rate 3-5. Without normalization, you'll get poor predictions.


```{r}
# -----------------------------
# 1. USER-ITEM MATRIX FUNCTION
# -----------------------------

create_user_item_matrix <- function(ratings_data, min_ratings_per_book = 3, 
                                    min_ratings_per_user = 3) {
  
  # Convert 0 ratings to NA (unrated)
  ratings_clean <- ratings_data %>%
    mutate(Book.Rating = ifelse(Book.Rating == 0, NA, Book.Rating))
  
  # Convert to wide format
  user_item_matrix <- ratings_clean %>%
    select(User.ID, ISBN, Book.Rating) %>%
    pivot_wider(names_from = ISBN, values_from = Book.Rating, values_fill = NA)
  
  # Convert to matrix
  user_ids <- user_item_matrix$User.ID
  user_item_matrix <- as.matrix(user_item_matrix[, -1])
  rownames(user_item_matrix) <- user_ids
  
  # Filter books with too few ratings
  books_to_keep <- colSums(!is.na(user_item_matrix)) >= min_ratings_per_book
  user_item_matrix <- user_item_matrix[, books_to_keep]
  cat("Kept", sum(books_to_keep), "books with >=", min_ratings_per_book, "ratings\n")
  
  # Filter users with too few ratings
  users_to_keep <- rowSums(!is.na(user_item_matrix)) >= min_ratings_per_user
  user_item_matrix <- user_item_matrix[users_to_keep, ]
  cat("Kept", sum(users_to_keep), "users with >=", min_ratings_per_user, "ratings\n")
  
  cat("Final matrix:", nrow(user_item_matrix), "users x", ncol(user_item_matrix), "books\n\n")
  
  return(user_item_matrix)
}

```


* Builds a matrix of users × books with ratings as entries.
* min_ratings_per_book and min_ratings_per_user are thresholds to filter out very sparse data --- reduce sparsity and improve recommendation quality.

In real-world datasets like Book-Crossing, most users rate very few books and most books are rated by very few users.

* Filter books with too few ratings (min_ratings_per_book)
    * A book with only 1–2 ratings does not have enough information to estimate meaningful similarity.
    * Removing such books ensures recommendations are based on items with sufficient popularity.

* Filter users with too few ratings (min_ratings_per_user)
    * If a user has rated only 1–2 books, we cannot meaningfully compute similarity between that user and others.
    * Removing such users ensures that similarity scores are computed from users with a decent amount of rating history

Benefits

* Better similarity estimates → similarity between users is based on overlapping rated books, which is more reliable if both users have rated several items.
* Less noise → removes "cold-start" users/books that don’t contribute much.
* Computational efficiency → smaller, denser matrix → faster similarity computations.

A similarity estimate or a predicted rating is only as reliable as the amount of data that supports it. If a book is rated by only 1–2 users, or a user has rated only 1–2 books, there’s simply very little evidence to estimate who they are similar to or how they will rate other books. That low sample size leads to high variance, instability, and numerical edge-cases.

Similarity between users is computed from the overlap of items they both rated. If that overlap is 0 or 1, the similarity is meaningless or undefined.


Hard thresholds (what you already have)

* Pros: removes very noisy rows/columns, speeds computation.
* Cons: may throw away useful niche items and rare-but-important users.


### Why low-rating-count users are problematic

1. Similarity can’t be computed well
    * Suppose a user rated only 1 book.
→ There’s no overlap with others (or just 1 overlap), so similarity is unreliable.
    * Even at 3–4 ratings, overlap with most other users is tiny, so cosine/Pearson similarity is basically random noise.

2. Bias toward popular books
    * These users often only rate famous/popular books.
    * Keeping them reinforces the popularity bias, drowning out niche items and reducing diversity in recommendations.

3. Adds computational cost
    * Thousands of “weak users” make the user–item matrix huge and sparse.
    * Similarity computations are slower but don’t improve recommendation quality much.

# Here’s the predicament in a nutshell

* Most of your users (≈77%) are very sparse → they’ve rated only 1–4 books.
* If you set a cutoff like min_ratings_per_user = 5, you’d lose 77% of users (7,702 out of 10,000), keeping only ~2,300 “power users.”
* Pros of filtering: similarity calculations become more reliable (since users with 1–2 ratings don’t provide enough overlap for CF).
* Cons of filtering: you drop the majority of users, which means the system can’t generate personalized recommendations for them.

In practice, this creates a trade-off:

* Use collaborative filtering on the smaller set of reliable users.
* Provide a fallback (e.g., popularity-based, content-based, random picks) for the large group of cold-start users.


```{r}
# -----------------------------------------
# 2. USER-MEAN NORMALIZATION OF THE MATRIX
# -----------------------------------------

normalize_matrix <- function(user_item_matrix) {
  
  # Center ratings by subtracting user mean
  user_means <- rowMeans(user_item_matrix, na.rm = TRUE)
  user_item_matrix_normalized <- sweep(user_item_matrix, 1, user_means, FUN = "-")
  
  return(list(normalized = user_item_matrix_normalized, user_means = user_means))
}

```


```{r}
# ----------------------------
# 3. COSINE SIMILARITY MATRIX 
# ----------------------------

compute_similarity_matrix <- function(user_item_matrix_normalized) {
  
  n_users <- nrow(user_item_matrix_normalized)
  
  # Replace NA with 0 - allows for matrix operations
  mat <- user_item_matrix_normalized
  mat[is.na(mat)] <- 0
  
  # Dot products between user rating vectors (numerator)
  numerator <- mat %*% t(mat)
  
  # Magnitudes (denominator)
  magnitudes <- sqrt(rowSums(mat^2))
  denominator <- outer(magnitudes, magnitudes)
  
  # Cosine similarity calculation
  user_similarity_matrix <- numerator / denominator
  
  # Set self-similarity to 0
  diag(user_similarity_matrix) <- 0

  rownames(user_similarity_matrix) <- rownames(user_item_matrix_normalized)
  colnames(user_similarity_matrix) <- rownames(user_item_matrix_normalized)
  
  return(user_similarity_matrix) # a user–user similarity matrix
}

```

```{r}
# --------------------------------------------------------
# 4. RECOMMENDING FOR AN EXISTING USER (WITH USE OF K-NN)
# --------------------------------------------------------

recommend_for_user <- function(target_user, user_item_matrix, 
                               user_item_matrix_normalized, user_sim_matrix,
                               user_means, book_info, n_recommendations = 10, 
                               k = NULL) {
  
  target_user <- as.character(target_user)
  
  # Get unrated books for target user
  unrated_books <- colnames(user_item_matrix)[is.na(user_item_matrix[target_user, ])]
  
  # Get user similarities
  sims <- user_sim_matrix[target_user, ]
  
  # Replace NA with 0
  sims[is.na(sims)] <- 0  
  
  # k-NN filtering – keeps only top k most similar users
  if (!is.null(k) && k < length(sims)) {
    non_zero_count <- sum(sims != 0)
    if (non_zero_count > 0) {  # Additional safety check
      k_actual <- min(k, non_zero_count)
      top_k_users <- names(sort(sims, decreasing = TRUE)[1:k_actual])
      sims_filtered <- rep(0, length(sims))
      names(sims_filtered) <- names(sims)
      sims_filtered[top_k_users] <- sims[top_k_users]
      sims <- sims_filtered
    }
  }
  
  # Check if any similar users exist - if not return empty df
  if (sum(abs(sims) > 0) == 0) {
    return(data.frame())
    }
  
  # Replace NA's with 0's - matrix operations
  mat <- user_item_matrix_normalized
  mat[is.na(mat)] <- 0
  
  # Predicting ratings for unrated books 
  # Taking a similarity-weighted average of ratings from similar users
  weighted_ratings <- t(mat[, unrated_books, drop = FALSE]) %*% sims
  
  # Uses only users who rated each specific book
  rated_mask <- !is.na(user_item_matrix[, unrated_books, drop = FALSE])
  
  # Sum of similarities only across users who rated the book
  sum_sims <- colSums(rated_mask * abs(sims))
  
  # If no similar user rated a book - avoid division by zero
  sum_sims[sum_sims == 0] <- 1
  
  # Calculate predictions (normalized to denormalised)
  preds <- weighted_ratings / sum_sims
  preds[is.nan(preds)] <- NA
  preds <- preds + user_means[target_user]
  
  # Convert matrix to named vector
  preds <- as.vector(preds)
  names(preds) <- unrated_books  
  
  # Clip to valid rating range [1, 10]
  preds <- pmin(pmax(preds, 1), 10)
  
  # Get top N recommendations
  preds_valid <- preds[!is.na(preds)]
  
  if (length(preds_valid) == 0) {return(data.frame())}
  
  top_books <- sort(preds_valid, decreasing = TRUE)[1:min(n_recommendations, length(preds_valid))]
  
  # Df of recommended books & thier predicted ratings
  recommendations <- data.frame(
    ISBN = names(top_books),
    Predicted_Rating = as.numeric(top_books)) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}

```


```{r}
# ------------------------------------------
# 5. RECOMMENDING FOR NEW USER (COLD START)
# ------------------------------------------

recommend_for_new_user <- function(new_user_ratings, user_item_matrix, 
                                   user_item_matrix_normalized, user_means,
                                   book_info, n_recommendations = 10, 
                                   k = NULL, use_positive_only = TRUE) {
  
  # New user vector - empty
  new_user_vector <- rep(NA, ncol(user_item_matrix))
  names(new_user_vector) <- colnames(user_item_matrix)
  
  # Fill in the ratings for the user
  matched_count <- 0
  for (isbn in names(new_user_ratings)) {
    if (isbn %in% names(new_user_vector)) {
      new_user_vector[isbn] <- new_user_ratings[isbn]
      matched_count <- matched_count + 1
    }
  }
  
  # If books don't match those in training dataset - return empty vector
  if (matched_count == 0) {return(data.frame())}
  
  # User-mean normalisation
  new_user_mean <- mean(new_user_vector, na.rm = TRUE)
  new_user_normalized <- new_user_vector - new_user_mean
  new_user_vec <- new_user_normalized
  new_user_vec[is.na(new_user_vec)] <- 0
  
  mat <- user_item_matrix_normalized
  mat[is.na(mat)] <- 0
  
  # Cosine similarity
  existing_magnitudes <- sqrt(rowSums(mat^2))
  new_user_magnitude <- sqrt(sum(new_user_vec^2))
  
  new_user_sims <- as.numeric((mat %*% new_user_vec) / (existing_magnitudes * new_user_magnitude))
  new_user_sims[is.nan(new_user_sims)] <- 0
  new_user_sims[is.infinite(new_user_sims)] <- 0
  names(new_user_sims) <- rownames(user_item_matrix_normalized)
  
  # k-NN filtering
  if (!is.null(k) && k < length(new_user_sims)) {
    top_k_users <- names(sort(new_user_sims, decreasing = TRUE)[1:min(k, sum(new_user_sims != 0))])
    sims_filtered <- rep(0, length(new_user_sims))
    names(sims_filtered) <- names(new_user_sims)
    sims_filtered[top_k_users] <- new_user_sims[top_k_users]
    new_user_sims <- sims_filtered
  }
  
  # No user similarity - empty df
  if (sum(abs(new_user_sims) > 0) == 0) {return(data.frame())}
  
  # Get unrated books
  unrated_books <- names(new_user_vector)[is.na(new_user_vector)]
  
  # If user has rated all the books - empty df
  if (length(unrated_books) == 0) {return(data.frame())}
  
  # Prediction for all unrated books
  weighted_ratings <- t(mat[, unrated_books, drop = FALSE]) %*% new_user_sims
  rated_mask <- !is.na(user_item_matrix[, unrated_books, drop = FALSE])
  sum_sims <- colSums(rated_mask * abs(new_user_sims))

  sum_sims[sum_sims == 0] <- 1
  
  # Calculate predictions (normalised to denormalised)
  preds <- weighted_ratings / sum_sims
  preds[is.nan(preds)] <- NA
  preds <- preds + new_user_mean
  
  # Convert matrix to named vector
  preds <- as.vector(preds)
  names(preds) <- unrated_books 
  
  # Clip to valid rating range [1, 10]
  preds <- pmin(pmax(preds, 1), 10)
  
  # Top N recommendations
  preds_valid <- preds[!is.na(preds)]
  if (length(preds_valid) == 0) {return(data.frame())}
  
  top_books <- sort(preds_valid, decreasing = TRUE)[1:min(n_recommendations, length(preds_valid))]
  
  recommendations <- data.frame(
    ISBN = names(top_books),
    Predicted_Rating = as.numeric(top_books)) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}
```


```{r}
# ================
# MAIN WORKFLOW
# ================

# Step 1: Create user-item matrix
user_item_matrix <- create_user_item_matrix(
  data, 
  min_ratings_per_book = 5, 
  min_ratings_per_user = 3)

# Step 2: Normalize matrix
normalized_result <- normalize_matrix(user_item_matrix)
user_item_matrix_normalized <- normalized_result$normalized
user_means <- normalized_result$user_means

# Step 3: Compute similarity matrix
user_similarity_matrix <- compute_similarity_matrix(user_item_matrix_normalized)

# =============================================
# EXAMPLE 1: RECOMMENDATIONS FOR EXISTING USER
# =============================================

sample_user <- rownames(user_item_matrix)[1]

recs <- recommend_for_user(
  target_user = sample_user,
  user_item_matrix = user_item_matrix,
  user_item_matrix_normalized = user_item_matrix_normalized,
  user_sim_matrix = user_similarity_matrix,
  user_means = user_means,
  book_info = book_info,
  n_recommendations = 10,
  k = 50)  # Use top 50 similar users

recs

# ======================================================
# EXAMPLE 2: RECOMMENDATIONS FOR NEW USER (COLD START)
# ======================================================

# Simulate new user with 5 ratings
sample_books <- colnames(user_item_matrix)[1:5]
new_user_ratings <- setNames(c(8, 9, 7, 6, 8), sample_books)


for (isbn in names(new_user_ratings)) {
  book_title <- book_info$Book.Title[book_info$ISBN == isbn][1]
  if (!is.na(book_title)) {
    cat("  -", book_title, ":", new_user_ratings[isbn], "\n")
  }
}

new_user_recs <- recommend_for_new_user(
  new_user_ratings = new_user_ratings,
  user_item_matrix = user_item_matrix,
  user_item_matrix_normalized = user_item_matrix_normalized,
  user_means = user_means,
  book_info = book_info,
  n_recommendations = 10,
  k = 50)  # Use top 50 similar users

new_user_recs


```






# Item-Based Collaborative Filtering (IBCF)


```{r}
# -----------------------------------------
# 2. ITEM-MEAN NORMALIZATION OF THE MATRIX
# -----------------------------------------

normalize_matrix <- function(user_item_matrix) {
  
  # Center ratings - subtract item mean
  item_means <- colMeans(user_item_matrix, na.rm = TRUE)
  user_item_matrix_normalized <- sweep(user_item_matrix, 2, item_means, FUN = "-")
  
  return(list(normalized = user_item_matrix_normalized, item_means = item_means))
}

```


```{r}
# ----------------------------
# 3. COSINE SIMILARITY MATRIX 
# ----------------------------

# Measure how similar items are to each other based on rating patterns
# Items rated similarly by users will have high cosine similarity

compute_similarity_matrix <- function(user_item_matrix_normalized) {
  
  n_items <- ncol(user_item_matrix_normalized)
  
  # Replace NA with 0 - allows for matrix operations
  mat <- user_item_matrix_normalized
  mat[is.na(mat)] <- 0
  
  # Transpose to work with items as rows
  mat_t <- t(mat)
  
  # Dot products between item rating vectors (numerator)
  numerator <- mat_t %*% t(mat_t)
  
  # Magnitudes (denominator)
  magnitudes <- sqrt(rowSums(mat_t^2))
  denominator <- outer(magnitudes, magnitudes)
  
  # Cosine similarity calculation
  item_similarity_matrix <- numerator / denominator
  
  # Set self-similarity to 0
  diag(item_similarity_matrix) <- 0

  rownames(item_similarity_matrix) <- colnames(user_item_matrix_normalized)
  colnames(item_similarity_matrix) <- colnames(user_item_matrix_normalized)
  
  return(item_similarity_matrix) # an item–item similarity matrix
}

```


```{r}
# --------------------------------------------------------
# 4. RECOMMENDING FOR AN EXISTING USER (WITH USE OF K-NN)
# --------------------------------------------------------

recommend_for_user <- function(target_user, user_item_matrix, 
                               user_item_matrix_normalized, item_sim_matrix,
                               item_means, book_info, n_recommendations = 10, 
                               k = NULL) {
  
  target_user <- as.character(target_user)
  
  # Get rated and unrated books for target user
  user_ratings <- user_item_matrix[target_user, ]
  rated_books <- names(user_ratings)[!is.na(user_ratings)]
  unrated_books <- names(user_ratings)[is.na(user_ratings)]
  
  # If user has rated everything - no recommendations possible
  if (length(unrated_books) == 0) {
    return(data.frame())
  }
  
  # Predict rating for each unrated book
  predictions <- numeric(length(unrated_books))
  names(predictions) <- unrated_books
  
  for (target_item in unrated_books) {
    
    # Get similarities between target item and all rated items
    sims <- item_sim_matrix[target_item, rated_books]
    sims[is.na(sims)] <- 0
    
    # k-NN filtering – keeps only top k most similar items
    if (!is.null(k) && k < length(sims)) {
      non_zero_count <- sum(sims != 0)
      
      if (non_zero_count > 0) {
        
        k_actual <- min(k, non_zero_count)
        
        # Top k most similar items
        top_k_items <- names(sort(sims, decreasing = TRUE)[1:k_actual])
        
        # Similarity vector with only top k items
        sims_filtered <- rep(0, length(sims))
        names(sims_filtered) <- names(sims)
        sims_filtered[top_k_items] <- sims[top_k_items]
        sims <- sims_filtered
      }
    }
    
    # If no similar items exist - can't make a prediction
    if (sum(abs(sims) > 0) == 0) {
      predictions[target_item] <- NA
      next # skip to next unrated item
    }
    
    # Get normalized ratings for similar items
    normalized_ratings <- user_item_matrix_normalized[target_user, names(sims)]
    normalized_ratings[is.na(normalized_ratings)] <- 0
    
    # Weighted sum of normalized ratings
    weighted_sum <- sum(sims * normalized_ratings)
    sum_abs_sims <- sum(abs(sims))
    
    # Avoid division by zero
    if (sum_abs_sims == 0) {
      predictions[target_item] <- NA
    } else {
      # Denormalize prediction
      predictions[target_item] <- (weighted_sum / sum_abs_sims) + item_means[target_item]
    }
  }
  
  # Clip to valid rating range [1, 10]
  predictions <- pmin(pmax(predictions, 1), 10)
  
  # Get top N recommendations
  preds_valid <- predictions[!is.na(predictions)]
  
  # If no valid predictions - empty df
  if (length(preds_valid) == 0) {
    return(data.frame())
  }
  
  top_books <- sort(preds_valid, decreasing = TRUE)[1:min(n_recommendations, length(preds_valid))]
  
  recommendations <- data.frame(
    ISBN = names(top_books),
    Predicted_Rating = as.numeric(top_books)) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}

```


```{r}
# ------------------------------------------
# 5. RECOMMENDING FOR NEW USER (COLD START)
# ------------------------------------------

recommend_for_new_user <- function(new_user_ratings, user_item_matrix, 
                                   user_item_matrix_normalized, item_means,
                                   item_sim_matrix, book_info, 
                                   n_recommendations = 10, k = NULL) {
  
  # Get rated and unrated books
  rated_books <- names(new_user_ratings)
  all_books <- colnames(user_item_matrix)
  unrated_books <- setdiff(all_books, rated_books)
  
  # Check if any rated books match the training dataset
  matched_books <- intersect(rated_books, all_books)
  
  if (length(matched_books) == 0) {
    return(data.frame())
  }
  
  if (length(unrated_books) == 0) {
    return(data.frame())
  }
  
  # Normalize new user's ratings
  new_user_mean <- mean(new_user_ratings[matched_books], na.rm = TRUE)
  new_user_normalized <- new_user_ratings[matched_books] - new_user_mean
  
  # Predict rating for each unrated book
  predictions <- numeric(length(unrated_books))
  names(predictions) <- unrated_books
  
  for (target_item in unrated_books) {
    
    # Get similarities between target item and rated items
    sims <- item_sim_matrix[target_item, matched_books]
    sims[is.na(sims)] <- 0
    
    # k-NN filtering
    if (!is.null(k) && k < length(sims)) {
      non_zero_count <- sum(sims != 0)
      if (non_zero_count > 0) {
        k_actual <- min(k, non_zero_count)
        top_k_items <- names(sort(sims, decreasing = TRUE)[1:k_actual])
        sims_filtered <- rep(0, length(sims))
        names(sims_filtered) <- names(sims)
        sims_filtered[top_k_items] <- sims[top_k_items]
        sims <- sims_filtered
      }
    }
    
    # If no similar items exist, skip
    if (sum(abs(sims) > 0) == 0) {
      predictions[target_item] <- NA
      next
    }
    
    # Get normalized ratings
    normalized_ratings <- new_user_normalized[names(sims)]
    
    # Weighted sum
    weighted_sum <- sum(sims * normalized_ratings)
    sum_abs_sims <- sum(abs(sims))
    
    # Avoid division by zero
    if (sum_abs_sims == 0) {
      predictions[target_item] <- NA
    } else {
      # Denormalize prediction using item mean (not user mean for item-based)
      predictions[target_item] <- (weighted_sum / sum_abs_sims) + item_means[target_item]
    }
  }
  
  # Clip to valid rating range [1, 10]
  predictions <- pmin(pmax(predictions, 1), 10)
  
  # Get top N recommendations
  preds_valid <- predictions[!is.na(predictions)]
  
  if (length(preds_valid) == 0) {
    return(data.frame())
  }
  
  top_books <- sort(preds_valid, decreasing = TRUE)[1:min(n_recommendations, length(preds_valid))]
  
  recommendations <- data.frame(
    ISBN = names(top_books),
    Predicted_Rating = as.numeric(top_books)) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}
```


```{r}
# ================
# MAIN WORKFLOW
# ================

# Step 1: Create user-item matrix
user_item_matrix <- create_user_item_matrix(
  data, 
  min_ratings_per_book = 5, 
  min_ratings_per_user = 3)

# Step 2: Normalize matrix (by item means)
normalized_result <- normalize_matrix(user_item_matrix)
user_item_matrix_normalized <- normalized_result$normalized
item_means <- normalized_result$item_means

# Step 3: Compute item-item similarity matrix
item_similarity_matrix <- compute_similarity_matrix(user_item_matrix_normalized)

# =============================================
# EXAMPLE 1: RECOMMENDATIONS FOR EXISTING USER
# =============================================

sample_user <- rownames(user_item_matrix)[1]

recs <- recommend_for_user(
  target_user = sample_user,
  user_item_matrix = user_item_matrix,
  user_item_matrix_normalized = user_item_matrix_normalized,
  item_sim_matrix = item_similarity_matrix,
  item_means = item_means,
  book_info = book_info,
  n_recommendations = 10,
  k = 50)  # Use top 50 similar items

recs

# ======================================================
# EXAMPLE 2: RECOMMENDATIONS FOR NEW USER (COLD START)
# ======================================================

# Simulate new user with 5 ratings
sample_books <- colnames(user_item_matrix)[1:5]
new_user_ratings <- setNames(c(8, 9, 7, 6, 8), sample_books)

cat("New user rated:\n")
for (isbn in names(new_user_ratings)) {
  book_title <- book_info$Book.Title[book_info$ISBN == isbn][1]
  if (!is.na(book_title)) {
    cat("  -", book_title, ":", new_user_ratings[isbn], "\n")
  }
}

new_user_recs <- recommend_for_new_user(
  new_user_ratings = new_user_ratings,
  user_item_matrix = user_item_matrix,
  user_item_matrix_normalized = user_item_matrix_normalized,
  item_means = item_means,
  item_sim_matrix = item_similarity_matrix,
  book_info = book_info,
  n_recommendations = 10,
  k = 50)  # Use top 50 similar items

new_user_recs
```






# Matrix factorization-based collaborative filtering

```{r}
library(recosystem)
```



```{r}
# ================================================================
# MATRIX FACTORIZATION USING RECOSYSTEM PACKAGE
# Following Lecture Notes Workflow
# ================================================================

library(recosystem)
library(dplyr)

# ---------------------------------------------------------------
# 1. PREPARE DATA IN RECOSYSTEM FORMAT
# ---------------------------------------------------------------

prepare_recosystem_data <- function(user_item_matrix) {
  
  # Convert matrix to long format with observed ratings only
  observed <- which(!is.na(user_item_matrix), arr.ind = TRUE)
  
  # recosystem expects 0-based or 1-based integer indices
  # Let's use 0-based (subtract 1 from R's 1-based indices)
  train_data <- data.frame(
    user_index = observed[, 1] - 1,
    item_index = observed[, 2] - 1,
    rating = user_item_matrix[observed]
  )
  
  # Store ID mappings for later
  user_ids <- data.frame(
    user_index = 0:(nrow(user_item_matrix) - 1),
    user_id = rownames(user_item_matrix)
  )
  
  item_ids <- data.frame(
    item_index = 0:(ncol(user_item_matrix) - 1),
    item_id = colnames(user_item_matrix)
  )
  
  return(list(
    train_data = train_data,
    user_ids = user_ids,
    item_ids = item_ids
  ))
}


# ---------------------------------------------------------------
# 2. SPLIT INTO TRAIN AND TEST SETS
# ---------------------------------------------------------------

create_train_test_split_reco <- function(user_item_matrix, test_ratio = 0.2, seed = 123) {
  
  set.seed(seed)
  
  # Get observed ratings
  observed <- which(!is.na(user_item_matrix), arr.ind = TRUE)
  n_ratings <- nrow(observed)
  
  # Randomly sample test indices
  test_indices <- sample(1:n_ratings, size = floor(n_ratings * test_ratio))
  
  # Split data
  test_obs <- observed[test_indices, , drop = FALSE]
  train_obs <- observed[-test_indices, , drop = FALSE]
  
  # Create train and test dataframes (0-based indexing)
  train_data <- data.frame(
    user_index = train_obs[, 1] - 1,
    item_index = train_obs[, 2] - 1,
    rating = user_item_matrix[train_obs]
  )
  
  test_data <- data.frame(
    user_index = test_obs[, 1] - 1,
    item_index = test_obs[, 2] - 1,
    rating = user_item_matrix[test_obs]
  )
  
  return(list(train = train_data, test = test_data))
}


# ---------------------------------------------------------------
# 3. TRAIN MODEL (SINGLE TRAINING)
# ---------------------------------------------------------------

train_reco_model <- function(train_data, 
                             n_factors = 10,
                             learning_rate = 0.1,
                             costp_l2 = 0.01,
                             costq_l2 = 0.01,
                             n_iter = 50,
                             verbose = TRUE) {
  
  # Write training data to temp file
  train_file <- tempfile()
  write.table(train_data, file = train_file, 
              sep = " ", row.names = FALSE, col.names = FALSE)
  
  # Create data source
  train_set <- data_memory(
    user_index = train_data$user_index,
    item_index = train_data$item_index,
    rating = train_data$rating,
    index1 = TRUE  # Indicate we're using 0-based indexing
  )
  
  # Create model object
  rs <- Reco()
  
  # Train with specified hyperparameters
  rs$train(train_set, opts = list(
    dim = n_factors,           # number of latent factors
    lrate = learning_rate,     # learning rate
    costp_l2 = costp_l2,      # L2 reg for user factors
    costq_l2 = costq_l2,      # L2 reg for item factors
    niter = n_iter,           # number of iterations
    verbose = verbose
  ))
  
  unlink(train_file)
  
  return(rs)
}


# ---------------------------------------------------------------
# 4. TUNE HYPERPARAMETERS
# ---------------------------------------------------------------

tune_reco_model <- function(train_data,
                            n_factors = c(10, 25, 50),
                            learning_rate = c(0.1, 0.01),
                            n_iter = 20,
                            verbose = TRUE) {
  
  # Create data source
  train_set <- data_memory(
    user_index = train_data$user_index,
    item_index = train_data$item_index,
    rating = train_data$rating,
    index1 = TRUE
  )
  
  # Create model
  rs <- Reco()
  
  # Tune hyperparameters
  opts <- rs$tune(train_set, opts = list(
    dim = n_factors,
    lrate = learning_rate,
    costp_l2 = c(0.01, 0.1),
    costq_l2 = c(0.01, 0.1),
    niter = n_iter,
    nthread = 4,
    verbose = verbose
  ))
  
  return(opts)
}


# ---------------------------------------------------------------
# 5. MAKE PREDICTIONS AND EVALUATE
# ---------------------------------------------------------------

predict_and_evaluate <- function(model, test_data) {
  
  # Create test data source
  test_set <- data_memory(
    user_index = test_data$user_index,
    item_index = test_data$item_index,
    rating = test_data$rating,
    index1 = TRUE
  )
  
  # Generate predictions
  predictions <- model$predict(test_set, out_memory())
  
  # Calculate RMSE
  rmse <- sqrt(mean((predictions - test_data$rating)^2))
  
  # Create results dataframe
  results <- test_data %>%
    mutate(
      predicted_rating = predictions,
      improved_rating = case_when(
        predictions < 1 ~ 1,
        predictions > 10 ~ 10,
        TRUE ~ predictions
      )
    )
  
  # Calculate improved RMSE
  improved_rmse <- sqrt(mean((results$improved_rating - results$rating)^2))
  
  return(list(
    predictions = results,
    rmse = rmse,
    improved_rmse = improved_rmse
  ))
}


# ---------------------------------------------------------------
# 6. COMPLETE WORKFLOW EXAMPLE (MATCHING LECTURE NOTES)
# ---------------------------------------------------------------

cat("\n=== STEP 1: Prepare Data ===\n")
prepared <- prepare_recosystem_data(user_item_matrix)

cat("\n=== STEP 2: Create Train/Test Split ===\n")
split <- create_train_test_split_reco(user_item_matrix, test_ratio = 0.2, seed = 123)
cat("Training set:", nrow(split$train), "ratings\n")
cat("Test set:", nrow(split$test), "ratings\n")

cat("\n=== STEP 3: Train Model with Fixed Hyperparameters ===\n")
model <- train_reco_model(
  train_data = split$train,
  n_factors = 10,
  learning_rate = 0.1,
  costp_l2 = 0.01,
  costq_l2 = 0.01,
  n_iter = 50,
  verbose = TRUE
)

cat("\n=== STEP 4: Make Predictions on Test Set ===\n")
results <- predict_and_evaluate(model, split$test)
cat("RMSE:", round(results$rmse, 4), "\n")
cat("Improved RMSE (clipped):", round(results$improved_rmse, 4), "\n")

# Display sample predictions
cat("\nSample predictions:\n")
print(head(results$predictions, 10))


cat("\n\n=== STEP 5 (OPTIONAL): Tune Hyperparameters ===\n")
# opts <- tune_reco_model(
#   train_data = split$train,
#   n_factors = c(10, 25, 50),
#   learning_rate = c(0.1, 0.01),
#   n_iter = 20,
#   verbose = TRUE
# )
# 
# cat("\nOptimal parameters:\n")
# print(opts$min)
# 
# cat("\n=== STEP 6: Retrain with Optimal Parameters ===\n")
# model_optimal <- train_reco_model(
#   train_data = split$train,
#   n_factors = opts$min$dim,
#   learning_rate = opts$min$lrate,
#   costp_l2 = opts$min$costp_l2,
#   costq_l2 = opts$min$costq_l2,
#   n_iter = 50,
#   verbose = TRUE
# )
# 
# results_optimal <- predict_and_evaluate(model_optimal, split$test)
# cat("Optimal RMSE:", round(results_optimal$rmse, 4), "\n")
```












//////////
```{r}
# ---------------------------------------------------------------
# 1. EVALUATION FUNCTION WITH L2 REGULARIZATION AND BIAS
# ---------------------------------------------------------------

evaluate_fit_l2_bias <- function(params, observed_ratings, lambda, 
                                  n_users, n_items, n_factors) {
  
  n_user_factors <- n_users * n_factors
  n_item_factors <- n_items * n_factors
  
  # User latent factors: n_users x n_factors
  user_factors <- matrix(params[1:n_user_factors], 
                         nrow = n_users, ncol = n_factors)
  
  # Item latent factors: n_factors x n_items
  item_factors <- matrix(params[(n_user_factors + 1):(n_user_factors + n_item_factors)], 
                         nrow = n_factors, ncol = n_items)
  
  # User & Item biases 
  user_bias <- params[(n_user_factors + n_item_factors + 1):
                      (n_user_factors + n_item_factors + n_users)]
  
  item_bias <- params[(n_user_factors + n_item_factors + n_users + 1):
                      length(params)]
  
  # Predicted ratings
  predicted <- user_factors %*% item_factors + 
    outer(user_bias, rep(1, n_items)) + 
    outer(rep(1, n_users), item_bias)

  # Compute RMSE only on observed ratings
  errors <- (observed_ratings - predicted)^2
  rmse <- sqrt(mean(errors[!is.na(observed_ratings)]))
  
  # L2 regularization INCLUDING bias terms
  penalty <- sqrt(sum(user_factors^2) + sum(item_factors^2) + 
                  sum(user_bias^2) + sum(item_bias^2))
  
  objective <- rmse + lambda * penalty
  
  return(objective)
}
  
```


```{r}
# ---------------------------
# 3. TRAIN MODEL USING OPTIM 
# ---------------------------

train_mf_optim <- function(user_item_matrix,
                           n_factors = 10,
                           lambda = 0.02,
                           max_iterations = 100000,
                           seed = 123) {
  
  set.seed(seed)
  n_users <- nrow(user_item_matrix)
  n_items <- ncol(user_item_matrix)
  
  global_mean <- mean(user_item_matrix, na.rm = TRUE)
  centered_ratings <- user_item_matrix - global_mean
  
  n_params <- (n_users * n_factors) + (n_items * n_factors) + n_users + n_items
  initial_params <- runif(n_params, min = -0.1, max = 0.1)
  
  result <- optim(
    par = initial_params,
    fn = evaluate_fit_l2_bias,
    observed_ratings = centered_ratings,
    lambda = lambda,
    n_users = n_users,
    n_items = n_items,
    n_factors = n_factors,
    control = list(maxit = max_iterations)
  )
  
  # Extract parameters
  user_factors <- matrix(result$par[1:(n_users*n_factors)], 
                         nrow = n_users, ncol = n_factors)
  
  # FIXED: item_factors is n_factors x n_items
  item_factors <- matrix(result$par[(n_users*n_factors + 1):
                                    (n_users*n_factors + n_items*n_factors)],
                         nrow = n_factors, ncol = n_items)
  
  user_bias <- result$par[(n_users*n_factors + n_items*n_factors + 1):
                          (n_users*n_factors + n_items*n_factors + n_users)]
  
  item_bias <- result$par[(n_users*n_factors + n_items*n_factors + n_users + 1):
                          length(result$par)]
  
  # Predicted ratings for RMSE
  predicted <- user_factors %*% item_factors + 
               outer(user_bias, rep(1, n_items)) + 
               outer(rep(1, n_users), item_bias) + 
               global_mean
  
  errors <- (user_item_matrix - predicted)^2
  actual_rmse <- sqrt(mean(errors[!is.na(user_item_matrix)]))
  
  # Store names
  rownames(user_factors) <- rownames(user_item_matrix)
  colnames(item_factors) <- colnames(user_item_matrix)
  names(user_bias) <- rownames(user_item_matrix)
  names(item_bias) <- colnames(user_item_matrix)
  
  return(list(
    user_factors = user_factors,
    item_factors = item_factors,
    user_bias = user_bias,
    item_bias = item_bias,
    global_mean = global_mean,
    n_factors = n_factors,
    convergence = result$convergence,
    objective_value = result$value,
    actual_rmse = actual_rmse,
    user_ids = rownames(user_item_matrix),
    item_ids = colnames(user_item_matrix)
  ))
}

```


```{r}
# ---------------------------------------------------------------
# 5. MAKE PREDICTIONS (FIXED)
# ---------------------------------------------------------------

predict_mf <- function(model, user_idx = NULL, item_idx = NULL) {
  
  if (is.null(user_idx) && is.null(item_idx)) {
    
    # Full prediction matrix
    predicted <- model$user_factors %*% model$item_factors +
      outer(model$user_bias, rep(1, length(model$item_bias))) +
      outer(rep(1, length(model$user_bias)), model$item_bias) +
      model$global_mean
    
    rownames(predicted) <- model$user_ids
    colnames(predicted) <- model$item_ids
    
  } else {
    # FIXED: Specific prediction - item_factors is n_factors x n_items
    interaction <- sum(model$user_factors[user_idx, ] * 
                       model$item_factors[, item_idx])
    
    predicted <- model$global_mean + model$user_bias[user_idx] +
      model$item_bias[item_idx] + interaction
  }
  
  predicted <- pmin(pmax(predicted, 1), 10)
  
  return(predicted)
}

```


```{r}
# ---------------------------------------------------------------
# 6. RECOMMEND FOR USER
# ---------------------------------------------------------------

recommend_mf <- function(model, target_user, user_item_matrix,
                         book_info, n_recommendations = 10) {
  
  target_user <- as.character(target_user)
  
  # Find user index
  user_idx <- which(model$user_ids == target_user)
  
  if (length(user_idx) == 0) {
    return(data.frame(ISBN = character(),
                      Book.Title = character(),
                      Book.Author = character(),
                      Predicted_Rating = numeric()))
  }
  
  # Find unrated items
  unrated_indices <- which(is.na(user_item_matrix[target_user, ]))
  
  if (length(unrated_indices) == 0) {
    return(data.frame(ISBN = character(),
                      Book.Title = character(),
                      Book.Author = character(),
                      Predicted_Rating = numeric()))
  }
  
  # Predict once for all items, filter unrated
  all_predictions <- predict_mf(model)[user_idx, ]
  predictions <- all_predictions[unrated_indices]
  
  # Top N recommendations
  top_idx <- order(predictions, decreasing = TRUE)[1:min(n_recommendations, length(predictions))]
  top_books <- predictions[top_idx]
  
  recommendations <- data.frame(
    ISBN = names(top_books),
    Predicted_Rating = as.numeric(top_books)) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}

```




```{r}
# Assuming you have user_item_matrix and book_info loaded

# Train the model
mf_model <- train_mf_optim(
  user_item_matrix = user_item_matrix,
  n_factors = 10,
  lambda = 0.02,
  max_iterations = 50000,
  seed = 123
)

# Check convergence
cat("Convergence:", mf_model$convergence, "\n")
cat("Training RMSE:", round(mf_model$actual_rmse, 4), "\n\n")

# Get recommendations for existing user
sample_user <- rownames(user_item_matrix)[1]
recommendations <- recommend_mf(
  model = mf_model,
  target_user = sample_user,
  user_item_matrix = user_item_matrix,
  book_info = book_info,
  n_recommendations = 10
)

print(recommendations)

# Get specific prediction for one user-item pair
user_idx <- 1
item_idx <- 5
single_pred <- predict_mf(mf_model, user_idx, item_idx)
cat("\nPredicted rating:", round(single_pred, 2), "\n")
```











///



```{r}

# ---------------------------------------------------------------
# 7. CROSS-VALIDATION FOR MATRIX FACTORIZATION
# ---------------------------------------------------------------

evaluate_mf_cv <- function(user_item_matrix,
                           k_folds = 5,
                           test_ratio = 0.2,
                           n_factors = 10,
                           lambda = 0.02,
                           learning_rate = 0.005,
                           n_epochs = 20,
                           use_sgd = TRUE,
                           seed = 123) {
  
  cat("\n========================================\n")
  cat("CROSS-VALIDATION: MATRIX FACTORIZATION\n")
  cat("========================================\n\n")
  
  # Create splits (assumes function from previous code)
  cat("Creating", k_folds, "fold splits...\n")
  splits <- create_k_fold_splits(user_item_matrix, k = k_folds,
                                  test_ratio = test_ratio, seed = seed)
  
  mf_results <- list()
  
  for (fold in 1:k_folds) {
    
    cat("\n--- FOLD", fold, "---\n")
    
    train <- splits[[fold]]$train
    test <- splits[[fold]]$test
    
    n_test <- sum(!is.na(test))
    cat("Test set size:", n_test, "ratings\n\n")
    
    # Train model
    if (use_sgd) {
      cat("Training with SGD...\n")
      model <- train_mf_sgd(
        train,
        n_factors = n_factors,
        learning_rate = learning_rate,
        lambda = lambda,
        n_epochs = n_epochs
      )
    } else {
      cat("Training with optim...\n")
      model <- train_mf_optim(
        train,
        n_factors = n_factors,
        lambda = lambda,
        max_iterations = 10000
      )
    }
    
    # Generate predictions on test set
    predictions <- predict_mf(model)
    
    # Calculate metrics
    metrics <- calculate_metrics(predictions, test)
    mf_results[[fold]] <- metrics
    
    cat("MF - RMSE:", round(metrics$RMSE, 4),
        "| MAE:", round(metrics$MAE, 4),
        "| Predictions:", metrics$n_predictions, "\n")
  }
  
  # Aggregate results
  cat("\n========================================\n")
  cat("FINAL RESULTS\n")
  cat("========================================\n\n")
  
  mf_rmse <- sapply(mf_results, function(x) x$RMSE)
  mf_mae <- sapply(mf_results, function(x) x$MAE)
  
  results_summary <- data.frame(
    Method = "Matrix Factorization",
    RMSE_Mean = mean(mf_rmse, na.rm = TRUE),
    RMSE_SD = sd(mf_rmse, na.rm = TRUE),
    MAE_Mean = mean(mf_mae, na.rm = TRUE),
    MAE_SD = sd(mf_mae, na.rm = TRUE)
  )
  
  print(results_summary)
  
  return(list(
    summary = results_summary,
    fold_results = mf_results
  ))
}


# ---------------------------------------------------------------
# 8. USAGE EXAMPLES
# ---------------------------------------------------------------


model <- train_mf_optim(
  user_item_matrix = user_item_matrix,
  n_factors = 5,
  lambda = 0.03,
  max_iterations = 100000)


```






/////

```{r}
library(recosystem)

# ---------------------------------------------------------------
# 7. PREPARE DATA FOR RECOSYSTEM
# ---------------------------------------------------------------
# PURPOSE: Convert user-item matrix to recosystem's required format
# Needs: (user_index, item_index, rating) triplets

prepare_recosystem_data <- function(user_item_matrix) {
  
  # Get all non-NA ratings with their positions
  observed <- which(!is.na(user_item_matrix), arr.ind = TRUE)
  
  # Create dataframe with integer indices and ratings
  # recosystem requires 1-based integer indices
  data_df <- data.frame(
    user_index = observed[, 1],      # Row index (user)
    item_index = observed[, 2],      # Column index (item)
    rating = user_item_matrix[observed]  # Rating value
  )
  
  # Store mappings for later
  user_mapping <- data.frame(
    user_index = 1:nrow(user_item_matrix),
    user_id = rownames(user_item_matrix)
  )
  
  item_mapping <- data.frame(
    item_index = 1:ncol(user_item_matrix),
    item_id = colnames(user_item_matrix)
  )
  
  return(list(
    data = data_df,
    user_mapping = user_mapping,
    item_mapping = item_mapping
  ))
}


# ---------------------------------------------------------------
# 8. TRAIN MODEL WITH RECOSYSTEM
# ---------------------------------------------------------------
# PURPOSE: Train matrix factorization model using optimized library

train_recosystem <- function(user_item_matrix, 
                              n_factors = 20,
                              lambda = 0.1,
                              learning_rate = 0.1,
                              n_iter = 20) {
  

  
  # Prepare data
  prepared <- prepare_recosystem_data(user_item_matrix)
  
  # Create temporary files for recosystem
  # recosystem reads/writes data from files for efficiency
  train_file <- tempfile()
  
  # Write training data to file
  write.table(prepared$data, file = train_file, 
              sep = " ", row.names = FALSE, col.names = FALSE)
  
  # Create recosystem object
  reco_obj <- Reco()
  
  # Create data source
  train_set <- data_file(train_file)
  
  # Set tuning options
  # These control the optimization process
  opts <- list(
    dim = n_factors,              # Number of latent factors
    costp_l2 = lambda,            # L2 regularization for user factors
    costq_l2 = lambda,            # L2 regularization for item factors
    lrate = learning_rate,        # Learning rate
    niter = n_iter,               # Number of iterations
    nthread = 1)                  # Number of CPU threads

  
  
  # Train the model
  reco_obj$train(train_set, opts = opts)
  
  # Clean up temporary file
  unlink(train_file)
  
  
  # Return model and mappings
  return(list(
    model = reco_obj,
    user_mapping = prepared$user_mapping,
    item_mapping = prepared$item_mapping
  ))
}


# ---------------------------------------------------------------
# 9. RECOMMEND WITH RECOSYSTEM
# ---------------------------------------------------------------
# PURPOSE: Generate recommendations using trained recosystem model

recommend_recosystem <- function(reco_model, target_user, 
                                  user_item_matrix, book_info,
                                  n_recommendations = 10) {
  
  target_user <- as.character(target_user)
  
  # Get user index
  user_idx <- reco_model$user_mapping$user_index[
    reco_model$user_mapping$user_id == target_user
  ]
  
  if (length(user_idx) == 0) {
    cat("User not found in training data\n")
    return(data.frame())
  }
  
  # Find unrated items
  unrated_cols <- which(is.na(user_item_matrix[target_user, ]))
  
  if (length(unrated_cols) == 0) {
    return(data.frame())
  }
  
  # Create prediction data: (user_index, item_index) pairs
  pred_data <- data.frame(
    user_index = rep(user_idx, length(unrated_cols)),
    item_index = unrated_cols
  )
  
  # Write to temporary file
  pred_file <- tempfile()
  write.table(pred_data, file = pred_file, 
              sep = " ", row.names = FALSE, col.names = FALSE)
  
  # Create prediction data source
  pred_set <- data_file(pred_file)
  
  # Generate predictions
  out_file <- tempfile()
  reco_model$model$predict(pred_set, out_file = out_file)
  
  # Read predictions
  predictions <- scan(out_file, quiet = TRUE)
  
  # Clean up
  unlink(pred_file)
  unlink(out_file)
  
  # Clip to valid range
  predictions <- pmin(pmax(predictions, 1), 10)
  
  # Get ISBNs
  item_ids <- colnames(user_item_matrix)[unrated_cols]
  names(predictions) <- item_ids
  
  # Get top N
  top_indices <- order(predictions, decreasing = TRUE)[1:min(n_recommendations, length(predictions))]
  top_books <- predictions[top_indices]
  
  # Format recommendations
  recommendations <- data.frame(
    ISBN = names(top_books),
    Predicted_Rating = as.numeric(top_books)
  ) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}


# ---------------------------------------------------------------
# 10. NEW USER RECOMMENDATIONS WITH RECOSYSTEM
# ---------------------------------------------------------------

recommend_recosystem_new_user <- function(reco_model, new_user_ratings,
                                           user_item_matrix, book_info,
                                           n_recommendations = 10) {
  
  # Match ratings with training items
  matched_books <- intersect(names(new_user_ratings), 
                             reco_model$item_mapping$item_id)
  
  if (length(matched_books) == 0) {
    return(data.frame())
  }
  
  # Get item indices
  item_indices <- reco_model$item_mapping$item_index[
    match(matched_books, reco_model$item_mapping$item_id)
  ]
  
  # Create training data for new user
  # Assign new user a temporary index
  new_user_idx <- max(reco_model$user_mapping$user_index) + 1
  
  new_user_data <- data.frame(
    user_index = rep(new_user_idx, length(matched_books)),
    item_index = item_indices,
    rating = new_user_ratings[matched_books]
  )
  
  # Train on new user's data (incremental learning)
  train_file <- tempfile()
  write.table(new_user_data, file = train_file,
              sep = " ", row.names = FALSE, col.names = FALSE)
  
  train_set <- data_file(train_file)
  
  # Quick training (fewer iterations for new user)
  opts <- list(
    dim = reco_model$model$._get_field("opts_dim"),
    niter = 10,
    nthread = 1)
  
  # Create new model object for this user
  new_reco <- Reco()
  new_reco$train(train_set, opts = opts)
  
  unlink(train_file)
  
  # Find unrated items
  all_items <- reco_model$item_mapping$item_id
  unrated_items <- setdiff(all_items, matched_books)
  
  if (length(unrated_items) == 0) {
    return(data.frame())
  }
  
  # Get unrated item indices
  unrated_indices <- reco_model$item_mapping$item_index[
    match(unrated_items, reco_model$item_mapping$item_id)
  ]
  
  # Create prediction data
  pred_data <- data.frame(
    user_index = rep(new_user_idx, length(unrated_indices)),
    item_index = unrated_indices
  )
  
  pred_file <- tempfile()
  write.table(pred_data, file = pred_file,
              sep = " ", row.names = FALSE, col.names = FALSE)
  
  pred_set <- data_file(pred_file)
  out_file <- tempfile()
  
  new_reco$predict(pred_set, out_file = out_file)
  predictions <- scan(out_file, quiet = TRUE)
  
  unlink(pred_file)
  unlink(out_file)
  
  predictions <- pmin(pmax(predictions, 1), 10)
  names(predictions) <- unrated_items
  
  # Get top N
  top_indices <- order(predictions, decreasing = TRUE)[1:min(n_recommendations, length(predictions))]
  top_books <- predictions[top_indices]
  
  recommendations <- data.frame(
    ISBN = names(top_books),
    Predicted_Rating = as.numeric(top_books)
  ) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}


# ---------------------------------------------------------------
# 11. USAGE EXAMPLE - RECOSYSTEM
# ---------------------------------------------------------------

cat("\n\n=== EXAMPLE: RECOSYSTEM MATRIX FACTORIZATION ===\n")

# Train model with recosystem
reco_model <- train_recosystem(
  user_item_matrix = user_item_matrix,
  n_factors = 20,
  lambda = 0.1,
  learning_rate = 0.1,
  n_iter = 20)

# Recommend for existing user
sample_user <- rownames(user_item_matrix)[1]
reco_recs <- recommend_recosystem(
  reco_model = reco_model,
  target_user = sample_user,
  user_item_matrix = user_item_matrix,
  book_info = book_info,
  n_recommendations = 10
)

cat("Recommendations for existing user:\n")
print(reco_recs)

# Recommend for new user
sample_books <- colnames(user_item_matrix)[1:5]
new_user_ratings <- setNames(c(8, 9, 7, 6, 8), sample_books)

new_user_reco_recs <- recommend_recosystem_new_user(
  reco_model = reco_model,
  new_user_ratings = new_user_ratings,
  user_item_matrix = user_item_matrix,
  book_info = book_info,
  n_recommendations = 10
)

cat("\nRecommendations for new user:\n")
print(new_user_reco_recs)
```




Test