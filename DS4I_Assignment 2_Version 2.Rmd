---
title: "DS4I_Assignment 2_Version 2"
author: "Hope Hennessy"
date: "2025-10-01"
output: pdf_document
---

Build an ensemble recommender system for book recommendations using a modified "Book-Crossing" dataset containing ratings (0-10 scale) from 10,000 users on 150 books.
Core Requirements

1. Build Four Types of Recommender Systems:

* Item-based collaborative filtering (code from scratch)
* User-based collaborative filtering (code from scratch)
* Matrix factorization-based collaborative filtering
* Neural network-based collaborative filtering

2. System Capabilities:

Recommend books to existing users
Handle new users (assuming they provide ratings for ≤5 books initially)

3. Evaluation and Analysis:

* Compare accuracy across all four methods using cross-validation
* Investigate the relationship between dataset size and accuracy (e.g., how does accuracy change with 5 vs 50 vs 100 titles?)
* Determine if there's a point where adding more titles doesn't improve accuracy

4. Data Analysis:

* Conduct exploratory data analysis (EDA)
* Use findings to inform train/test data splitting


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 5, fig.height = 5, 
                      fig.align = "center", warning = FALSE, message = FALSE, 
                      fig.show = 'hold', out.width = '70%')


library(tidyverse)
library(patchwork)
library(caret)
library(kableExtra)
library(recosystem)
```

```{r}
load("book_ratings.Rdata")

# Data structure
head(book_info)
str(book_info)
dim(book_info)
head(book_ratings)
str(book_ratings)
dim(book_ratings)
head(user_info) # don't need Age to build recommender, but can include this info if want to go further
str(user_info)
dim(user_info)


# print("Missing values in ratings:")
sum(is.na(book_ratings$Book.Rating))
print("Unique users:")
length(unique(book_ratings$User.ID))
print("Unique books:")
length(unique(book_ratings$ISBN))


# Check for missing values 
colSums(is.na(book_info))
colSums(is.na(book_ratings))
colSums(is.na(user_info))  # 12098 missing Age values

```

```{r}
# Merging book_ratings with book_info 
data <- book_ratings %>%
  left_join(book_info, by = "ISBN")

summary(data) # can clearly see age has some impossible outliers
head(data)
dim(data)

sapply(data, function(x) if(is.numeric(x)) range(x, na.rm = TRUE)) # check var ranges

# Check rating distribution
table(data$Book.Rating)

length(unique(data$User.ID))
length(data$User.ID)

```

```{r}
# Group and count the number of records per User.ID
hist_data <- data %>%
  group_by(User.ID) %>%
  count(name = "count")
hist_data

# Check the maximum count
max(hist_data$count)

# Plot using ggplot
ggplot(hist_data, aes(x = User.ID, y = count)) +
  geom_col() +
  labs(x = "User ID", y = "Count", title = "Counts per User ID") +
  theme_minimal()

```


### Adding Age to dataset

```{r}
# Merge with user_info to include age 
full_data <- data %>%
  left_join(user_info, by = "User.ID")

boxplot(full_data$Age) # Age has some very large outliers
full_data <- full_data %>% filter(full_data$Age < 110) 
# data %>% filter(Age < 5)
```


```{r}
hist(data$Book.Rating, main = "Distribution of Jester Ratings",
     col = "yellow", xlab = "Ratings")
```



# SAMPLE DATA FOR TESTING

```{r}
set.seed(123)
sample_users <- sample(unique(book_ratings$User.ID), 4000)
sample_data <- data %>% filter(User.ID %in% sample_users)
```


```{r}
# -------------------------------------------------------------------
# 1. Count ratings per book
# -------------------------------------------------------------------
counts_per_book <- data %>%
  group_by(ISBN) %>%
  summarise(num_ratings = n(), .groups = "drop")
counts_per_book


# Plot distribution
ggplot(counts_per_book, aes(x = num_ratings)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  scale_x_continuous(limits = c(0, quantile(counts_per_book$num_ratings, 0.95))) +
  labs(title = "Distribution of Ratings per Book",
       x = "Number of Ratings",
       y = "Count of Books")

# Percentiles
quantile(counts_per_book$num_ratings, probs = c(0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 1))

# -------------------------------------------------------------------
# 2. Count ratings per user
# -------------------------------------------------------------------
counts_per_user <- data %>%
  group_by(User.ID) %>%
  summarise(num_ratings = n(), .groups = "drop")
counts_per_user

users_per_count <- counts_per_user %>%
  count(num_ratings, name = "num_users")

users_per_count


# Plot distribution
ggplot(counts_per_user, aes(x = num_ratings)) +
  geom_histogram(binwidth = 1, fill = "lightgreen", color = "black") +
  scale_x_continuous(limits = c(0, quantile(counts_per_user$num_ratings, 0.95))) +
  labs(title = "Distribution of Ratings per User",
       x = "Number of Ratings",
       y = "Count of Users")


length(unique(data$User.ID))
```




# User-Based Collaborative Filtering (UBCF)

User-Mean Normalization is ESSENTIALY - Some users rate everything 8-10, others rate 3-5. Without normalization, you'll get poor predictions.


```{r}
# -----------------------------
# 1. USER-ITEM MATRIX FUNCTION
# -----------------------------

create_user_item_matrix <- function(ratings_data, min_ratings_per_book = 3, 
                                    min_ratings_per_user = 3) {
  
  # Convert 0 ratings to NA (unrated)
  ratings_clean <- ratings_data %>%
    mutate(Book.Rating = ifelse(Book.Rating == 0, NA, Book.Rating))
  
  # Convert to wide format
  user_item_matrix <- ratings_clean %>%
    select(User.ID, ISBN, Book.Rating) %>%
    pivot_wider(names_from = ISBN, values_from = Book.Rating, values_fill = NA)
  
  # Convert to matrix
  user_ids <- user_item_matrix$User.ID
  user_item_matrix <- as.matrix(user_item_matrix[, -1])
  rownames(user_item_matrix) <- user_ids
  
  # Filter books with too few ratings
  books_to_keep <- colSums(!is.na(user_item_matrix)) >= min_ratings_per_book
  user_item_matrix <- user_item_matrix[, books_to_keep]
  cat("Kept", sum(books_to_keep), "books with >=", min_ratings_per_book, "ratings\n")
  
  # Filter users with too few ratings
  users_to_keep <- rowSums(!is.na(user_item_matrix)) >= min_ratings_per_user
  user_item_matrix <- user_item_matrix[users_to_keep, ]
  cat("Kept", sum(users_to_keep), "users with >=", min_ratings_per_user, "ratings\n")
  
  cat("Final matrix:", nrow(user_item_matrix), "users x", ncol(user_item_matrix), "books\n\n")
  
  return(user_item_matrix)
}

```


* Builds a matrix of users × books with ratings as entries.
* min_ratings_per_book and min_ratings_per_user are thresholds to filter out very sparse data --- reduce sparsity and improve recommendation quality.

In real-world datasets like Book-Crossing, most users rate very few books and most books are rated by very few users.

* Filter books with too few ratings (min_ratings_per_book)
    * A book with only 1–2 ratings does not have enough information to estimate meaningful similarity.
    * Removing such books ensures recommendations are based on items with sufficient popularity.

* Filter users with too few ratings (min_ratings_per_user)
    * If a user has rated only 1–2 books, we cannot meaningfully compute similarity between that user and others.
    * Removing such users ensures that similarity scores are computed from users with a decent amount of rating history

Benefits

* Better similarity estimates → similarity between users is based on overlapping rated books, which is more reliable if both users have rated several items.
* Less noise → removes "cold-start" users/books that don’t contribute much.
* Computational efficiency → smaller, denser matrix → faster similarity computations.

A similarity estimate or a predicted rating is only as reliable as the amount of data that supports it. If a book is rated by only 1–2 users, or a user has rated only 1–2 books, there’s simply very little evidence to estimate who they are similar to or how they will rate other books. That low sample size leads to high variance, instability, and numerical edge-cases.

Similarity between users is computed from the overlap of items they both rated. If that overlap is 0 or 1, the similarity is meaningless or undefined.


Hard thresholds (what you already have)

* Pros: removes very noisy rows/columns, speeds computation.
* Cons: may throw away useful niche items and rare-but-important users.


### Why low-rating-count users are problematic

1. Similarity can’t be computed well
    * Suppose a user rated only 1 book.
→ There’s no overlap with others (or just 1 overlap), so similarity is unreliable.
    * Even at 3–4 ratings, overlap with most other users is tiny, so cosine/Pearson similarity is basically random noise.

2. Bias toward popular books
    * These users often only rate famous/popular books.
    * Keeping them reinforces the popularity bias, drowning out niche items and reducing diversity in recommendations.

3. Adds computational cost
    * Thousands of “weak users” make the user–item matrix huge and sparse.
    * Similarity computations are slower but don’t improve recommendation quality much.

# Here’s the predicament in a nutshell

* Most of your users (≈77%) are very sparse → they’ve rated only 1–4 books.
* If you set a cutoff like min_ratings_per_user = 5, you’d lose 77% of users (7,702 out of 10,000), keeping only ~2,300 “power users.”
* Pros of filtering: similarity calculations become more reliable (since users with 1–2 ratings don’t provide enough overlap for CF).
* Cons of filtering: you drop the majority of users, which means the system can’t generate personalized recommendations for them.

In practice, this creates a trade-off:

* Use collaborative filtering on the smaller set of reliable users.
* Provide a fallback (e.g., popularity-based, content-based, random picks) for the large group of cold-start users.


```{r}
# -----------------------------------------
# 2. USER-MEAN NORMALIZATION OF THE MATRIX
# -----------------------------------------

normalize_matrix <- function(user_item_matrix) {
  
  # Center ratings by subtracting user mean
  user_means <- rowMeans(user_item_matrix, na.rm = TRUE)
  user_item_matrix_normalized <- sweep(user_item_matrix, 1, user_means, FUN = "-")
  
  return(list(normalized = user_item_matrix_normalized, user_means = user_means))
}

```


```{r}
# ----------------------------
# 3. COSINE SIMILARITY MATRIX 
# ----------------------------

compute_similarity_matrix <- function(user_item_matrix_normalized) {
  
  n_users <- nrow(user_item_matrix_normalized)
  
  # Replace NA with 0 - allows for matrix operations
  mat <- user_item_matrix_normalized
  mat[is.na(mat)] <- 0
  
  # Dot products between user rating vectors (numerator)
  numerator <- mat %*% t(mat)
  
  # Magnitudes (denominator)
  magnitudes <- sqrt(rowSums(mat^2))
  denominator <- outer(magnitudes, magnitudes)
  
  # Cosine similarity calculation
  user_similarity_matrix <- numerator / denominator
  
  # Set self-similarity to 0
  diag(user_similarity_matrix) <- 0

  rownames(user_similarity_matrix) <- rownames(user_item_matrix_normalized)
  colnames(user_similarity_matrix) <- rownames(user_item_matrix_normalized)
  
  return(user_similarity_matrix) # a user–user similarity matrix
}

```

```{r}
# --------------------------------------------------------
# 4. RECOMMENDING FOR AN EXISTING USER (WITH USE OF K-NN)
# --------------------------------------------------------

recommend_for_user <- function(target_user, user_item_matrix, 
                               user_item_matrix_normalized, user_sim_matrix,
                               user_means, book_info, n_recommendations = 10, 
                               k = NULL) {
  
  target_user <- as.character(target_user)
  
  # Get unrated books for target user
  unrated_books <- colnames(user_item_matrix)[is.na(user_item_matrix[target_user, ])]
  
  # Get user similarities
  sims <- user_sim_matrix[target_user, ]
  
  # Replace NA with 0
  sims[is.na(sims)] <- 0  
  
  # k-NN filtering – keeps only top k most similar users
  if (!is.null(k) && k < length(sims)) {
    non_zero_count <- sum(sims != 0)
    if (non_zero_count > 0) {  # Additional safety check
      k_actual <- min(k, non_zero_count)
      top_k_users <- names(sort(sims, decreasing = TRUE)[1:k_actual])
      sims_filtered <- rep(0, length(sims))
      names(sims_filtered) <- names(sims)
      sims_filtered[top_k_users] <- sims[top_k_users]
      sims <- sims_filtered
    }
  }
  
  # Check if any similar users exist - if not return empty df
  if (sum(abs(sims) > 0) == 0) {
    return(data.frame())
    }
  
  # Replace NA's with 0's - matrix operations
  mat <- user_item_matrix_normalized
  mat[is.na(mat)] <- 0
  
  # Predicting ratings for unrated books 
  # Taking a similarity-weighted average of ratings from similar users
  weighted_ratings <- t(mat[, unrated_books, drop = FALSE]) %*% sims
  
  # Uses only users who rated each specific book
  rated_mask <- !is.na(user_item_matrix[, unrated_books, drop = FALSE])
  
  # Sum of similarities only across users who rated the book
  sum_sims <- colSums(rated_mask * abs(sims))
  
  # If no similar user rated a book - avoid division by zero
  sum_sims[sum_sims == 0] <- 1
  
  # Calculate predictions (normalized to denormalised)
  preds <- weighted_ratings / sum_sims
  preds[is.nan(preds)] <- NA
  preds <- preds + user_means[target_user]
  
  # Convert matrix to named vector
  preds <- as.vector(preds)
  names(preds) <- unrated_books  
  
  # Clip to valid rating range [1, 10]
  preds <- pmin(pmax(preds, 1), 10)
  
  # Get top N recommendations
  preds_valid <- preds[!is.na(preds)]
  
  if (length(preds_valid) == 0) {return(data.frame())}
  
  top_books <- sort(preds_valid, decreasing = TRUE)[1:min(n_recommendations, length(preds_valid))]
  
  # Df of recommended books & thier predicted ratings
  recommendations <- data.frame(
    ISBN = names(top_books),
    Predicted_Rating = as.numeric(top_books)) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}

```


```{r}
# ------------------------------------------
# 5. RECOMMENDING FOR NEW USER (COLD START)
# ------------------------------------------

recommend_for_new_user <- function(new_user_ratings, user_item_matrix, 
                                   user_item_matrix_normalized, user_means,
                                   book_info, n_recommendations = 10, 
                                   k = NULL, use_positive_only = TRUE) {
  
  # New user vector - empty
  new_user_vector <- rep(NA, ncol(user_item_matrix))
  names(new_user_vector) <- colnames(user_item_matrix)
  
  # Fill in the ratings for the user
  matched_count <- 0
  for (isbn in names(new_user_ratings)) {
    if (isbn %in% names(new_user_vector)) {
      new_user_vector[isbn] <- new_user_ratings[isbn]
      matched_count <- matched_count + 1
    }
  }
  
  # If books don't match those in training dataset - return empty vector
  if (matched_count == 0) {return(data.frame())}
  
  # User-mean normalisation
  new_user_mean <- mean(new_user_vector, na.rm = TRUE)
  new_user_normalized <- new_user_vector - new_user_mean
  new_user_vec <- new_user_normalized
  new_user_vec[is.na(new_user_vec)] <- 0
  
  mat <- user_item_matrix_normalized
  mat[is.na(mat)] <- 0
  
  # Cosine similarity
  existing_magnitudes <- sqrt(rowSums(mat^2))
  new_user_magnitude <- sqrt(sum(new_user_vec^2))
  
  new_user_sims <- as.numeric((mat %*% new_user_vec) / (existing_magnitudes * new_user_magnitude))
  new_user_sims[is.nan(new_user_sims)] <- 0
  new_user_sims[is.infinite(new_user_sims)] <- 0
  names(new_user_sims) <- rownames(user_item_matrix_normalized)
  
  # k-NN filtering
  if (!is.null(k) && k < length(new_user_sims)) {
    top_k_users <- names(sort(new_user_sims, decreasing = TRUE)[1:min(k, sum(new_user_sims != 0))])
    sims_filtered <- rep(0, length(new_user_sims))
    names(sims_filtered) <- names(new_user_sims)
    sims_filtered[top_k_users] <- new_user_sims[top_k_users]
    new_user_sims <- sims_filtered
  }
  
  # No user similarity - empty df
  if (sum(abs(new_user_sims) > 0) == 0) {return(data.frame())}
  
  # Get unrated books
  unrated_books <- names(new_user_vector)[is.na(new_user_vector)]
  
  # If user has rated all the books - empty df
  if (length(unrated_books) == 0) {return(data.frame())}
  
  # Prediction for all unrated books
  weighted_ratings <- t(mat[, unrated_books, drop = FALSE]) %*% new_user_sims
  rated_mask <- !is.na(user_item_matrix[, unrated_books, drop = FALSE])
  sum_sims <- colSums(rated_mask * abs(new_user_sims))

  sum_sims[sum_sims == 0] <- 1
  
  # Calculate predictions (normalised to denormalised)
  preds <- weighted_ratings / sum_sims
  preds[is.nan(preds)] <- NA
  preds <- preds + new_user_mean
  
  # Convert matrix to named vector
  preds <- as.vector(preds)
  names(preds) <- unrated_books 
  
  # Clip to valid rating range [1, 10]
  preds <- pmin(pmax(preds, 1), 10)
  
  # Top N recommendations
  preds_valid <- preds[!is.na(preds)]
  if (length(preds_valid) == 0) {return(data.frame())}
  
  top_books <- sort(preds_valid, decreasing = TRUE)[1:min(n_recommendations, length(preds_valid))]
  
  recommendations <- data.frame(
    ISBN = names(top_books),
    Predicted_Rating = as.numeric(top_books)) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(recommendations)
}
```


```{r}
# ================
# MAIN WORKFLOW
# ================

# Step 1: Create user-item matrix
user_item_matrix <- create_user_item_matrix(
  data, 
  min_ratings_per_book = 5, 
  min_ratings_per_user = 3)

# Step 2: Normalize matrix
normalized_result <- normalize_matrix(user_item_matrix)
user_item_matrix_normalized <- normalized_result$normalized
user_means <- normalized_result$user_means

# Step 3: Compute similarity matrix
user_similarity_matrix <- compute_similarity_matrix(user_item_matrix_normalized)

# =============================================
# EXAMPLE 1: RECOMMENDATIONS FOR EXISTING USER
# =============================================

sample_user <- rownames(user_item_matrix)[1]

recs <- recommend_for_user(
  target_user = sample_user,
  user_item_matrix = user_item_matrix,
  user_item_matrix_normalized = user_item_matrix_normalized,
  user_sim_matrix = user_similarity_matrix,
  user_means = user_means,
  book_info = book_info,
  n_recommendations = 10,
  k = 50)  # Use top 50 similar users

recs

# ======================================================
# EXAMPLE 2: RECOMMENDATIONS FOR NEW USER (COLD START)
# ======================================================

# Simulate new user with 5 ratings
sample_books <- colnames(user_item_matrix)[1:5]
new_user_ratings <- setNames(c(8, 9, 7, 6, 8), sample_books)


for (isbn in names(new_user_ratings)) {
  book_title <- book_info$Book.Title[book_info$ISBN == isbn][1]
  if (!is.na(book_title)) {
    cat("  -", book_title, ":", new_user_ratings[isbn], "\n")
  }
}

new_user_recs <- recommend_for_new_user(
  new_user_ratings = new_user_ratings,
  user_item_matrix = user_item_matrix,
  user_item_matrix_normalized = user_item_matrix_normalized,
  user_means = user_means,
  book_info = book_info,
  n_recommendations = 10,
  k = 50)  # Use top 50 similar users

new_user_recs


```













# Item-Based Collaborative Filtering (IBCF)

USING item-mean normalization!!!! (ensure the other methods do the same thing!)

```{r}
# ---------------------------------
# Item-based CF with normalization
# ---------------------------------

# Compute item similarity using mean-centered ratings
compute_item_similarity_centered <- function(user_item_matrix) {
  mat <- user_item_matrix
  # compute item means (ignoring NA)
  item_means <- colMeans(mat, na.rm = TRUE)
  
  # center ratings by subtracting item mean
  centered <- sweep(mat, 2, item_means, FUN = "-")
  
  # replace NA with 0 for dot products
  centered0 <- centered
  centered0[is.na(centered0)] <- 0
  
  # dot-products (items x items)
  numerator <- t(centered0) %*% centered0
  
  # magnitudes
  mags <- sqrt(colSums(centered0^2))
  denom <- outer(mags, mags, FUN = "*")
  
  sim <- numerator / denom
  sim[is.nan(sim)] <- 0
  sim[is.infinite(sim)] <- 0
  diag(sim) <- 1
  
  return(list(similarity = as.matrix(sim), item_means = item_means))
}

```

```{r}
# Predict rating with normalization
predict_rating_item_based_centered <- function(user_id, item_id, user_item_matrix, sim_obj, k = NULL, 
                                               min_sim_threshold = 0) {
  user_id <- as.character(user_id)
  if (!user_id %in% rownames(user_item_matrix)) return(NA)
  if (!item_id %in% colnames(user_item_matrix)) return(NA)
  
  sim_matrix <- sim_obj$similarity
  item_means <- sim_obj$item_means
  
  # user’s ratings
  user_row <- user_item_matrix[user_id, ]
  rated_idx <- which(!is.na(user_row))
  if (length(rated_idx) == 0) return(NA)
  
  rated_items <- colnames(user_item_matrix)[rated_idx]
  
  # similarities between target item and items rated by user
  sims <- sim_matrix[item_id, rated_items]
  
  # keep only those above threshold
  keep <- which(sims > min_sim_threshold)
  if (length(keep) == 0) return(NA)
  sims <- sims[keep]
  rated_items <- rated_items[keep]
  
  # centered ratings for user’s rated items
  ratings_centered <- user_row[rated_items] - item_means[rated_items]
  
  # optionally top-k
  if (!is.null(k) && length(sims) > k) {
    topk_idx <- order(abs(sims), decreasing = TRUE)[1:k]
    sims <- sims[topk_idx]
    ratings_centered <- ratings_centered[topk_idx]
  }
  
  denom <- sum(abs(sims))
  if (denom == 0) return(NA)
  
  pred_centered <- sum(sims * ratings_centered) / denom
  
  # add back mean of target item
  pred <- item_means[item_id] + pred_centered
  return(as.numeric(pred))
}

```



```{r}
# Recommend top-N with normalized item-based CF
predict_topN_item_based_centered <- function(user_id, user_item_matrix, sim_obj, book_info,
                                             n_recommendations = 10, k = 50, min_sim_threshold = 0) {
  user_id <- as.character(user_id)
  if (!user_id %in% rownames(user_item_matrix)) return(NULL)
  
  sim_matrix <- sim_obj$similarity
  item_means <- sim_obj$item_means
  
  unread_items <- colnames(user_item_matrix)[is.na(user_item_matrix[user_id, ])]
  
  preds <- sapply(unread_items, function(it) {
    predict_rating_item_based_centered(user_id, it, user_item_matrix, sim_obj, k = k, 
                                       min_sim_threshold = min_sim_threshold)
  })
  
  preds_df <- tibble(ISBN = unread_items, Predicted_Rating = preds) %>%
    filter(!is.na(Predicted_Rating)) %>%
    arrange(desc(Predicted_Rating)) %>%
    slice_head(n = n_recommendations) %>%
    left_join(book_info, by = "ISBN") %>%
    select(ISBN, Book.Title, Book.Author, Predicted_Rating)
  
  return(preds_df)
}

```



```{r}
# Build similarity (centered)
sim_obj_centered <- compute_item_similarity_centered(user_item_matrix)

# Recommend for a sample user
sample_user <- rownames(user_item_matrix)[13]
cat("Top 10 normalized item-based CF recommendations for user", sample_user, ":\n")
recs_norm <- predict_topN_item_based_centered(sample_user, user_item_matrix, sim_obj_centered, 
                                              book_info, n_recommendations = 10, k = 10)
print(recs_norm)

```










