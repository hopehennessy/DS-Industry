---
title: "Topic Modelling"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Topic modelling is a type of bag-of-words model that aims to summarize a document by describing it in terms of a small number of "topics".

A topic is essentially a cluster of words that frequently appear together, representing a theme or concept. By expressing documents in terms of these topics, we can uncover hidden structure in the text and gain insights into what the documents are “about.”

The resulting topic descriptions can be used for a variety of purposes, including:

* Measuring similarity between documents
* Grouping (clustering) similar documents together
* Organizing and navigating large text collections
* Building features for tasks such as document classification or recommendation systems.

As we’ve seen with the bag-of-words (BoW) model, a document is represented by:

1. The words that appear in it, and
2. The frequency of each word.

When we collect a number of documents into a corpus, the first step is to identify all the distinct words used across all documents - this collection of unique words forms the vocabulary of the corpus.
Each document can then be expressed as a vector of word counts or frequencies, where each position in the vector corresponds to a word in the vocabulary.

Stacking these document vectors together produces a document-term matrix (DTM):

* Rows represent documents
* Columns represent words
* The entries represent the frequency of each word in each document

However, this DTM is often extremely wide and sparse, especially for large vocabularies. This high dimensionality can make further analyses like clustering or classification computationally expensive and conceptually challenging.


### Why Topic Modelling?

To address this problem, topic modelling reduces dimensionality by representing each document as a mixture of a small number of topics, and each topic as a distribution over words.

* Each topic is itself defined as a distribution of words, capturing common themes.
* Each document is then described by how strongly it is associated with each topic.

This representation has two key benefits:

1. It compresses the data, making downstream tasks like clustering and classification more efficient.
2. It provides interpretable summaries that reveal what the documents are about in terms of meaningful, data-driven themes.

Therefore, topic modelling summarises each document in a way that:

1. Preserves most of the important information
2. Simplifies further analysis


In this notebook, we will:

* Explore how topic models summarize the document-term matrix and introduce some foundational modelling ideas, including maximum likelihood estimation.
* Focus on Latent Dirichlet Allocation (LDA), a widely used and powerful method for topic modelling that overcomes some limitations of simpler approaches.
* Learn to implement topic modelling in R using the **topicmodels** package, with further analysis done in **tidytext**.
* We will build a topic model for a collection of movie reviews to demonstrate how to move from raw text to actionable insights.

The code in this lesson borrows heavily from [Chapter 6](http://tidytextmining.com/topicmodeling.html) of TMR, and also contains some of the material on converting between tidy and non-tidy text formats in [Chapter 5](http://tidytextmining.com/dtm.html).


## Topics

* A topic is a repeating group of statistically significant tokens or words in a corpus.
* Its is a statistical distribution of words that often appear together.

Statistical Significance 

* Groups of words occurring together in the documents
* Similar term frequency (TF) and inverse document frequency (IDF) values, with comparable ranges of TF-IDF scores
* Frequently occurring together as meaningful patterns


Statistical significance implies the group of words is important to the corpus.

* Topic modelling is a process of automatically finding the hidden topics in a text data in an unsupervised manner.
* Topic modelling can also be refered to as a text mining approach to find recurring patterns in the text documents.


## Summary of Common Topic Models

Below is an overview of some foundational topic modelling approaches. The first two (unigram and mixture of unigrams) are rarely used in practice but are useful for understanding the motivation behind more advanced models like pLSA and LDA.


### 1. Unigram Model

Each document is generated by sampling words independently from a single multinomial distribution over the vocabulary.

* This model is very simple and assumes no topic structure - all words in a corpus are treated as if they came from one overarching source.
* Rarely useful because it cannot distinguish between documents about different themes.


### 2. Mixture of Unigrams

Each document is assumed to belong entirely to one of K topics, where each topic is a multinomial distribution over words:

1. A topic is first chosen for the entire document.
2. Every word in that document is then sampled from that topic's word distribution.

Limitation: This model forces each document to be about one topic only, which is unrealistic for most real-world text (e.g. a news article might cover both politics and economics).


### 3. Latent Semantic Analysis (LSA)

* Type: Matrix factorization approach (deterministic).
* Idea: Instead of treating documents probabilistically, LSA uses linear algebra to uncover latent structure in the document-term matrix.

The DTM is decomposed using Singular Value Decomposition (SVD) into three smaller matrices:

$$\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T$$


* $\mathbf{U}$ : Document-topic matrix
* $\mathbf{\Sigma}$ : Singular values (weights of topics)
* $\mathbf{V}^T$ : Topic-word matrix

Each topic corresponds to a singular vector that captures patterns of word co-occurrence.

* Strengths:
    * Reduces dimensionality effectively.
    * Useful for tasks like information retrieval and document similarity.
* Limitations:
    * LSA is not a probabilistic model, so outputs (like negative weights) are harder to interpret.
    * It can capture noise as well as meaningful patterns.





### 4. Probabilistic Latent Semantic Analysis (pLSA)

* Type: Probabilistic model (bridges gap between LSA and LDA).
* Introduces a probabilistic interpretation of LSA
* pLSA improves upon the mixture of unigrams by allowing documents to be mixtures of topics, just like LDA. However, its probabilistic structure is slightly different:

For each word position in a document:

1. A topic is selected based on a document-specific topic distribution.
2. A word is then generated from the selected topic's word distribution.

Key characteristics:

* Unlike LDA, pLSA does not place a prior distribution on the document-topic mixtures. This makes it purely descriptive: it can model the training data well, but it cannot naturally generalize to new documents, as it has no way to generate topic distributions for unseen texts.
* It was an important stepping stone in the development of LDA, providing the first practical way to model documents as mixtures of latent topics.

Key difference from LDA:

* pLSA directly estimates a separate parameter for each document, so the number of parameters grows with the corpus size.
* This makes pLSA prone to overfitting and poor generalization to new, unseen documents.

pLSA was an important step towards LDA but is now largely replaced by LDA because of its limitations.


### 5. Latent Dirichlet Allocation (LDA)

* Type: Fully generative probabilistic model.
* LDA improves upon pLSA by introducing Dirichlet priors, which regularize the topic distributions and solve the overfitting issue.
* This simple addition makes the model fully generative and more robust.

Each document is represented as a mixture of K topics, and each topic is represented as a distribution over words. Each word is assigned to a topic independently.

Generative process:

1. For each document, a topic distribution is drawn from a Dirichlet prior.
2. For each word position in the document:
    * A topic is chosen based on that document’s topic distribution (drawn from a Dirichlet prior).
    * A word is sampled from that topic's word distribution

What is the topic-to-term distributions & document-to-topic distributions?

Why LDA is So Powerful

1. Handles unseen documents naturally using the Dirichlet prior.
2. Regularization through the Dirichlet prior $\to$ prevents the model from overfitting to the training data

3. Fully generative model – can naturally assign topics to unseen documents, unlike pLSA.

4. Interpretability of Topics
    * LDA naturally produces human-readable topics:
        * Each topic is a list of words with associated probabilities, which often form coherent themes (e.g. "doctor, hospital, treatment" $\to$ medical topic).
    * Documents are expressed as simple mixtures of these topics, making it easy to see what a document is about and to compare documents by their topic composition.

5. Efficient dimensionality reduction – compresses large, sparse text data into a smaller set of semantic features (topics).
6. Scalable and flexible – works on large datasets and serves as a foundation for advanced topic modelling methods.



This progression shows how topic models evolved:

* From no topic structure (unigram),
* To one topic per document (mixture of unigrams),
* To mixtures of topics but no priors (pLSA),
* To fully generative and regularized mixtures of topics (LDA).




## LDA Example

We have five documents:

* Document 1: "I want to have fruits for my breakfast."
* Document 2: "I like to eat almonds, eggs and fruits."
* Document 3: "I will take fruits and biscuits with me while going to Zoo."
* Document 4: "The zookeeper feeds the lion very carefully."
* Document 5: "One should give good quality biscuits to their dogs."

LDA will process these documents and try to uncover hidden themes (topics).
Here, it finds two topics:

* Topic 1 $\to$ Food-related words. Example terms: fruits, eggs, biscuits, breakfast
* Topic 2 $\to$ Animal-related words. Example terms: lion, dogs, zoo, zookeeper


### 1. Topic-to-Term Distribution: P(word|topic)

“Given a topic, what is the probability of each word appearing?”

It represents how words define a topic.

* Topic 1 (Food): Words like fruits (30%), eggs (15%), and biscuits (10%) have the highest probabilities, showing this topic is strongly food-related.
* Topic 2 (Animals): Words like zoo (20%), lion (10%), and dogs (8%) dominate, indicating this topic focuses on animals.

When LDA generates a word, it first picks a topic, then picks a word based on this topic-to-term probability. This is how the model clusters similar words together to form coherent topics.


### 2. Document-to-Topic Distribution: P(topic|document)

“Given a document, what proportion of it belongs to each topic?”

It represents how each document is composed of topics.

* Documents 1 and 2: 100% Topic 1 Documents 3: 100% Topic 2 Document 4 and * * Document 5: 70% Topic 1, 30% Topic 2

### LDA connects topics, words, and documents through these two probability distributions

* The topic-to-term distribution gives insights into themes and helps interpret topics.
* The document-to-topic distribution lets you:
    * Cluster documents by their dominant topics,
    * Measure similarity between documents,
    * Or use topic proportions as features in machine learning models.


## The unigram model

#### Basic idea

There is a single probability distribution over words that applies to all documents in the corpus. All documents are generated by the *same* probability distribution. The goal is to estimate the parameters of this distribution.

#### Some more details

We are interested in estimating $p(w_{ij})$, the probability that word $i$ "appears in" or "is generated by" document $j$, for all $i=1,\dots,n$ words in the vocabulary and all $j=1,\dots,N$ documents in the corpus. The unigram model assumes that all documents are generated by the same probability distribution, so $p(w_{i1})=p(w_{i2})=\dots=p(w_{iN})$ and we can just drop the $j$ subscript and refer to $p(w_i)$.

The multinomial distribution is the appropriate distribution to use in this context, where we are sampling from a discrete distribution with $n$ classes. The parameter vector of the multinomial distribution is the vector of probabilities $[p(w_1),\dots,p(w_n)]$. 

Say we have three words in our vocabulary $w_1$, $w_2$, $w_3$. The probability that, in a document of 6 words, word 1 appears four times, word 2 twice, and word 3 not all is, is $$p(w_1)^4p(w_2)^2p(w_3)^0$$

This is a joint probability, and is also the *likelihood*. By the principle of maximum likelihood, we try and find the values of $p(w_1)$, $p(w_2)$, $p(w_3)$ that maximize this likelihood. 

Equivalently, we can find the values of $p(w_1)$, $p(w_2)$, $p(w_3)$ that maximize the *log* of the likelihood, given by $4\ln(p(w_1)) + 2\ln(p(w_2))$. This is usually easier to do.

Over all $N$ documents, the likelihood we want to maximize is given by 

$$L = \prod_{j=1}^N \prod_{i=1}^n p(w_i)^{f_j(w_i)} = \prod_{i=1}^n p(w_i)^{\sum_{j=1}^N f_j(w_i)}$$ 

where $f_j(w_i)$ is the observed number of times that word $i$ appears in document $j$.

Again it is simpler to work with the log:

$$l = \ln(L) = \sum_{i=1}^n \sum_{j=1}^N f_j(w_i) \ln p(w_i)$$


## Mixture of unigrams

#### Basic idea

Here we say that, rather than there just being a single distribution over words, there are several possible distributions. Each possible distribution can be thought of as a "topic". Put this way, a "topic" is a probability distribution over words - in fact this makes quite a lot of sense (with some topics, some words are more likely to come up).

In the mixture of unigrams model a document can only belong to one topic. 

A good way to think of the mixture of unigrams model is:

1. When we generate a document, we first choose a topic from a set of $K$ topics (we have to set $K$ beforehand). The probability of choosing topic $k$ is $p(z_k)$.
2. We then generate the words in that document from the appropriate distribution. Previously we had a single word distribution with terms like $p(w_i)$. Now we have one distribution for each topic, and the terms are $p(w_i|z=k)$. We read this as "the probability that topic $k$ generates word $i$.  

#### Some more details

We now basically have to estimate the parameters of *several* multinomial distributions, rather than just one, and we *also* have to estimate the probabilities of generating the topics $p(z=1),\dots,p(z=K)$. With $K$ topics, we have $K+Kn$ parameters to estimate.   

Say we have three words in our vocabulary $w_1$, $w_2$, $w_3$. The probability that, in a document of 6 words, word 1 appears four times, word 2 twice, and word 3 not at all, now depends on what "topic" the document belongs to:

* if the document is from topic 1, the probability is $p(w_1|z_1)^4p(w_2|z_1)^2p(w_3|z_1)^0$. The probability that the document is from topic 1 is given by $p(z_1)$. This is *another parameter* that we will have to estimate.
* if the document is from topic 2, the probability is $p(w_1|z_2)^4p(w_2|z_2)^2p(w_3|z_2)^0$. The probability that the document is from topic 2 is given by $p(z_2)$. 
* if the document is from topic 3, the probability is $p(w_1|z_3)^4p(w_2|z_3)^2p(w_3|z_3)^0$. The probability that the document is from topic 3 is given by $p(z_3)$. 

The joint probability is given by:

* multiplying together the probability that a document is from a topic (e.g. $p(z_1)$) with the probability of generating the observed frequencies *given that topic* (e.g. $p(w_1|z_1)^4p(w_2|z_1)^2p(w_3|z_1)^0$). This will give three *joint probabilities*, each one gives the probability of being a particular topic *and* getting the observed word frequencies.
* adding up the three joint probabilities gives the marginal probability of getting the observed frequencies 

$$p(\mathbf{w}) = \sum_{k=1}^K p(z_k)\prod_{i=1}^n p(w_i|z_k)$$

We can again estimate parameter values by maximizing the likelihood. All that needs to be done is to take a product of the joint probabilities worked out above over documents - that is the likelihood function. 

## Latent semantic analysis (LSA)

#### Basic idea

This is a different, non-probabilistic approach that uses matrix factorization. The document-term matrix is factorized, similar to what we saw in the recommendation systems sections. The resulting matrices summarize documents and words respectively, but lose their probabilistic interpretation. 

See the recommender system notebook on matrix factorization. We can use singular value decomposition to do the matrix factorization.

## Probabilistic latent semantic analysis (pLSA)

#### Basic idea

The core ideas in pLSA are:

1. **Each document is a mixture of topics**. For example, a document could consist of 90% topic 1 and 10% topic 2, while another document could have different proportions of the two topics.
2. **Each topic is a mixture of words**. This mixture, expressed as a probability distribution defined over the words in a vocabulary, can be thought of as the propensity of the topic to use certain words over others. 

The second of these is as for the mixture of unigrams model, but the first is new, and important - because most documents are not about just one topic.

A handy way of thinking of pLSA is to think of how it can be used to generate a document, assuming you know the distribution of topics for the document, and probabilities of generating different words conditional on each topic (of course, in reality finding these distributions is the problem that pLSA is trying to solve!).

To generate a document we would:

1. Start with a number of words you want to generate (say 100)
2. Divide up those "placeholders" for words between topics, in the specified proportions. Say, for example, we want a document with 80% topic 1 and 20% topic 2. 80 of the 100 words will come from topic 1, 20 from topic 2.
3. Draw 80 words from the conditional probability distribution giving the probability of word *i* appearing given topic 1. 
4. Draw 20 words from the conditional probability distribution giving the probability of word *i* appearing given topic 2. 

#### Some more details

Things get a bit hairy here, so I'll just give a sketch. 

In the mixture of unigrams model we model the probability of document $j$ generating word $i$ as 

$$p(w_{ij}) = \sum_{k=1}^K p(w_i|z_k)p(z_k)$$

The topic probabilities are the same for all documents. Put another way, documents are not allowed to be different mixtures of topics.

The pLSA model relaxes this assumption

$$p(w_{ij}) = \sum_{k=1}^K p(w_i|z_k,d_j)p(z_k|d_j)p(d_j)$$

In order to formulate a likelihood function that can be maximized, pLSA needs to respect some probability rules. Specifically, to work out the joint probability that a document $j$ produces a word $i$ the following "ingredients" are needed.

* The probability of word i given topic k, for all i and k. This is the same as before, and gives $nK$ parameters to estimate.
* The "mixture" of topics in each document -- the probability that document $j$ generates topic $k$. This is new, and gives $NK$ parameters to estimate.
* The probability that the corpus generates document $j$. This is also new, and gives $N$ parameters to estimate.

The last of these in particular is tricky conceptually, but it is needed to make sure the probabilities actually come out as probabilities -- that, for example, they sum to one. 

### Latent Dirichlet allocation

#### Basic idea

Latent Dirichlet allocation uses the same basic setup as pLSA:

* Each document is a mixture of topics
* Each topic is a mixture of words

In pLSA the topic distribution (the proportion of topics making up a document) is different for each document. That's why there were $nK$ parameters to estimate. LDA says that the topic distributions are drawn from *another* distribution (!), specifically a Dirichlet distribution with parameters $[a_1,\dots,a_K]$. The Dirichlet distribution is a useful distribution for modelling things like proportions and probabilities that sum to one. This means that we only estimate the $K$ parameters of the Dirichlet, substantially less than the $nK$ parameters estimated by pLSA. This also does away with the need to specifically estimate the probability that the corpus generates document $j$. These are the two main benefits of LDA over pLSA.

To generate a document we would:

1. Start with a number of words you want to generate (say 100)
2. Draw a vector of proportions by sampling from a Dirichlet distribution with $[a_1,\dots,a_K]$. This might give, for example, 80\% in topic 1 and 20\% for topic 2.
3. Divide up those "placeholders" for words between topics, in the specified proportions. Say, for example, we want a document with 80% topic 1 and 20% topic 2. 80 of the 100 words will come from topic 1, 20 from topic 2.
4. Draw 80 words from the conditional probability distribution giving the probability of word *i* appearing given topic 1. 
5. Draw 20 words from the conditional probability distribution giving the probability of word *i* appearing given topic 2. 

#### Some more details

LDA is a hierarchical model and estimation is technically complex. The two main approaches are a variational expectation maximisation algorithm, and collapsed Gibbs sampling. Both of these are beyond the scope of this course, but there is a nice explanation of collapsed Gibbs sampling in Chapter 20 of Joel Grus' ["Data Science from Scratch: First Principles with Python"](http://shop.oreilly.com/product/0636920033400.do).


# Testing the topic model on reviews for two movies

* The goal is to test a topic model (LDA) on movie reviews for two very different films: Taxi Driver and Waterworld.
* Since we know the “true topics” (each movie is a topic), we can easily see if LDA can separate reviews by movie.
* This serves as a controlled example to explore how well LDA identifies topics.
* Using web scraping, we have collected 160 reviews of Taxi Driver, and 160 reviews of Waterworld from the IMDb site. 

```{r}
library(tidyverse)
library(tidytext)
library(topicmodels)

load("my_imdb_reviews.RData")
head(reviews, 3)
```

We then do a bit of data cleaning to make subsequent analyses easier. We change the class of the `review` variable from factor to character and add a document ID variable.

```{r}
reviews <- as_tibble(reviews) %>% 
  mutate(review = as.character(review), reviewId = 1:nrow(reviews))
```

Tokenization and stop-word removal:

```{r}
tidy_reviews <- reviews %>% 
  unnest_tokens(word, review, token = "words", to_lower = T) %>%
  filter(!word %in% stop_words$word)

head(tidy_reviews, 3)
```

* unnest_tokens() splits each review into individual words (tokens).
* to_lower = TRUE ensures all words are lowercase for consistency.
* filter(!word %in% stop_words$word) removes common stop words like the, and, is.

```{r}
reviews_tdf <- tidy_reviews %>%
  count(reviewId, word)
```

* Counts how many times each word occurs in each review.
* This is still in tidy (long) format, suitable for converting into a proper document-term matrix.


* Previously, we created "wide" document-term matrices using pivot_wider() for bag-of-words features, suitable for models like rpart.
* Some R packages, including topicmodels, require a specific DocumentTermMatrix format rather than a wide data frame.
* The tm package provides the DocumentTermMatrix class, which is widely used for text modeling in R.
* tidytext offers functions like cast_dtm() to convert tidy or ordinary document-term matrices into DocumentTermMatrix objects and vice versa.

```{r}
dtm_reviews <- reviews_tdf %>%
  cast_dtm(document = reviewId, # the ID of each document
           term = word,         # the word or token
           value = n)           # the count of the word
dtm_reviews

str(dtm_reviews)
dtm_reviews$dimnames$Docs[1:5]  # view first 5 document IDs
dtm_reviews$dimnames$Terms[1:10]  # view first 10 terms

tidy(dtm_reviews) %>% sample_n(10) # or head(10), etc.

```

tidy(dtm_reviews) $\to$ converts the DocumentTermMatrix to a tidy data frame

Estimating Latent Dirichlet Allocation (LDA) on the DTM:

* Topic model parameters are estimated using the LDA() function from the topicmodels package.
* You must specify the number of latent topics (k) to be extracted.
* The resulting object is of class LDA_VEM, which is an S4 object in R.
* Unlike S3 objects, S4 objects do not use $ to access slots; instead, you use the @ operator.

```{r}
reviews_lda <- LDA(dtm_reviews, k = 2, control = list(seed = 1234))
reviews_lda
str(reviews_lda)
head(reviews_lda@gamma,3)
```

The main things we are interested in are:

* beta: these parameters control the probability of a given topic k generating a particular word i.
    * In LDA, each topic is modeled as a probability distribution over words in the vocabulary.
    * beta is the log-probabilities of words given each topic
    * The probability of word $j$ given topic $k$ is $\beta_{kj} = \log P(\text{word}_j \mid \text{topic}_k)$
    * Rows are topics (1 … k), columns are words in the vocabulary, and values are log-probabilities that a given word belongs to a given topic

* gamma: this gives the topic "mixtures" for each document.
    * In an LDA model, each document is assumed to be a mixture of topics.
    * gamma is the document-topic probability matrix.
        * Rows $\to$ documents (reviewId)
        * Columns $\to$ topics (k topics specified in LDA)
        * Values $\to$ the estimated probability that a given document belongs to a given topic.
    * Each row sums to approximately 1.
    * gamma[i, j] = probability that document i belongs to topic j






We'll first look at the beta parameters and examine which words are most associated with each topic. This will be a good way of checking, in our case, whether the words align themselves into two topics we are expecting (i.e. one to do with Taxi Driver, the other Waterworld). More generally, you would do this to assess what the topics are about. 

### Word-topic probabilities (uses the beta values)

We start by creating a data frame containing the "terms" (the words we tokenized the reviews into), and the beta parameters (one beta parameter per-word-per-topic).

```{r}
term <- as.character(reviews_lda@terms)
topic1 <- reviews_lda@beta[1,]
topic2 <- reviews_lda@beta[2,]
reviews_topics <- tibble(term = term, topic1 = topic1, topic2 = topic2)
head(reviews_topics,3)
```

The data frame above is not yet in "tidy" format, because the beta parameters are spread across two columns. We use the `pivot_longer()` function to get the data into a tidy format. At the same time, we transform the beta parameters into probabilities by exponentiating each beta.

```{r}
reviews_topics <- reviews_topics %>% 
  pivot_longer(c(topic1, topic2), names_to = "topic", values_to = "beta") %>%
  mutate(beta = exp(beta)) # pr(topic k generates word i) = exp(beta_ik)
head(reviews_topics)
```

The **tidytext** package provides a function `tidy()` that tidies a number of different object types, including "LDA_VEM" objects. So you can skip the previous step and use a direct call to `tidy()` the `reviews_lda` object we created earlier. You can use the same approach with `matrix = "gamma"` to extract those parameters.

```{r}
reviews_topics <- tidy(reviews_lda, matrix = "beta")
head(reviews_topics)
```

Finally, we can extract the top 20 terms used in each topic and arrange these in a tidy format, and plot with a call to `ggplot()`.

```{r}
top_terms <- reviews_topics %>%
  group_by(topic) %>%
  top_n(20, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```


* The visualization highlights the most frequent words for each topic, helping interpret what each topic represents.
* Topic 2 mainly relates to Waterworld, while Topic 1 mainly relates to Taxi Driver, showing the model captures the movie-specific themes.
* Common words like "movie" and "film" appear in both topics, illustrating LDA’s ability to associate words with multiple topics.


* Differences between topics can also be highlighted by examining words with the largest differences in beta values between Topic 1 and Topic 2.
* A common approach is using the log base 2 ratio of the two betas: $ \log_2(\beta_2 / \beta_1) $
* Any log ratio is symmetrical: the log ratio for $\beta_2/\beta_1$ is -1 times the log ratio for $\beta_1/\beta_2$.
* Using base 2 makes interpretation easier: the log ratio increases by 1 for every doubling of $\beta_2$ relative to $\beta_1$.


We calculate the log ratios below and plot the words with the biggest positive and negative log ratios. We first filter out common words, such as those that have a beta greater than 1/1000 in at least one topic.

```{r}
beta_spread <- reviews_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 2 / topic 1") +
  coord_flip()
```


* The log2 ratio plot reveals distinct thematic differences between the two topics by showing which words are most characteristic of each topic.
* Clear thematic separation: The topics have successfully separated character/actor-focused discussion (Topic 1) from plot/setting-focused discussion (Topic 2).
* Magnitude matters: Words like "dryland," "helen," and "tanker" have very high positive ratios (around 600 on the log2 scale), meaning they appear exponentially more often in Topic 2 than Topic 1.
* Balanced distribution: The negative ratios are smaller in absolute value, suggesting Topic 1's characteristic words are less exclusively concentrated in that topic compared to Topic 2's words.


### Document-topic probabilities (uses the gamma values)

LDA models each document as a mixture of topics. These mixtures can be used in a straightforward way to assess whether a particular document is "mostly about Topic 1" or "mostly about Topic 2". 

In our case, since we have a very clear idea both of what the topics are about (i.e. how topics map onto movies) and what the reviews were *supposed to be about* we can examine these mixtures and see whether they make sense.

The "mixture" of topics in each document is given by document-topic probabilities referred to collectively as "gamma". Below we extract these from the object containing the LDA results, and merge the gamma values back into the original data frame containing the reviews. 


```{r}
reviews_gamma <- reviews %>% 
  left_join(tidy(reviews_lda, matrix = "gamma") %>% 
              mutate(reviewId = as.numeric(document)) %>%       
              select(-document) %>%
              spread(key = topic, value = gamma, sep = "_"))
head(reviews_gamma, 3)
```

We know that Topic 1 is clearly about Taxi Driver and Topic 2 is about Waterworld. We can therefore ask: 

1. How many of the reviews for *Taxi Driver* did the LDA estimate to be *mostly* about Topic 2 (*Waterworld*)?
2. How many of the reviews for *Waterwold* did the LDA estimate to be *mostly* about Topic 1 (*Taxi Driver*)? 

We answer this question below (*Taxi Driver* has `imdbID = 0075314`). Six of the 160 *Taxi Driver* reviews were estimated to mostly be about Topic 2, while three of the 160 *Waterworld* reviews were mostly about Topic 1, according to LDA. 

```{r}
reviews_gamma %>% group_by(imdbId) %>% summarize(ntopic1 = sum(topic_1 > 0.5))
```

Let's have a look at a few cases where the topic model got it wrong. First, we can look at a few reviews for *Taxi Driver* that the model said were mostly about Topic 2.

```{r}
reviews_gamma %>% filter(imdbId == "0075314") %>% arrange(topic_1) %>% head(7) %>% select(review, topic_1, topic_2)
```

Compare these to the *Taxi Driver* reviews that were "most" about **Topic 1** (which was, remember, to do with *Taxi Driver*).


```{r}
reviews_gamma %>% filter(imdbId == "0075314") %>% arrange(desc(topic_1)) %>% head(3) %>% select(review, topic_1, topic_2)
```

Let's do the same thing for *Waterworld*: look at a few reviews that the model said were mostly about Topic 1 (*Taxi Driver*).

```{r}
reviews_gamma %>% filter(imdbId != "0075314") %>% arrange(topic_2) %>% head(4) %>% select(review, topic_1, topic_2)
```

And finally, compare the reviews above to reviews that were "most" about Topic 2.

```{r}
reviews_gamma %>% filter(imdbId != "0075314") %>% arrange(desc(topic_2)) %>% head(3) %>% select(review, topic_1, topic_2)
```


# Exercises

1. Do topic modelling on the set of Trump tweets that we have been using all along. 
    + experiment with the number of topics.
    + look at topics for tweets created in (a) the year before and (b) the year after he became president. Are there differences?
    
2. Work through [this example](http://tidytextmining.com/topicmodeling.html#library-heist) in Chapter 6 of TMR. 

3. There is also a topic modelling part to the case study in [Chapter 9](http://tidytextmining.com/usenet.html).




# Topic Modelling Example: the great library heist

In text mining, we often have collections of documents, such as blog posts or news articles, that we’d like to divide into natural groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.

Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.

The example below involves recovering chapters from four classic books that have been mixed together, treating each book as a separate topic. Topic modeling is applied to cluster the unlabeled chapters, aiming to correctly group them by book.


```{r}
titles <- c("Twenty Thousand Leagues under the Sea", 
            "The War of the Worlds",
            "Pride and Prejudice", 
            "Great Expectations")

library(gutenbergr)

books <- gutenberg_works(title %in% titles) %>% gutenberg_download(meta_fields = "title")
```

As pre-processing, we divide these into chapters, use tidytext’s unnest_tokens() to separate them into words, then remove stop_words. We’re treating every chapter as a separate “document”, each with a name like Great Expectations_1 or Pride and Prejudice_11. (In other applications, each document might be one newspaper article, or one blog post).


```{r}
library(stringr)

# divide into documents, each representing one chapter
by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter = cumsum(str_detect(
    text, regex("^(chapter|book)", ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, title, chapter)

# split into words
by_chapter_word <- by_chapter %>%
  unnest_tokens(word, text)

# find document-word counts
word_counts <- by_chapter_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE)

head(word_counts)
```


### LDA on chapters

Right now our data frame word_counts is in a tidy form, with one-term-per-document-per-row, but the topicmodels package requires a DocumentTermMatrix.

```{r}
chapters_dtm <- word_counts %>%
  cast_dtm(document, word, n)

head(chapters_dtm)
```

We can then use the LDA() function to create a four-topic model. In this case we know we’re looking for four topics because there are four books; in other problems we may need to try a few different values of k.

```{r}
chapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))
chapters_lda
```

### Examine per-topic-per-word probabilities

```{r}
chapter_topics <- tidy(chapters_lda, matrix = "beta")
head(chapter_topics)
```

Notice that this has turned the model into a one-topic-per-term-per-row format. For each combination, the model computes the probability of that term being generated from that topic. For example, the term “joe” has an almost zero probability of being generated from topics 1, 2, or 3, but it makes up 0% of topic 4.

We could use dplyr’s slice_max() to find the top 5 terms within each topic.

```{r}
top_terms <- chapter_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

This tidy output lends itself well to a ggplot2 visualization

```{r}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```


These topics are pretty clearly associated with the four books! There’s no question that the topic of “captain”, “nautilus”, “sea”, and “nemo” belongs to Twenty Thousand Leagues Under the Sea, and that “jane”, “darcy”, and “elizabeth” belongs to Pride and Prejudice. We see “pip” and “joe” from Great Expectations and “martians”, “black”, and “night” from The War of the Worlds. We also notice that, in line with LDA being a “fuzzy clustering” method, there can be words in common between multiple topics, such as “miss” in topics 1 and 4, and “time” in topics 3 and 4.


### Per-document classification

Each document in this analysis represented a single chapter. Thus, we may want to know which topics are associated with each document. Can we put the chapters back together in the correct books? We can find this by examining the per-document-per-topic probabilities, gamma.

```{r}
chapters_gamma <- tidy(chapters_lda, matrix = "gamma")
```

Each of these values is an estimated proportion of words from that document that are generated from that topic.

Now that we have these topic probabilities, we can see how well our unsupervised learning did at distinguishing the four books. We’d expect that chapters within a book would be found to be mostly (or entirely), generated from the corresponding topic.

First we re-separate the document name into title and chapter, after which we can visualize the per-document-per-topic probability for each.


```{r}
chapters_gamma <- chapters_gamma %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)

head(chapters_gamma)

# reorder titles in order of topic 1, topic 2, etc before plotting
chapters_gamma %>%
  mutate(title = reorder(title, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title, ncol = 2) +
  labs(x = "topic", y = expression(gamma))


```


We notice that almost all of the chapters from Pride and Prejudice, War of the Worlds, and Twenty Thousand Leagues Under the Sea were uniquely identified as a single topic each.


It does look like some chapters from Great Expectations (which should be topic 4) were somewhat associated with other topics. Are there any cases where the topic most associated with a chapter belonged to another book? First we’d find the topic that was most associated with each chapter using slice_max(), which is effectively the “classification” of that chapter.


```{r}
chapter_classifications <- chapters_gamma %>%
  group_by(title, chapter) %>%
  slice_max(gamma) %>%
  ungroup()

head(chapter_classifications)
```


We can then compare each to the “consensus” topic for each book (the most common topic among its chapters), and see which were most often misidentified.

```{r}
book_topics <- chapter_classifications %>%
  count(title, topic) %>%
  group_by(title) %>%
  slice_max(n, n = 1) %>% 
  ungroup() %>%
  transmute(consensus = title, topic)

chapter_classifications %>%
  inner_join(book_topics, by = "topic") %>%
  filter(title != consensus)

#>   title              chapter topic gamma consensus       
#>   <chr>              <int>  <int>  <dbl>  <chr>           
#> 1 Great Expectations   54     1    0.700  The War of the Worlds
```

We see that only two chapters from Great Expectations were misclassified, as LDA described one as coming from the “Pride and Prejudice” topic (topic 1) and one from The War of the Worlds (topic 3). That’s not bad for unsupervised clustering!


### By word assignments: augment

....... Continue example....
